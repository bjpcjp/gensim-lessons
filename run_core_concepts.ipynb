{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Core Concepts - Documents, Corpora, Vectors & Models\n",
    "====================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core concepts of ``gensim`` are:\n",
    "\n",
    "1. `core_concepts_document`: some text.\n",
    "2. `core_concepts_corpus`: a collection of documents.\n",
    "3. `core_concepts_vector`: a mathematically convenient representation of a document.\n",
    "4. `core_concepts_model`: an algorithm for transforming vectors from one representation to another.\n",
    "\n",
    "Document\n",
    "--------\n",
    "* In Gensim, a *document* is a [Python text sequence](https://docs.python.org/3.7/library/stdtypes.html#text-sequence-type-str), aka a \"string\". It could be anything from a 140 character tweet, a single paragraph, a news article, or a book.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "document = \"Human machine interface for lab abc computer applications\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Corpus\n",
    "------\n",
    "\n",
    "A *corpus* is a collection of `core_concepts_document` objects. They serve two purposes.\n",
    "\n",
    "1. Input for training a *core_concepts_model*. During training, the models use this *training corpus* to look for common themes and topics, initializing their internal model parameters. Gensim focuses on *unsupervised* models - no human intervention is required.\n",
    "\n",
    "2. Documents to organize: after training, a topic model can extract topics from new documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "text_corpus = [\n",
    "    \"Human machine interface for lab abc computer applications\",\n",
    "    \"A survey of user opinion of computer system response time\",\n",
    "    \"The EPS user interface management system\",\n",
    "    \"System and human system engineering testing of EPS\",\n",
    "    \"Relation of user perceived response time to error measurement\",\n",
    "    \"The generation of random binary unordered trees\",\n",
    "    \"The intersection graph of paths in trees\",\n",
    "    \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "    \"Graph minors A survey\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This example loads the entire corpus into memory. In practice, corpora may be very large, so loading them into memory may be impossible. Gensim handles these cases by *streaming* them one document at a time.\n",
    "\n",
    "* We then remove common English words (such as 'the') and words that occur only once in the corpus. In the process of doing so, we'll tokenize our data. Tokenization breaks up the documents into words (in this case using space as a delimiter).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['human', 'interface', 'computer'],\n",
      " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
      " ['eps', 'user', 'interface', 'system'],\n",
      " ['system', 'human', 'system', 'eps'],\n",
      " ['user', 'response', 'time'],\n",
      " ['trees'],\n",
      " ['graph', 'trees'],\n",
      " ['graph', 'minors', 'trees'],\n",
      " ['graph', 'minors', 'survey']]\n"
     ]
    }
   ],
   "source": [
    "# Create a set of frequent words\n",
    "stoplist = set('for a of the and to in'.split(' '))\n",
    "\n",
    "# Lowercase each document, split it by white space and filter out stopwords\n",
    "texts = [[word for word in document.lower().split() if word not in stoplist]\n",
    "         for document in text_corpus]\n",
    "\n",
    "# Count word frequencies\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "# Only keep words that appear more than once\n",
    "processed_corpus = [[token for token in text if frequency[token] > 1] for text in texts]\n",
    "pprint.pprint(processed_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Associate each word in the corpus with a unique integer ID. This dictionary defines the vocabulary of all words that our model knows about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to import 'smart_open.gcs', disabling that module\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...)\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "dictionary = corpora.Dictionary(processed_corpus)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Because our corpus is small, there are only 12 different tokens in this dictionary. Larger dictionaries with hundreds of thousands of tokens are quite common."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector\n",
    "------\n",
    "\n",
    "* We need to represent documents mathematically. One approach is to represent each document as a vector of *features*. A single feature may be thought of as a question-answer pair:\n",
    "\n",
    "1. How many times does the word *splonge* appear in the document? (Zero.)\n",
    "2. How many paragraphs does the document consist of? (Two.)\n",
    "3. How many fonts does the document use? (Five.)\n",
    "\n",
    "* The question is represented by its integer id: *(1,2,3)* - so this document becomes a series of pairs: *(1, 0.0), (2, 2.0), (3, 5.0)*. This *dense vector* contains an explicit answer to each question.\n",
    "\n",
    "* If we already know all the questions, we may leave them implicit and represent the document as *(0, 2, 5)*. This is the **vector** for our document.\n",
    "\n",
    "* Vectors usually consist of many zero values. Gensim omits all vector elements with value 0.0 to save memory - our example becomes *(2, 2.0), (3, 5.0)* - a 'sparse' (ie 'bag of words' (BoW) vector). All missing feature values can therefore be resolved to zero.\n",
    "\n",
    "* Assuming identical questions, we can compare the vectors of two documents. For example, assume we are given two vectors (0.0, 2.0, 5.0) and (0.1, 1.9, 4.9). Because the vectors are similar to each other, we can conclude the corresponding documents are also similar.\n",
    "\n",
    "* Another approach to represent a document as a vector containing the frequency counts of each word in the dictionary. For example, assume we have a dictionary containing the words ['coffee', 'milk', 'sugar', 'spoon']. A document consisting of the string \"coffee milk coffee\" would be represented by the vector [2, 1, 0, 0] where the entries are (in order) the occurrences of \"coffee\", \"milk\", \"sugar\" and \"spoon\" in the document. The length of the vector is the number of entries in the dictionary. \n",
    "\n",
    "* One of the main properties of this model is that it completely ignores the order of the tokens in the document - hence the 'bag-of-words' name.\n",
    "\n",
    "* Our corpus has 12 unique words in it, so each document will be represented by a 12-dimensional vector. We can use the dictionary to turn tokenized documents into these 12-dimensional vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'computer': 0,\n",
      " 'eps': 8,\n",
      " 'graph': 10,\n",
      " 'human': 1,\n",
      " 'interface': 2,\n",
      " 'minors': 11,\n",
      " 'response': 3,\n",
      " 'survey': 4,\n",
      " 'system': 5,\n",
      " 'time': 6,\n",
      " 'trees': 9,\n",
      " 'user': 7}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Suppose we wanted to vectorize the phrase \"Human computer interaction\" (Note: this phrase was not in our original corpus). We can create the BoW representation for a document using the **doc2bow** method - it returns a sparse representation of the word counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1)]\n"
     ]
    }
   ],
   "source": [
    "new_doc = \"Human computer interaction\"\n",
    "new_vec = dictionary.doc2bow(new_doc.lower().split())\n",
    "print(new_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The 1st entry in each tuple = the token ID in the dictionary.\n",
    "* The 2nd entry in each tuple = the count of this token.\n",
    "\n",
    "* Note: \"interaction\" was not in the original corpus - so it was not included in the vectorization. Also: this vector only contains entries for words that actually appeared in the document. Because any document may only contain a few words out in the dictionary, words that do not appear in the vectorization are represented as implicitly zero as a space saving measure.\n",
    "\n",
    "* We can convert our entire original corpus to a list of vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1)],\n",
      " [(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)],\n",
      " [(2, 1), (5, 1), (7, 1), (8, 1)],\n",
      " [(1, 1), (5, 2), (8, 1)],\n",
      " [(3, 1), (6, 1), (7, 1)],\n",
      " [(9, 1)],\n",
      " [(9, 1), (10, 1)],\n",
      " [(9, 1), (10, 1), (11, 1)],\n",
      " [(4, 1), (10, 1), (11, 1)]]\n"
     ]
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(text) for text in processed_corpus]\n",
    "pprint.pprint(bow_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* While this list lives entirely in memory, in most applications you will want a more scalable solution. **gensim** accepts any iterator that returns a single document vector at a time.\n",
    "\n",
    "Model\n",
    "-----\n",
    "\n",
    "* **gensim** documents are represented as vectors, so a model can be thought of as a transformation between two vectors. The model learns the details of this transformation during training, when it reads the training corpus\n",
    "\n",
    "* One example is [tf-idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf). it transforms vectors from BoW format to a vector space where the frequency counts are weighted according to the relative rarity of each word in the corpus.\n",
    "\n",
    "* Here's a simple example. Let's initialize the tf-idf model, train it on our corpus and transform the string \"system minors\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5, 0.5898341626740045), (11, 0.8075244024440723)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import models\n",
    "\n",
    "# train the model\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "\n",
    "# transform the \"system minors\" string\n",
    "words = \"system minors\".lower().split()\n",
    "print(tfidf[dictionary.doc2bow(words)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **tfidf** returns a list of (ID, weighting) tuples. Note the ID corresponding to \"system\" (which occurred 4 times in the original corpus) has been weighted lower than the ID corresponding to \"minors\" (which only occurred twice).\n",
    "\n",
    "* Once you've created the model, you can do all sorts of cool stuff with it. For example, to transform the whole corpus via TfIdf and index it, in preparation for similarity queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from gensim import similarities\n",
    "\n",
    "index = similarities.SparseMatrixSimilarity(tfidf[bow_corpus], num_features=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Find the similarity of our query document against every document in the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.0),\n",
      " (1, 0.32448703),\n",
      " (2, 0.41707572),\n",
      " (3, 0.7184812),\n",
      " (4, 0.0),\n",
      " (5, 0.0),\n",
      " (6, 0.0),\n",
      " (7, 0.0),\n",
      " (8, 0.0)]\n"
     ]
    }
   ],
   "source": [
    "query_document = 'system engineering'.split()\n",
    "query_bow = dictionary.doc2bow(query_document)\n",
    "sims = index[tfidf[query_bow]]\n",
    "pprint.pprint(list(enumerate(sims)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Document 3 has a similarity score of 0.718=72%; document 2 has a similarity score of 42%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, 0.7184812),\n",
      " (2, 0.41707572),\n",
      " (1, 0.32448703),\n",
      " (0, 0.0),\n",
      " (4, 0.0),\n",
      " (5, 0.0),\n",
      " (6, 0.0),\n",
      " (7, 0.0),\n",
      " (8, 0.0)]\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(list(sorted(enumerate(sims), key=lambda x: x[1], reverse=True)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
