{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "FastText Model\n",
    "==============\n",
    "\n",
    "Introduces Gensim's fastText model and demonstrates its use on the Lee Corpus.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "#import logging\n",
    "#logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When to use FastText?\n",
    "---------------------\n",
    "\n",
    "* The morphological structure of a word carries information about its meaning which is not taken into account by traditional word embeddings which train an embedding for each word.\n",
    "\n",
    "* This is significant for morphologically rich languages (ex: German, Turkish) in which a single word can have a large number of morphological forms, each of which might occur rarely, thus making it hard to train good word embeddings.\n",
    "\n",
    "* _fastText treats each word as the aggregation of its subwords_. Subwords are taken to be the character ngrams of the word. The vector for a word is simply taken to be the sum of all vectors of its component char-ngrams.\n",
    "\n",
    "* fastText does significantly better on **syntactic tasks** vs the original Word2Vec, especially on smaller training sets. Word2Vec slightly outperforms FastText on **semantic tasks**. The differences grow smaller as the size of training corpus increases.\n",
    "\n",
    "* Training time for fastText is higher than the Gensim version of Word2Vec.\n",
    "\n",
    "* fastText can be used to obtain vectors for out-of-vocabulary (OOV) words, by summing up vectors for its component char-ngrams, if least one of the char-ngrams was present in the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training models\n",
    "---------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the following examples, we'll use the Lee Corpus (which you already have if you've installed gensim) for training our model.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to import 'smart_open.gcs', disabling that module\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<gensim.models.fasttext.FastText object at 0x7fa3c26d1128>\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint as print\n",
    "from gensim.models.fasttext import FastText as FT_gensim\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "# Set file names for train and test data\n",
    "corpus_file = datapath('lee_background.cor')\n",
    "model       = FT_gensim(size=100)\n",
    "\n",
    "model.build_vocab(corpus_file=corpus_file)\n",
    "\n",
    "model.train(\n",
    "    corpus_file    = corpus_file, \n",
    "    epochs         = model.epochs,\n",
    "    total_examples = model.corpus_count, \n",
    "    total_words    = model.corpus_total_words\n",
    ")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters for training the model follow the same pattern as Word2Vec. FastText supports the following parameters from the original word2vec:\n",
    "\n",
    "- model: Training architecture. Allowed values: `cbow`, `skipgram` (Default `cbow`)\n",
    "- size: Size of embeddings to be learnt (Default 100)\n",
    "- alpha: Initial learning rate (Default 0.025)\n",
    "- window: Context window size (Default 5)\n",
    "- min_count: Ignore words with number of occurrences below this (Default 5)\n",
    "- loss: Training objective. Allowed values: `ns`, `hs`, `softmax` (Default `ns`)\n",
    "- sample: Threshold for downsampling higher-frequency words (Default 0.001)\n",
    "- negative: Number of negative words to sample, for `ns` (Default 5)\n",
    "- iter: Number of epochs (Default 5)\n",
    "- sorted_vocab: Sort vocab by descending frequency (Default 1)\n",
    "- threads: Number of threads to use (Default 12)\n",
    "\n",
    "FastText has three additional parameters:\n",
    "\n",
    "- _min_n_: min length of char ngrams (Default 3)\n",
    "- _max_n_: max length of char ngrams (Default 6)\n",
    "- _bucket_: number of buckets used for hashing ngrams (Default 2000000)\n",
    "\n",
    "* _min_n_ and _max_n_ control the lengths of character ngrams that each word is broken down into while training and looking up embeddings. If ``max_n`` is set to 0 or less thatn _min_n_, no character ngrams are used - the model effectively reduces to Word2Vec.\n",
    "\n",
    "* A hashing function maps ngrams to integers [1-K]. The [Fowler-Noll-Vo hashing function](http://www.isthe.com/chongo/tech/comp/fnv) (FNV-1a variant) is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** As in the case of Word2Vec, you can continue to train your model while using Gensim's native implementation of fastText.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving/loading models\n",
    "---------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models can be saved and loaded via the ``load`` and ``save`` methods.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<gensim.models.fasttext.FastText object at 0x7fa3c26d10b8>\n"
     ]
    }
   ],
   "source": [
    "# saving a model trained via Gensim's fastText implementation\n",
    "import tempfile\n",
    "import os\n",
    "with tempfile.NamedTemporaryFile(prefix='saved_model_gensim-', delete=False) as tmp:\n",
    "    model.save(tmp.name, separately=[])\n",
    "\n",
    "loaded_model = FT_gensim.load(tmp.name)\n",
    "print(loaded_model)\n",
    "\n",
    "os.unlink(tmp.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* _save_word2vec_method_ causes the ngram vectors to be lost - so models loaded this way will behave as regular word2vec models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word vector lookup\n",
    "------------------\n",
    "**Note:** Operations like word vector lookups and similarity queries work exactly like their FastText equivalents - so they have been demonstrated using only the native fastText implementation here.\n",
    "* FastText models support vector lookups for out-of-vocabulary words by summing up character ngrams belonging to the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print('night' in model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print('nights' in model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([ 0.0951848 ,  0.00439684, -0.5730612 ,  0.48200205,  0.59976846,\n",
      "       -0.31887755, -0.18905407, -0.04429929,  0.42820224,  0.33637044,\n",
      "       -0.6341972 , -0.01148262, -0.67034054,  0.41099748,  0.279039  ,\n",
      "       -0.0609754 , -0.19455384,  0.18634835,  0.2372648 , -0.3728167 ,\n",
      "       -0.2411645 ,  0.27350628, -0.37734354,  0.02136977, -0.82818276,\n",
      "        0.7267074 ,  0.11816929,  0.17816381,  0.4048582 ,  0.0064444 ,\n",
      "       -0.6489303 ,  0.23216677,  0.08447264, -0.47895163,  0.46647942,\n",
      "        0.10955972, -0.16104989, -0.0692726 ,  0.41848487,  0.21457395,\n",
      "       -0.00909338, -0.07110799,  0.38920757, -0.06391519,  0.12494167,\n",
      "        0.18315583, -0.13085683,  0.21250576, -0.01720706, -0.3876734 ,\n",
      "       -0.568277  , -0.55089724,  0.06869221,  0.01142327,  0.40094298,\n",
      "       -0.79913646, -0.10639761, -0.1799068 ,  0.01203778, -0.02949148,\n",
      "        0.21544454, -0.12062592, -0.48480937, -0.09911016, -0.53330535,\n",
      "        0.35012123,  0.00529964,  0.15234514,  0.03122097,  0.47152737,\n",
      "       -0.5667298 , -0.51113737, -0.00970246, -0.11250026, -0.3372492 ,\n",
      "        0.03334659,  0.2744025 ,  0.10833993, -0.1398816 ,  0.18215661,\n",
      "        0.4534146 ,  0.05685214, -0.16581866,  0.4537347 , -0.39371023,\n",
      "       -0.256516  ,  0.12320353,  0.28864732,  0.31867015,  0.29335305,\n",
      "       -0.17207709,  0.11897218, -0.03791546, -0.19243279, -0.2693747 ,\n",
      "        0.55998015,  0.28280327,  0.4424981 ,  0.54906666,  0.47264728],\n",
      "      dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bjpcjp/.local/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "print(model['night'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([ 0.08390839,  0.00439281, -0.49997562,  0.4197932 ,  0.52237827,\n",
      "       -0.2796327 , -0.16560148, -0.03786143,  0.373268  ,  0.29412517,\n",
      "       -0.55540466, -0.01145034, -0.5850891 ,  0.35996655,  0.23944286,\n",
      "       -0.05291162, -0.17186531,  0.1608732 ,  0.20536552, -0.32568985,\n",
      "       -0.21039045,  0.23954262, -0.32993677,  0.01930434, -0.72327334,\n",
      "        0.63382936,  0.10306088,  0.15591006,  0.35340077,  0.0052096 ,\n",
      "       -0.5640832 ,  0.20378862,  0.07240348, -0.4200235 ,  0.40685552,\n",
      "        0.09530769, -0.13907899, -0.06173637,  0.3651373 ,  0.18804902,\n",
      "       -0.00701293, -0.06219959,  0.33914077, -0.05404743,  0.10795779,\n",
      "        0.16007127, -0.11776429,  0.18721513, -0.01566308, -0.33687627,\n",
      "       -0.49844742, -0.47994205,  0.06110583,  0.01093521,  0.35052204,\n",
      "       -0.6975338 , -0.09294295, -0.15586945,  0.00817089, -0.02542602,\n",
      "        0.18758976, -0.10613206, -0.42290136, -0.08708555, -0.46591488,\n",
      "        0.3049851 ,  0.00474294,  0.13284114,  0.02900123,  0.41230112,\n",
      "       -0.4946037 , -0.4456535 , -0.00851887, -0.0990812 , -0.29412687,\n",
      "        0.02828098,  0.23955758,  0.09476331, -0.12016022,  0.15818532,\n",
      "        0.39653596,  0.04944329, -0.14557382,  0.39515826, -0.34259874,\n",
      "       -0.22324617,  0.10957074,  0.25319406,  0.27745992,  0.25673515,\n",
      "       -0.1505285 ,  0.10361007, -0.03448138, -0.16766554, -0.23489884,\n",
      "        0.4890186 ,  0.2458742 ,  0.38475513,  0.47880778,  0.41260922],\n",
      "      dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bjpcjp/.local/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "print(model['nights'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The _in_ operation works slightly differently than the original word2vec implementation. It tests whether a vector for the given word exists or not, not whether the word is present in the word vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(\"word\" in model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bjpcjp/.local/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "print(\"word\" in model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarity operations\n",
    "---------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Similarity operations work the same way as word2vec. **Out-of-vocabulary words can also be used, provided they have at least one character ngram present in the training data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(\"nights\" in model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(\"night\" in model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999927\n"
     ]
    }
   ],
   "source": [
    "#print(model.similarity(\"night\", \"nights\"))\n",
    "print(model.wv.similarity(\"night\",\"nights\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Syntactically similar words will have high similarity scores in fastText models, since a large number of the component char-ngrams will be the same. Therefore fastText usually does better at syntactic tasks than Word2Vec. \n",
    "* See [here](Word2Vec_FastText_Comparison.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bjpcjp/.local/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Arafat', 0.9982719421386719),\n",
      " ('study', 0.9982718825340271),\n",
      " ('\"That', 0.9982710480690002),\n",
      " ('boat', 0.9982693791389465),\n",
      " ('Arafat,', 0.9982660412788391),\n",
      " ('often', 0.9982610940933228),\n",
      " ('north.', 0.9982523918151855),\n",
      " ('Endeavour', 0.998251736164093),\n",
      " (\"Arafat's\", 0.9982442855834961),\n",
      " ('details', 0.9982432126998901)]\n"
     ]
    }
   ],
   "source": [
    "print(model.most_similar(\"nights\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bjpcjp/.local/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `n_similarity` (Method will be removed in 4.0.0, use self.wv.n_similarity() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "print(model.n_similarity(['sushi', 'shop'], ['japanese', 'restaurant']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'breakfast'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bjpcjp/.local/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `doesnt_match` (Method will be removed in 4.0.0, use self.wv.doesnt_match() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/bjpcjp/.local/lib/python3.6/site-packages/gensim/models/keyedvectors.py:877: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
     ]
    }
   ],
   "source": [
    "print(model.doesnt_match(\"breakfast cereal dinner lunch\".split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('1', 0.2420959174633026),\n",
      " ('40', 0.2376674860715866),\n",
      " ('2', 0.23438993096351624),\n",
      " ('26', 0.23292623460292816),\n",
      " ('20', 0.23248399794101715),\n",
      " ('UN', 0.23195061087608337),\n",
      " ('blaze', 0.23188075423240662),\n",
      " ('keep', 0.2311323881149292),\n",
      " ('...', 0.23067551851272583),\n",
      " ('As', 0.23064441978931427)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bjpcjp/.local/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "print(model.most_similar(positive=['baghdad', 'england'], negative=['london']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bjpcjp/.local/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `accuracy` (Method will be removed in 4.0.0, use self.wv.accuracy() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'correct': [], 'incorrect': [], 'section': 'capital-common-countries'},\n",
      " {'correct': [], 'incorrect': [], 'section': 'capital-world'},\n",
      " {'correct': [], 'incorrect': [], 'section': 'currency'},\n",
      " {'correct': [], 'incorrect': [], 'section': 'city-in-state'},\n",
      " {'correct': [],\n",
      "  'incorrect': [('HE', 'SHE', 'HIS', 'HER'), ('HIS', 'HER', 'HE', 'SHE')],\n",
      "  'section': 'family'},\n",
      " {'correct': [], 'incorrect': [], 'section': 'gram1-adjective-to-adverb'},\n",
      " {'correct': [], 'incorrect': [], 'section': 'gram2-opposite'},\n",
      " {'correct': [('GOOD', 'BETTER', 'GREAT', 'GREATER'),\n",
      "              ('LONG', 'LONGER', 'GREAT', 'GREATER')],\n",
      "  'incorrect': [('GOOD', 'BETTER', 'LONG', 'LONGER'),\n",
      "                ('GOOD', 'BETTER', 'LOW', 'LOWER'),\n",
      "                ('GREAT', 'GREATER', 'LONG', 'LONGER'),\n",
      "                ('GREAT', 'GREATER', 'LOW', 'LOWER'),\n",
      "                ('GREAT', 'GREATER', 'GOOD', 'BETTER'),\n",
      "                ('LONG', 'LONGER', 'LOW', 'LOWER'),\n",
      "                ('LONG', 'LONGER', 'GOOD', 'BETTER'),\n",
      "                ('LOW', 'LOWER', 'GOOD', 'BETTER'),\n",
      "                ('LOW', 'LOWER', 'GREAT', 'GREATER'),\n",
      "                ('LOW', 'LOWER', 'LONG', 'LONGER')],\n",
      "  'section': 'gram3-comparative'},\n",
      " {'correct': [('GREAT', 'GREATEST', 'LARGE', 'LARGEST')],\n",
      "  'incorrect': [('BIG', 'BIGGEST', 'GOOD', 'BEST'),\n",
      "                ('BIG', 'BIGGEST', 'GREAT', 'GREATEST'),\n",
      "                ('BIG', 'BIGGEST', 'LARGE', 'LARGEST'),\n",
      "                ('GOOD', 'BEST', 'GREAT', 'GREATEST'),\n",
      "                ('GOOD', 'BEST', 'LARGE', 'LARGEST'),\n",
      "                ('GOOD', 'BEST', 'BIG', 'BIGGEST'),\n",
      "                ('GREAT', 'GREATEST', 'BIG', 'BIGGEST'),\n",
      "                ('GREAT', 'GREATEST', 'GOOD', 'BEST'),\n",
      "                ('LARGE', 'LARGEST', 'BIG', 'BIGGEST'),\n",
      "                ('LARGE', 'LARGEST', 'GOOD', 'BEST'),\n",
      "                ('LARGE', 'LARGEST', 'GREAT', 'GREATEST')],\n",
      "  'section': 'gram4-superlative'},\n",
      " {'correct': [('PLAY', 'PLAYING', 'SAY', 'SAYING'),\n",
      "              ('PLAY', 'PLAYING', 'LOOK', 'LOOKING'),\n",
      "              ('SAY', 'SAYING', 'LOOK', 'LOOKING'),\n",
      "              ('SAY', 'SAYING', 'PLAY', 'PLAYING')],\n",
      "  'incorrect': [('GO', 'GOING', 'LOOK', 'LOOKING'),\n",
      "                ('GO', 'GOING', 'PLAY', 'PLAYING'),\n",
      "                ('GO', 'GOING', 'RUN', 'RUNNING'),\n",
      "                ('GO', 'GOING', 'SAY', 'SAYING'),\n",
      "                ('LOOK', 'LOOKING', 'PLAY', 'PLAYING'),\n",
      "                ('LOOK', 'LOOKING', 'RUN', 'RUNNING'),\n",
      "                ('LOOK', 'LOOKING', 'SAY', 'SAYING'),\n",
      "                ('LOOK', 'LOOKING', 'GO', 'GOING'),\n",
      "                ('PLAY', 'PLAYING', 'RUN', 'RUNNING'),\n",
      "                ('PLAY', 'PLAYING', 'GO', 'GOING'),\n",
      "                ('RUN', 'RUNNING', 'SAY', 'SAYING'),\n",
      "                ('RUN', 'RUNNING', 'GO', 'GOING'),\n",
      "                ('RUN', 'RUNNING', 'LOOK', 'LOOKING'),\n",
      "                ('RUN', 'RUNNING', 'PLAY', 'PLAYING'),\n",
      "                ('SAY', 'SAYING', 'GO', 'GOING'),\n",
      "                ('SAY', 'SAYING', 'RUN', 'RUNNING')],\n",
      "  'section': 'gram5-present-participle'},\n",
      " {'correct': [('AUSTRALIA', 'AUSTRALIAN', 'INDIA', 'INDIAN'),\n",
      "              ('AUSTRALIA', 'AUSTRALIAN', 'ISRAEL', 'ISRAELI'),\n",
      "              ('FRANCE', 'FRENCH', 'INDIA', 'INDIAN'),\n",
      "              ('FRANCE', 'FRENCH', 'ISRAEL', 'ISRAELI'),\n",
      "              ('INDIA', 'INDIAN', 'ISRAEL', 'ISRAELI'),\n",
      "              ('INDIA', 'INDIAN', 'AUSTRALIA', 'AUSTRALIAN'),\n",
      "              ('ISRAEL', 'ISRAELI', 'INDIA', 'INDIAN'),\n",
      "              ('SWITZERLAND', 'SWISS', 'INDIA', 'INDIAN')],\n",
      "  'incorrect': [('AUSTRALIA', 'AUSTRALIAN', 'FRANCE', 'FRENCH'),\n",
      "                ('AUSTRALIA', 'AUSTRALIAN', 'SWITZERLAND', 'SWISS'),\n",
      "                ('FRANCE', 'FRENCH', 'SWITZERLAND', 'SWISS'),\n",
      "                ('FRANCE', 'FRENCH', 'AUSTRALIA', 'AUSTRALIAN'),\n",
      "                ('INDIA', 'INDIAN', 'SWITZERLAND', 'SWISS'),\n",
      "                ('INDIA', 'INDIAN', 'FRANCE', 'FRENCH'),\n",
      "                ('ISRAEL', 'ISRAELI', 'SWITZERLAND', 'SWISS'),\n",
      "                ('ISRAEL', 'ISRAELI', 'AUSTRALIA', 'AUSTRALIAN'),\n",
      "                ('ISRAEL', 'ISRAELI', 'FRANCE', 'FRENCH'),\n",
      "                ('SWITZERLAND', 'SWISS', 'AUSTRALIA', 'AUSTRALIAN'),\n",
      "                ('SWITZERLAND', 'SWISS', 'FRANCE', 'FRENCH'),\n",
      "                ('SWITZERLAND', 'SWISS', 'ISRAEL', 'ISRAELI')],\n",
      "  'section': 'gram6-nationality-adjective'},\n",
      " {'correct': [('PAYING', 'PAID', 'SAYING', 'SAID')],\n",
      "  'incorrect': [('GOING', 'WENT', 'PAYING', 'PAID'),\n",
      "                ('GOING', 'WENT', 'PLAYING', 'PLAYED'),\n",
      "                ('GOING', 'WENT', 'SAYING', 'SAID'),\n",
      "                ('GOING', 'WENT', 'TAKING', 'TOOK'),\n",
      "                ('PAYING', 'PAID', 'PLAYING', 'PLAYED'),\n",
      "                ('PAYING', 'PAID', 'TAKING', 'TOOK'),\n",
      "                ('PAYING', 'PAID', 'GOING', 'WENT'),\n",
      "                ('PLAYING', 'PLAYED', 'SAYING', 'SAID'),\n",
      "                ('PLAYING', 'PLAYED', 'TAKING', 'TOOK'),\n",
      "                ('PLAYING', 'PLAYED', 'GOING', 'WENT'),\n",
      "                ('PLAYING', 'PLAYED', 'PAYING', 'PAID'),\n",
      "                ('SAYING', 'SAID', 'TAKING', 'TOOK'),\n",
      "                ('SAYING', 'SAID', 'GOING', 'WENT'),\n",
      "                ('SAYING', 'SAID', 'PAYING', 'PAID'),\n",
      "                ('SAYING', 'SAID', 'PLAYING', 'PLAYED'),\n",
      "                ('TAKING', 'TOOK', 'GOING', 'WENT'),\n",
      "                ('TAKING', 'TOOK', 'PAYING', 'PAID'),\n",
      "                ('TAKING', 'TOOK', 'PLAYING', 'PLAYED'),\n",
      "                ('TAKING', 'TOOK', 'SAYING', 'SAID')],\n",
      "  'section': 'gram7-past-tense'},\n",
      " {'correct': [('MAN', 'MEN', 'CHILD', 'CHILDREN')],\n",
      "  'incorrect': [('BUILDING', 'BUILDINGS', 'CAR', 'CARS'),\n",
      "                ('BUILDING', 'BUILDINGS', 'CHILD', 'CHILDREN'),\n",
      "                ('BUILDING', 'BUILDINGS', 'MAN', 'MEN'),\n",
      "                ('CAR', 'CARS', 'CHILD', 'CHILDREN'),\n",
      "                ('CAR', 'CARS', 'MAN', 'MEN'),\n",
      "                ('CAR', 'CARS', 'BUILDING', 'BUILDINGS'),\n",
      "                ('CHILD', 'CHILDREN', 'MAN', 'MEN'),\n",
      "                ('CHILD', 'CHILDREN', 'BUILDING', 'BUILDINGS'),\n",
      "                ('CHILD', 'CHILDREN', 'CAR', 'CARS'),\n",
      "                ('MAN', 'MEN', 'BUILDING', 'BUILDINGS'),\n",
      "                ('MAN', 'MEN', 'CAR', 'CARS')],\n",
      "  'section': 'gram8-plural'},\n",
      " {'correct': [], 'incorrect': [], 'section': 'gram9-plural-verbs'},\n",
      " {'correct': [('GOOD', 'BETTER', 'GREAT', 'GREATER'),\n",
      "              ('LONG', 'LONGER', 'GREAT', 'GREATER'),\n",
      "              ('GREAT', 'GREATEST', 'LARGE', 'LARGEST'),\n",
      "              ('PLAY', 'PLAYING', 'SAY', 'SAYING'),\n",
      "              ('PLAY', 'PLAYING', 'LOOK', 'LOOKING'),\n",
      "              ('SAY', 'SAYING', 'LOOK', 'LOOKING'),\n",
      "              ('SAY', 'SAYING', 'PLAY', 'PLAYING'),\n",
      "              ('AUSTRALIA', 'AUSTRALIAN', 'INDIA', 'INDIAN'),\n",
      "              ('AUSTRALIA', 'AUSTRALIAN', 'ISRAEL', 'ISRAELI'),\n",
      "              ('FRANCE', 'FRENCH', 'INDIA', 'INDIAN'),\n",
      "              ('FRANCE', 'FRENCH', 'ISRAEL', 'ISRAELI'),\n",
      "              ('INDIA', 'INDIAN', 'ISRAEL', 'ISRAELI'),\n",
      "              ('INDIA', 'INDIAN', 'AUSTRALIA', 'AUSTRALIAN'),\n",
      "              ('ISRAEL', 'ISRAELI', 'INDIA', 'INDIAN'),\n",
      "              ('SWITZERLAND', 'SWISS', 'INDIA', 'INDIAN'),\n",
      "              ('PAYING', 'PAID', 'SAYING', 'SAID'),\n",
      "              ('MAN', 'MEN', 'CHILD', 'CHILDREN')],\n",
      "  'incorrect': [('HE', 'SHE', 'HIS', 'HER'),\n",
      "                ('HIS', 'HER', 'HE', 'SHE'),\n",
      "                ('GOOD', 'BETTER', 'LONG', 'LONGER'),\n",
      "                ('GOOD', 'BETTER', 'LOW', 'LOWER'),\n",
      "                ('GREAT', 'GREATER', 'LONG', 'LONGER'),\n",
      "                ('GREAT', 'GREATER', 'LOW', 'LOWER'),\n",
      "                ('GREAT', 'GREATER', 'GOOD', 'BETTER'),\n",
      "                ('LONG', 'LONGER', 'LOW', 'LOWER'),\n",
      "                ('LONG', 'LONGER', 'GOOD', 'BETTER'),\n",
      "                ('LOW', 'LOWER', 'GOOD', 'BETTER'),\n",
      "                ('LOW', 'LOWER', 'GREAT', 'GREATER'),\n",
      "                ('LOW', 'LOWER', 'LONG', 'LONGER'),\n",
      "                ('BIG', 'BIGGEST', 'GOOD', 'BEST'),\n",
      "                ('BIG', 'BIGGEST', 'GREAT', 'GREATEST'),\n",
      "                ('BIG', 'BIGGEST', 'LARGE', 'LARGEST'),\n",
      "                ('GOOD', 'BEST', 'GREAT', 'GREATEST'),\n",
      "                ('GOOD', 'BEST', 'LARGE', 'LARGEST'),\n",
      "                ('GOOD', 'BEST', 'BIG', 'BIGGEST'),\n",
      "                ('GREAT', 'GREATEST', 'BIG', 'BIGGEST'),\n",
      "                ('GREAT', 'GREATEST', 'GOOD', 'BEST'),\n",
      "                ('LARGE', 'LARGEST', 'BIG', 'BIGGEST'),\n",
      "                ('LARGE', 'LARGEST', 'GOOD', 'BEST'),\n",
      "                ('LARGE', 'LARGEST', 'GREAT', 'GREATEST'),\n",
      "                ('GO', 'GOING', 'LOOK', 'LOOKING'),\n",
      "                ('GO', 'GOING', 'PLAY', 'PLAYING'),\n",
      "                ('GO', 'GOING', 'RUN', 'RUNNING'),\n",
      "                ('GO', 'GOING', 'SAY', 'SAYING'),\n",
      "                ('LOOK', 'LOOKING', 'PLAY', 'PLAYING'),\n",
      "                ('LOOK', 'LOOKING', 'RUN', 'RUNNING'),\n",
      "                ('LOOK', 'LOOKING', 'SAY', 'SAYING'),\n",
      "                ('LOOK', 'LOOKING', 'GO', 'GOING'),\n",
      "                ('PLAY', 'PLAYING', 'RUN', 'RUNNING'),\n",
      "                ('PLAY', 'PLAYING', 'GO', 'GOING'),\n",
      "                ('RUN', 'RUNNING', 'SAY', 'SAYING'),\n",
      "                ('RUN', 'RUNNING', 'GO', 'GOING'),\n",
      "                ('RUN', 'RUNNING', 'LOOK', 'LOOKING'),\n",
      "                ('RUN', 'RUNNING', 'PLAY', 'PLAYING'),\n",
      "                ('SAY', 'SAYING', 'GO', 'GOING'),\n",
      "                ('SAY', 'SAYING', 'RUN', 'RUNNING'),\n",
      "                ('AUSTRALIA', 'AUSTRALIAN', 'FRANCE', 'FRENCH'),\n",
      "                ('AUSTRALIA', 'AUSTRALIAN', 'SWITZERLAND', 'SWISS'),\n",
      "                ('FRANCE', 'FRENCH', 'SWITZERLAND', 'SWISS'),\n",
      "                ('FRANCE', 'FRENCH', 'AUSTRALIA', 'AUSTRALIAN'),\n",
      "                ('INDIA', 'INDIAN', 'SWITZERLAND', 'SWISS'),\n",
      "                ('INDIA', 'INDIAN', 'FRANCE', 'FRENCH'),\n",
      "                ('ISRAEL', 'ISRAELI', 'SWITZERLAND', 'SWISS'),\n",
      "                ('ISRAEL', 'ISRAELI', 'AUSTRALIA', 'AUSTRALIAN'),\n",
      "                ('ISRAEL', 'ISRAELI', 'FRANCE', 'FRENCH'),\n",
      "                ('SWITZERLAND', 'SWISS', 'AUSTRALIA', 'AUSTRALIAN'),\n",
      "                ('SWITZERLAND', 'SWISS', 'FRANCE', 'FRENCH'),\n",
      "                ('SWITZERLAND', 'SWISS', 'ISRAEL', 'ISRAELI'),\n",
      "                ('GOING', 'WENT', 'PAYING', 'PAID'),\n",
      "                ('GOING', 'WENT', 'PLAYING', 'PLAYED'),\n",
      "                ('GOING', 'WENT', 'SAYING', 'SAID'),\n",
      "                ('GOING', 'WENT', 'TAKING', 'TOOK'),\n",
      "                ('PAYING', 'PAID', 'PLAYING', 'PLAYED'),\n",
      "                ('PAYING', 'PAID', 'TAKING', 'TOOK'),\n",
      "                ('PAYING', 'PAID', 'GOING', 'WENT'),\n",
      "                ('PLAYING', 'PLAYED', 'SAYING', 'SAID'),\n",
      "                ('PLAYING', 'PLAYED', 'TAKING', 'TOOK'),\n",
      "                ('PLAYING', 'PLAYED', 'GOING', 'WENT'),\n",
      "                ('PLAYING', 'PLAYED', 'PAYING', 'PAID'),\n",
      "                ('SAYING', 'SAID', 'TAKING', 'TOOK'),\n",
      "                ('SAYING', 'SAID', 'GOING', 'WENT'),\n",
      "                ('SAYING', 'SAID', 'PAYING', 'PAID'),\n",
      "                ('SAYING', 'SAID', 'PLAYING', 'PLAYED'),\n",
      "                ('TAKING', 'TOOK', 'GOING', 'WENT'),\n",
      "                ('TAKING', 'TOOK', 'PAYING', 'PAID'),\n",
      "                ('TAKING', 'TOOK', 'PLAYING', 'PLAYED'),\n",
      "                ('TAKING', 'TOOK', 'SAYING', 'SAID'),\n",
      "                ('BUILDING', 'BUILDINGS', 'CAR', 'CARS'),\n",
      "                ('BUILDING', 'BUILDINGS', 'CHILD', 'CHILDREN'),\n",
      "                ('BUILDING', 'BUILDINGS', 'MAN', 'MEN'),\n",
      "                ('CAR', 'CARS', 'CHILD', 'CHILDREN'),\n",
      "                ('CAR', 'CARS', 'MAN', 'MEN'),\n",
      "                ('CAR', 'CARS', 'BUILDING', 'BUILDINGS'),\n",
      "                ('CHILD', 'CHILDREN', 'MAN', 'MEN'),\n",
      "                ('CHILD', 'CHILDREN', 'BUILDING', 'BUILDINGS'),\n",
      "                ('CHILD', 'CHILDREN', 'CAR', 'CARS'),\n",
      "                ('MAN', 'MEN', 'BUILDING', 'BUILDINGS'),\n",
      "                ('MAN', 'MEN', 'CAR', 'CARS')],\n",
      "  'section': 'total'}]\n"
     ]
    }
   ],
   "source": [
    "print(model.accuracy(questions=datapath('questions-words.txt')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Movers distance\n",
    "* Start with two sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "sentence_obama     = 'Obama speaks to the media in Illinois'.lower().split()\n",
    "sentence_president = 'The president greets the press in Chicago'.lower().split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Remove their stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords          = stopwords.words('english')\n",
    "sentence_obama     = [w for w in sentence_obama if w not in stopwords]\n",
    "sentence_president = [w for w in sentence_president if w not in stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [earth mover's distance](https://en.wikipedia.org/wiki/Earth_mover%27s_distance) - computed by the [pyemd](https://pypi.org/project/pyemd/) package:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.389856288520813\n"
     ]
    }
   ],
   "source": [
    "import pyemd\n",
    "distance = model.wv.wmdistance(sentence_obama, sentence_president)\n",
    "print(distance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
