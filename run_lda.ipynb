{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "LDA Model\n",
    "=========\n",
    "\n",
    "Introduces Gensim's LDA model and demonstrates its use on the NIPS corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this tutorial is to demonstrate training an LDA model and\n",
    "obtaining good results.\n",
    "\n",
    "This tutorial will **not**:\n",
    "\n",
    "* Explain how Latent Dirichlet Allocation works\n",
    "* Explain how the LDA model performs inference\n",
    "* Teach you how to use Gensim's LDA implementation in its entirety\n",
    "\n",
    "If you are not familiar with the LDA model or how to use it in Gensim, I\n",
    "suggest you read up on that before continuing with this tutorial.\n",
    "\n",
    "* [Introduction to LDA](http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation)\n",
    "* [Gensim tutorial](sphx_glr_auto_examples_core_run_topics_and_transformations.py)\n",
    "\n",
    "Data\n",
    "----\n",
    "\n",
    "I have used a corpus of NIPS papers in this tutorial, but if you're following\n",
    "this tutorial just to learn about LDA I encourage you to consider picking a\n",
    "corpus on a subject that you are familiar with. Qualitatively evaluating the\n",
    "output of an LDA model is challenging and can require you to understand the\n",
    "subject matter of your corpus (depending on your goal with the model).\n",
    "\n",
    "NIPS (Neural Information Processing Systems) is a machine learning conference\n",
    "so the subject matter should be well suited for most of the target audience\n",
    "of this tutorial.  You can download the original data from Sam Roweis'\n",
    "`website <http://www.cs.nyu.edu/~roweis/data.html>`_.  The code below will\n",
    "also do that for you.\n",
    "\n",
    "The corpus contains 1740 relatively short documents. Keep in mind that this tutorial is not geared towards efficiency, and be careful before applying the code to a large dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-21 21:28:28,079 : WARNING : unable to import 'smart_open.gcs', disabling that module\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import os.path\n",
    "import re\n",
    "import tarfile\n",
    "\n",
    "import smart_open\n",
    "\n",
    "def extract_documents(url='https://cs.nyu.edu/~roweis/data/nips12raw_str602.tgz'):\n",
    "    fname = url.split('/')[-1]\n",
    "    \n",
    "    # Download the file to local storage first.\n",
    "    # We can't read it on the fly because of \n",
    "    # https://github.com/RaRe-Technologies/smart_open/issues/331\n",
    "    if not os.path.isfile(fname):\n",
    "        with smart_open.open(url, \"rb\") as fin:\n",
    "            with smart_open.open(fname, 'wb') as fout:\n",
    "                while True:\n",
    "                    buf = fin.read(io.DEFAULT_BUFFER_SIZE)\n",
    "                    if not buf:\n",
    "                        break\n",
    "                    fout.write(buf)\n",
    "                         \n",
    "    with tarfile.open(fname, mode='r:gz') as tar:\n",
    "        # Ignore directory entries, as well as files like README, etc.\n",
    "        files = [\n",
    "            m for m in tar.getmembers()\n",
    "            if m.isfile() and re.search(r'nipstxt/nips\\d+/\\d+\\.txt', m.name)\n",
    "        ]\n",
    "        for member in sorted(files, key=lambda x: x.name):\n",
    "            member_bytes = tar.extractfile(member).read()\n",
    "            yield member_bytes.decode('utf-8', errors='replace')\n",
    "\n",
    "docs = list(extract_documents())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have a list of 1740 documents, where each document is a Unicode string. \n",
    "If you're thinking about using your own corpus, then you need to make sure\n",
    "that it's in the same format (list of Unicode strings) before proceeding\n",
    "with the rest of this tutorial.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1740\n",
      "1 \n",
      "CONNECTIVITY VERSUS ENTROPY \n",
      "Yaser S. Abu-Mostafa \n",
      "California Institute of Technology \n",
      "Pasadena, CA 91125 \n",
      "ABSTRACT \n",
      "How does the connectivity of a neural network (number of synapses per \n",
      "neuron) relate to the complexity of the problems it can handle (measured by \n",
      "the entropy)? Switching theory would suggest no relation at all, since all Boolean \n",
      "functions can be implemented using a circuit with very low connectivity (e.g., \n",
      "using two-input NAND gates). However, for a network that learns a pr\n"
     ]
    }
   ],
   "source": [
    "print(len(docs))\n",
    "print(docs[0][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-process and vectorize the documents\n",
    "---------------------------------------\n",
    "\n",
    "As part of preprocessing, we will:\n",
    "\n",
    "* Tokenize (split the documents into tokens).\n",
    "* Lemmatize the tokens.\n",
    "* Compute bigrams.\n",
    "* Compute a bag-of-words representation of the data.\n",
    "\n",
    "First we tokenize the text using a regular expression tokenizer from NLTK. We\n",
    "remove numeric tokens and tokens that are only a single character, as they\n",
    "don't tend to be useful, and the dataset contains a lot of them.\n",
    "\n",
    "You can replace NLTK with something else if you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Tokenize the documents.\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Split the documents into tokens.\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "for idx in range(len(docs)):\n",
    "    docs[idx] = docs[idx].lower()  # Convert to lowercase.\n",
    "    docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n",
    "\n",
    "# Remove numbers, but not words that contain numbers.\n",
    "docs = [[token for token in doc if not token.isnumeric()] for doc in docs]\n",
    "\n",
    "# Remove words that are only one character.\n",
    "docs = [[token for token in doc if len(token) > 1] for doc in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We use the **WordNet lemmatizer** from NLTK. A lemmatizer is preferred over a stemmer in this case because it produces more readable words. \n",
    "* Output that is easy to read is very desirable in topic modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Lemmatize the documents.\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We find **bigrams** in the documents. Bigrams are sets of two adjacent words.\n",
    "Using bigrams we can get phrases like \"machine_learning\" in our output\n",
    "(spaces are replaced with underscores); without bigrams we would only get\n",
    "\"machine\" and \"learning\".\n",
    "\n",
    "* Note: below, we find bigrams & add them to the original data - we to keep the words \"machine\" & \"learning\" & (the bigram) \"machine_learning\".\n",
    "\n",
    "* Remember: Computing n-grams of large datasets is computationally & memory intensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-22 07:59:30,363 : INFO : collecting all words and their counts\n",
      "2020-04-22 07:59:30,364 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2020-04-22 07:59:40,361 : INFO : collected 1311757 word types from a corpus of 4953968 words (unigram + bigrams) and 1740 sentences\n",
      "2020-04-22 07:59:40,362 : INFO : using 1311757 counts as vocab in Phrases<0 vocab, min_count=20, threshold=10.0, max_vocab_size=40000000>\n"
     ]
    }
   ],
   "source": [
    "# Compute bigrams.\n",
    "from gensim.models import Phrases\n",
    "\n",
    "# Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n",
    "bigram = Phrases(docs, min_count=20)\n",
    "\n",
    "   \n",
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            docs[idx].append(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Remove rare words and common words based on *document frequency*.\n",
    "* (words that appear in <20 documents or >50% of documents). \n",
    "* Consider trying to remove words only based on their **frequency** too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-22 08:06:05,488 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-04-22 08:06:09,750 : INFO : built Dictionary(79983 unique tokens: ['0a', '2h', '2h2', '2he', '2n']...) from 1740 documents (total 5617423 corpus positions)\n",
      "2020-04-22 08:06:09,895 : INFO : discarding 70969 tokens: [('0a', 19), ('2h', 16), ('2h2', 1), ('2he', 3), ('__c', 2), ('_k', 6), ('a', 1740), ('about', 1058), ('abstract', 1740), ('after', 1087)]...\n",
      "2020-04-22 08:06:09,896 : INFO : keeping 9014 tokens which were in no less than 20 and no more than 870 (=50.0%) documents\n",
      "2020-04-22 08:06:09,935 : INFO : resulting dictionary: Dictionary(9014 unique tokens: ['2n', '_c', 'a2', 'a_follows', 'ability']...)\n"
     ]
    }
   ],
   "source": [
    "# Remove rare and common tokens.\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Create a dictionary representation of the documents.\n",
    "dictionary = Dictionary(docs)\n",
    "\n",
    "# Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Transform documents to vector. \n",
    "* We compute the frequency of each word, including the bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Bag-of-words representation of the documents.\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How many?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 9014\n",
      "Number of documents: 1740\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training\n",
    "--------\n",
    "\n",
    "We are ready to train the LDA model. We will first discuss how to set some of\n",
    "the training parameters.\n",
    "\n",
    "* First: **how many topics do I need?** There is no easy answer for this, it will depend on your data & application. I have used **10 topics here** because I wanted to have a few topics that I could interpret and \"label\", and because that turned out to give me\n",
    "reasonably good results. You might not need to interpret all your topics, so\n",
    "you could use a large number of topics, for example 100.\n",
    "\n",
    "* ``chunksize`` controls how many documents are processed at a time during training. \n",
    "* Bigger chunksizes speed up training, as long as the chunk of documents easily fit into memory.\n",
    "* I've set ``chunksize = 2000``, which is more than the amount of documents, so I process all the data in one go. it can influence model quality. (see Hoffman & co-authors [2].)\n",
    "* ``passes``, aka \"epochs\", controls how often we train the model on the entire corpus.\n",
    "* ``iterations`` controls how often we repeat a particular loop over each document. It is important to set a satisfactory (high enough) #\"passes\" and #\"iterations\".\n",
    "\n",
    "* **How to** choose iterations and passes: \n",
    "    * enable logging, ``eval_every = 1`` in ``LdaModel``. \n",
    "    * When training: review log for something like this::\n",
    "    \n",
    "        ```2016-06-21 15:40:06,753 - gensim.models.ldamodel - DEBUG - 68/1566 documents converged within 400 iterations```\n",
    "    * If you set ``passes = 20`` you will see this line 20 times. \n",
    "    * Make sure that by the final passes, most of the documents have converged. You you want to choose both passes and iterations to be high enough for this to happen.\n",
    "\n",
    "* We set **alpha = 'auto'** and **eta = 'auto'**. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-22 08:20:39,835 : INFO : using autotuned alpha, starting with [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "2020-04-22 08:20:39,838 : INFO : using serial LDA version on this node\n",
      "2020-04-22 08:20:39,854 : INFO : running online (multi-pass) LDA training, 10 topics, 20 passes over the supplied corpus of 1740 documents, updating model once every 1740 documents, evaluating perplexity every 0 documents, iterating 400x with a convergence threshold of 0.001000\n",
      "2020-04-22 08:20:39,856 : INFO : PROGRESS: pass 0, at document #1740/1740\n",
      "2020-04-22 08:20:57,492 : INFO : optimized alpha [0.0693838, 0.10109078, 0.08631849, 0.059373535, 0.08150441, 0.07864842, 0.06427334, 0.08243809, 0.05018346, 0.11289825]\n",
      "2020-04-22 08:20:57,513 : INFO : topic #8 (0.050): 0.004*\"direction\" + 0.003*\"cell\" + 0.003*\"training_set\" + 0.003*\"signal\" + 0.003*\"image\" + 0.002*\"class\" + 0.002*\"map\" + 0.002*\"layer\" + 0.002*\"density\" + 0.002*\"activity\"\n",
      "2020-04-22 08:20:57,515 : INFO : topic #3 (0.059): 0.004*\"circuit\" + 0.003*\"control\" + 0.003*\"training_set\" + 0.003*\"layer\" + 0.003*\"image\" + 0.003*\"dynamic\" + 0.002*\"noise\" + 0.002*\"hidden_unit\" + 0.002*\"speech\" + 0.002*\"field\"\n",
      "2020-04-22 08:20:57,517 : INFO : topic #2 (0.086): 0.005*\"neuron\" + 0.003*\"layer\" + 0.003*\"node\" + 0.003*\"training_set\" + 0.003*\"back_propagation\" + 0.003*\"hidden_unit\" + 0.003*\"control\" + 0.002*\"noise\" + 0.002*\"dynamic\" + 0.002*\"signal\"\n",
      "2020-04-22 08:20:57,518 : INFO : topic #1 (0.101): 0.005*\"neuron\" + 0.005*\"image\" + 0.003*\"field\" + 0.003*\"cell\" + 0.002*\"training_set\" + 0.002*\"hidden_unit\" + 0.002*\"layer\" + 0.002*\"recognition\" + 0.002*\"visual\" + 0.002*\"matrix\"\n",
      "2020-04-22 08:20:57,520 : INFO : topic #9 (0.113): 0.006*\"cell\" + 0.004*\"image\" + 0.003*\"neuron\" + 0.003*\"hidden_unit\" + 0.003*\"response\" + 0.003*\"layer\" + 0.002*\"recognition\" + 0.002*\"hidden\" + 0.002*\"node\" + 0.002*\"visual\"\n",
      "2020-04-22 08:20:57,520 : INFO : topic diff=1.136834, rho=1.000000\n",
      "2020-04-22 08:20:57,537 : INFO : PROGRESS: pass 1, at document #1740/1740\n",
      "2020-04-22 08:21:08,537 : INFO : optimized alpha [0.057728656, 0.06916973, 0.068568416, 0.049053445, 0.06820994, 0.065096326, 0.053674385, 0.06497311, 0.04213778, 0.08643665]\n",
      "2020-04-22 08:21:08,548 : INFO : topic #8 (0.042): 0.006*\"direction\" + 0.003*\"ocular_dominance\" + 0.003*\"cell\" + 0.003*\"map\" + 0.003*\"training_set\" + 0.003*\"image\" + 0.003*\"signal\" + 0.003*\"cost_function\" + 0.002*\"activity\" + 0.002*\"weight_decay\"\n",
      "2020-04-22 08:21:08,550 : INFO : topic #3 (0.049): 0.006*\"circuit\" + 0.004*\"control\" + 0.003*\"speech\" + 0.003*\"field\" + 0.003*\"layer\" + 0.003*\"dynamic\" + 0.003*\"channel\" + 0.003*\"temporal\" + 0.002*\"signal\" + 0.002*\"motion\"\n",
      "2020-04-22 08:21:08,551 : INFO : topic #2 (0.069): 0.004*\"neuron\" + 0.004*\"back_propagation\" + 0.004*\"node\" + 0.003*\"layer\" + 0.003*\"control\" + 0.003*\"gradient_descent\" + 0.003*\"learning_rate\" + 0.003*\"hidden_unit\" + 0.003*\"gradient\" + 0.003*\"dynamic\"\n",
      "2020-04-22 08:21:08,552 : INFO : topic #1 (0.069): 0.007*\"image\" + 0.006*\"neuron\" + 0.004*\"object\" + 0.003*\"field\" + 0.003*\"receptive_field\" + 0.003*\"visual\" + 0.003*\"recognition\" + 0.002*\"layer\" + 0.002*\"matrix\" + 0.002*\"cell\"\n",
      "2020-04-22 08:21:08,553 : INFO : topic #9 (0.086): 0.008*\"cell\" + 0.005*\"neuron\" + 0.004*\"response\" + 0.004*\"image\" + 0.003*\"stimulus\" + 0.003*\"visual\" + 0.003*\"layer\" + 0.003*\"receptive_field\" + 0.003*\"activity\" + 0.002*\"signal\"\n",
      "2020-04-22 08:21:08,554 : INFO : topic diff=0.286157, rho=0.577350\n",
      "2020-04-22 08:21:08,573 : INFO : PROGRESS: pass 2, at document #1740/1740\n",
      "2020-04-22 08:21:16,509 : INFO : optimized alpha [0.05164774, 0.056544174, 0.059019446, 0.043578822, 0.061447598, 0.05908761, 0.046972528, 0.05608455, 0.03729346, 0.07309398]\n",
      "2020-04-22 08:21:16,522 : INFO : topic #8 (0.037): 0.007*\"direction\" + 0.006*\"ocular_dominance\" + 0.004*\"map\" + 0.003*\"cost_function\" + 0.003*\"cell\" + 0.003*\"image\" + 0.003*\"activity\" + 0.003*\"weight_decay\" + 0.002*\"signal\" + 0.002*\"training_set\"\n",
      "2020-04-22 08:21:16,523 : INFO : topic #3 (0.044): 0.007*\"circuit\" + 0.005*\"control\" + 0.004*\"speech\" + 0.003*\"analog_vlsi\" + 0.003*\"field\" + 0.003*\"signal\" + 0.003*\"voltage\" + 0.003*\"motion\" + 0.003*\"channel\" + 0.003*\"sound\"\n",
      "2020-04-22 08:21:16,524 : INFO : topic #2 (0.059): 0.004*\"back_propagation\" + 0.004*\"neuron\" + 0.004*\"node\" + 0.004*\"gradient_descent\" + 0.003*\"learning_rate\" + 0.003*\"layer\" + 0.003*\"gradient\" + 0.003*\"control\" + 0.003*\"fixed_point\" + 0.003*\"hidden_unit\"\n",
      "2020-04-22 08:21:16,526 : INFO : topic #4 (0.061): 0.011*\"hidden_unit\" + 0.007*\"hidden\" + 0.005*\"training_set\" + 0.004*\"recognition\" + 0.003*\"speech\" + 0.003*\"layer\" + 0.003*\"net\" + 0.003*\"back_propagation\" + 0.003*\"mean_field\" + 0.003*\"mixture\"\n",
      "2020-04-22 08:21:16,528 : INFO : topic #9 (0.073): 0.010*\"cell\" + 0.007*\"neuron\" + 0.005*\"response\" + 0.004*\"stimulus\" + 0.004*\"image\" + 0.004*\"visual\" + 0.003*\"receptive_field\" + 0.003*\"activity\" + 0.003*\"spike\" + 0.003*\"synaptic\"\n",
      "2020-04-22 08:21:16,530 : INFO : topic diff=0.255328, rho=0.500000\n",
      "2020-04-22 08:21:16,548 : INFO : PROGRESS: pass 3, at document #1740/1740\n",
      "2020-04-22 08:21:23,626 : INFO : optimized alpha [0.048199344, 0.050300606, 0.05357006, 0.040302817, 0.057240784, 0.05561683, 0.042652436, 0.05127425, 0.034200586, 0.06494872]\n",
      "2020-04-22 08:21:23,638 : INFO : topic #8 (0.034): 0.007*\"direction\" + 0.007*\"ocular_dominance\" + 0.006*\"map\" + 0.004*\"cost_function\" + 0.003*\"activity\" + 0.003*\"ocular\" + 0.003*\"image\" + 0.003*\"dominance\" + 0.003*\"cell\" + 0.003*\"weight_decay\"\n",
      "2020-04-22 08:21:23,640 : INFO : topic #3 (0.040): 0.008*\"circuit\" + 0.005*\"control\" + 0.004*\"analog_vlsi\" + 0.004*\"speech\" + 0.004*\"signal\" + 0.004*\"voltage\" + 0.004*\"motion\" + 0.004*\"field\" + 0.003*\"analog\" + 0.003*\"figure_show\"\n",
      "2020-04-22 08:21:23,641 : INFO : topic #5 (0.056): 0.004*\"training_set\" + 0.004*\"matrix\" + 0.004*\"gaussian\" + 0.004*\"generalization_error\" + 0.004*\"noise\" + 0.003*\"prior\" + 0.003*\"likelihood\" + 0.003*\"hidden_unit\" + 0.003*\"with_respect\" + 0.003*\"component\"\n",
      "2020-04-22 08:21:23,644 : INFO : topic #4 (0.057): 0.011*\"hidden_unit\" + 0.008*\"hidden\" + 0.005*\"recognition\" + 0.005*\"training_set\" + 0.004*\"speech\" + 0.004*\"word\" + 0.004*\"layer\" + 0.003*\"test_set\" + 0.003*\"net\" + 0.003*\"speech_recognition\"\n",
      "2020-04-22 08:21:23,645 : INFO : topic #9 (0.065): 0.011*\"cell\" + 0.009*\"neuron\" + 0.005*\"response\" + 0.004*\"stimulus\" + 0.004*\"visual\" + 0.004*\"spike\" + 0.004*\"activity\" + 0.004*\"receptive_field\" + 0.003*\"synaptic\" + 0.003*\"signal\"\n",
      "2020-04-22 08:21:23,646 : INFO : topic diff=0.230072, rho=0.447214\n",
      "2020-04-22 08:21:23,662 : INFO : PROGRESS: pass 4, at document #1740/1740\n",
      "2020-04-22 08:21:30,128 : INFO : optimized alpha [0.04597513, 0.046528973, 0.050430287, 0.037988815, 0.054887276, 0.05333806, 0.039784618, 0.04820654, 0.032161843, 0.05956994]\n",
      "2020-04-22 08:21:30,140 : INFO : topic #8 (0.032): 0.008*\"ocular_dominance\" + 0.007*\"direction\" + 0.007*\"map\" + 0.004*\"cost_function\" + 0.003*\"activity\" + 0.003*\"self_organizing\" + 0.003*\"ocular\" + 0.003*\"dominance\" + 0.003*\"feature_map\" + 0.003*\"orientation\"\n",
      "2020-04-22 08:21:30,141 : INFO : topic #3 (0.038): 0.009*\"circuit\" + 0.005*\"control\" + 0.005*\"analog_vlsi\" + 0.004*\"signal\" + 0.004*\"voltage\" + 0.004*\"motion\" + 0.004*\"analog\" + 0.004*\"speech\" + 0.004*\"field\" + 0.004*\"figure_show\"\n",
      "2020-04-22 08:21:30,143 : INFO : topic #5 (0.053): 0.005*\"gaussian\" + 0.005*\"matrix\" + 0.004*\"training_set\" + 0.004*\"generalization_error\" + 0.004*\"noise\" + 0.003*\"prior\" + 0.003*\"likelihood\" + 0.003*\"density\" + 0.003*\"component\" + 0.003*\"with_respect\"\n",
      "2020-04-22 08:21:30,145 : INFO : topic #4 (0.055): 0.012*\"hidden_unit\" + 0.008*\"hidden\" + 0.006*\"recognition\" + 0.006*\"training_set\" + 0.005*\"speech\" + 0.004*\"word\" + 0.004*\"layer\" + 0.004*\"test_set\" + 0.004*\"speech_recognition\" + 0.003*\"net\"\n",
      "2020-04-22 08:21:30,147 : INFO : topic #9 (0.060): 0.012*\"cell\" + 0.010*\"neuron\" + 0.006*\"response\" + 0.005*\"stimulus\" + 0.005*\"spike\" + 0.004*\"activity\" + 0.004*\"visual\" + 0.004*\"receptive_field\" + 0.004*\"synaptic\" + 0.003*\"signal\"\n",
      "2020-04-22 08:21:30,149 : INFO : topic diff=0.212926, rho=0.408248\n",
      "2020-04-22 08:21:30,172 : INFO : PROGRESS: pass 5, at document #1740/1740\n",
      "2020-04-22 08:21:36,135 : INFO : optimized alpha [0.044619415, 0.04408665, 0.048688345, 0.036473986, 0.053421486, 0.051862385, 0.037757687, 0.046117585, 0.030801447, 0.055901863]\n",
      "2020-04-22 08:21:36,147 : INFO : topic #8 (0.031): 0.008*\"map\" + 0.008*\"ocular_dominance\" + 0.007*\"direction\" + 0.004*\"cost_function\" + 0.004*\"self_organizing\" + 0.003*\"feature_map\" + 0.003*\"activity\" + 0.003*\"ocular\" + 0.003*\"dominance\" + 0.003*\"orientation\"\n",
      "2020-04-22 08:21:36,149 : INFO : topic #3 (0.036): 0.009*\"circuit\" + 0.006*\"control\" + 0.005*\"analog_vlsi\" + 0.005*\"signal\" + 0.004*\"motion\" + 0.004*\"voltage\" + 0.004*\"analog\" + 0.004*\"figure_show\" + 0.004*\"motor\" + 0.004*\"chip\"\n",
      "2020-04-22 08:21:36,151 : INFO : topic #5 (0.052): 0.005*\"gaussian\" + 0.005*\"matrix\" + 0.004*\"generalization_error\" + 0.004*\"training_set\" + 0.004*\"noise\" + 0.003*\"prior\" + 0.003*\"likelihood\" + 0.003*\"density\" + 0.003*\"component\" + 0.003*\"mixture\"\n",
      "2020-04-22 08:21:36,153 : INFO : topic #4 (0.053): 0.012*\"hidden_unit\" + 0.009*\"hidden\" + 0.006*\"recognition\" + 0.006*\"training_set\" + 0.005*\"speech\" + 0.005*\"word\" + 0.004*\"layer\" + 0.004*\"test_set\" + 0.004*\"speech_recognition\" + 0.004*\"net\"\n",
      "2020-04-22 08:21:36,154 : INFO : topic #9 (0.056): 0.012*\"cell\" + 0.011*\"neuron\" + 0.006*\"response\" + 0.005*\"stimulus\" + 0.005*\"spike\" + 0.005*\"activity\" + 0.004*\"visual\" + 0.004*\"receptive_field\" + 0.004*\"synaptic\" + 0.004*\"firing\"\n",
      "2020-04-22 08:21:36,155 : INFO : topic diff=0.200586, rho=0.377964\n",
      "2020-04-22 08:21:36,172 : INFO : PROGRESS: pass 6, at document #1740/1740\n",
      "2020-04-22 08:21:41,791 : INFO : optimized alpha [0.0437055, 0.042483333, 0.047612112, 0.03532616, 0.052581433, 0.051018775, 0.03625528, 0.044548213, 0.029868525, 0.05337268]\n",
      "2020-04-22 08:21:41,807 : INFO : topic #8 (0.030): 0.009*\"map\" + 0.008*\"ocular_dominance\" + 0.007*\"direction\" + 0.005*\"self_organizing\" + 0.004*\"cost_function\" + 0.004*\"feature_map\" + 0.003*\"activity\" + 0.003*\"dominance\" + 0.003*\"ocular\" + 0.003*\"orientation\"\n",
      "2020-04-22 08:21:41,809 : INFO : topic #3 (0.035): 0.010*\"circuit\" + 0.006*\"control\" + 0.005*\"analog_vlsi\" + 0.005*\"signal\" + 0.005*\"motion\" + 0.005*\"voltage\" + 0.004*\"analog\" + 0.004*\"motor\" + 0.004*\"figure_show\" + 0.004*\"chip\"\n",
      "2020-04-22 08:21:41,810 : INFO : topic #5 (0.051): 0.005*\"gaussian\" + 0.005*\"matrix\" + 0.004*\"noise\" + 0.004*\"generalization_error\" + 0.004*\"training_set\" + 0.004*\"density\" + 0.003*\"likelihood\" + 0.003*\"prior\" + 0.003*\"mixture\" + 0.003*\"component\"\n",
      "2020-04-22 08:21:41,812 : INFO : topic #4 (0.053): 0.012*\"hidden_unit\" + 0.009*\"hidden\" + 0.007*\"recognition\" + 0.006*\"training_set\" + 0.005*\"speech\" + 0.005*\"word\" + 0.004*\"layer\" + 0.004*\"test_set\" + 0.004*\"speech_recognition\" + 0.004*\"trained\"\n",
      "2020-04-22 08:21:41,813 : INFO : topic #9 (0.053): 0.013*\"cell\" + 0.012*\"neuron\" + 0.006*\"response\" + 0.006*\"stimulus\" + 0.005*\"spike\" + 0.005*\"activity\" + 0.004*\"receptive_field\" + 0.004*\"visual\" + 0.004*\"synaptic\" + 0.004*\"firing\"\n",
      "2020-04-22 08:21:41,815 : INFO : topic diff=0.190746, rho=0.353553\n",
      "2020-04-22 08:21:41,833 : INFO : PROGRESS: pass 7, at document #1740/1740\n",
      "2020-04-22 08:21:47,506 : INFO : optimized alpha [0.043271497, 0.041450664, 0.04698655, 0.034522112, 0.05209492, 0.05061069, 0.035140704, 0.04344199, 0.029232727, 0.05156518]\n",
      "2020-04-22 08:21:47,518 : INFO : topic #8 (0.029): 0.010*\"map\" + 0.009*\"ocular_dominance\" + 0.007*\"direction\" + 0.005*\"self_organizing\" + 0.004*\"feature_map\" + 0.004*\"cost_function\" + 0.003*\"activity\" + 0.003*\"dominance\" + 0.003*\"ocular\" + 0.003*\"orientation\"\n",
      "2020-04-22 08:21:47,519 : INFO : topic #3 (0.035): 0.010*\"circuit\" + 0.006*\"control\" + 0.006*\"signal\" + 0.005*\"analog_vlsi\" + 0.005*\"motion\" + 0.005*\"voltage\" + 0.005*\"motor\" + 0.005*\"analog\" + 0.004*\"figure_show\" + 0.004*\"chip\"\n",
      "2020-04-22 08:21:47,521 : INFO : topic #5 (0.051): 0.006*\"gaussian\" + 0.005*\"matrix\" + 0.004*\"noise\" + 0.004*\"generalization_error\" + 0.004*\"density\" + 0.004*\"training_set\" + 0.004*\"likelihood\" + 0.004*\"prior\" + 0.003*\"mixture\" + 0.003*\"component\"\n",
      "2020-04-22 08:21:47,522 : INFO : topic #9 (0.052): 0.013*\"cell\" + 0.012*\"neuron\" + 0.006*\"response\" + 0.006*\"stimulus\" + 0.005*\"spike\" + 0.005*\"activity\" + 0.005*\"receptive_field\" + 0.004*\"visual\" + 0.004*\"synaptic\" + 0.004*\"firing\"\n",
      "2020-04-22 08:21:47,524 : INFO : topic #4 (0.052): 0.013*\"hidden_unit\" + 0.009*\"hidden\" + 0.007*\"recognition\" + 0.006*\"training_set\" + 0.005*\"speech\" + 0.005*\"word\" + 0.005*\"layer\" + 0.004*\"test_set\" + 0.004*\"speech_recognition\" + 0.004*\"trained\"\n",
      "2020-04-22 08:21:47,525 : INFO : topic diff=0.182148, rho=0.333333\n",
      "2020-04-22 08:21:47,544 : INFO : PROGRESS: pass 8, at document #1740/1740\n",
      "2020-04-22 08:21:52,975 : INFO : optimized alpha [0.043087035, 0.040779665, 0.046688132, 0.03391309, 0.051901016, 0.050497632, 0.03426727, 0.04263834, 0.02880565, 0.050231766]\n",
      "2020-04-22 08:21:52,988 : INFO : topic #8 (0.029): 0.011*\"map\" + 0.009*\"ocular_dominance\" + 0.007*\"direction\" + 0.006*\"self_organizing\" + 0.004*\"feature_map\" + 0.004*\"cost_function\" + 0.004*\"activity\" + 0.004*\"self\" + 0.004*\"distance\" + 0.003*\"dominance\"\n",
      "2020-04-22 08:21:52,989 : INFO : topic #3 (0.034): 0.010*\"circuit\" + 0.006*\"control\" + 0.006*\"signal\" + 0.006*\"analog_vlsi\" + 0.005*\"motion\" + 0.005*\"voltage\" + 0.005*\"motor\" + 0.005*\"analog\" + 0.004*\"figure_show\" + 0.004*\"chip\"\n",
      "2020-04-22 08:21:52,990 : INFO : topic #9 (0.050): 0.014*\"cell\" + 0.013*\"neuron\" + 0.007*\"response\" + 0.006*\"stimulus\" + 0.005*\"spike\" + 0.005*\"activity\" + 0.005*\"receptive_field\" + 0.004*\"synaptic\" + 0.004*\"visual\" + 0.004*\"firing\"\n",
      "2020-04-22 08:21:52,992 : INFO : topic #5 (0.050): 0.006*\"gaussian\" + 0.005*\"matrix\" + 0.004*\"noise\" + 0.004*\"density\" + 0.004*\"generalization_error\" + 0.004*\"training_set\" + 0.004*\"likelihood\" + 0.004*\"mixture\" + 0.004*\"prior\" + 0.003*\"component\"\n",
      "2020-04-22 08:21:52,994 : INFO : topic #4 (0.052): 0.013*\"hidden_unit\" + 0.009*\"hidden\" + 0.007*\"recognition\" + 0.006*\"training_set\" + 0.006*\"speech\" + 0.006*\"word\" + 0.005*\"layer\" + 0.005*\"test_set\" + 0.004*\"speech_recognition\" + 0.004*\"trained\"\n",
      "2020-04-22 08:21:52,996 : INFO : topic diff=0.174011, rho=0.316228\n",
      "2020-04-22 08:21:53,013 : INFO : PROGRESS: pass 9, at document #1740/1740\n",
      "2020-04-22 08:21:58,271 : INFO : optimized alpha [0.04313458, 0.040319208, 0.046552047, 0.03348781, 0.051833596, 0.05047128, 0.033590805, 0.042053636, 0.028498901, 0.049269706]\n",
      "2020-04-22 08:21:58,283 : INFO : topic #8 (0.028): 0.011*\"map\" + 0.009*\"ocular_dominance\" + 0.007*\"direction\" + 0.007*\"self_organizing\" + 0.005*\"feature_map\" + 0.004*\"distance\" + 0.004*\"self\" + 0.004*\"cost_function\" + 0.004*\"activity\" + 0.003*\"dominance\"\n",
      "2020-04-22 08:21:58,284 : INFO : topic #3 (0.033): 0.010*\"circuit\" + 0.007*\"control\" + 0.006*\"signal\" + 0.006*\"analog_vlsi\" + 0.006*\"motion\" + 0.005*\"motor\" + 0.005*\"voltage\" + 0.005*\"analog\" + 0.005*\"figure_show\" + 0.005*\"chip\"\n",
      "2020-04-22 08:21:58,285 : INFO : topic #9 (0.049): 0.014*\"cell\" + 0.013*\"neuron\" + 0.007*\"response\" + 0.006*\"stimulus\" + 0.006*\"spike\" + 0.005*\"activity\" + 0.005*\"receptive_field\" + 0.005*\"synaptic\" + 0.004*\"visual\" + 0.004*\"firing\"\n",
      "2020-04-22 08:21:58,286 : INFO : topic #5 (0.050): 0.006*\"gaussian\" + 0.005*\"matrix\" + 0.005*\"noise\" + 0.004*\"density\" + 0.004*\"generalization_error\" + 0.004*\"likelihood\" + 0.004*\"mixture\" + 0.004*\"training_set\" + 0.004*\"prior\" + 0.004*\"component\"\n",
      "2020-04-22 08:21:58,287 : INFO : topic #4 (0.052): 0.013*\"hidden_unit\" + 0.009*\"hidden\" + 0.007*\"recognition\" + 0.007*\"training_set\" + 0.006*\"speech\" + 0.006*\"word\" + 0.005*\"layer\" + 0.005*\"test_set\" + 0.005*\"speech_recognition\" + 0.004*\"trained\"\n",
      "2020-04-22 08:21:58,288 : INFO : topic diff=0.165856, rho=0.301511\n",
      "2020-04-22 08:21:58,305 : INFO : PROGRESS: pass 10, at document #1740/1740\n",
      "2020-04-22 08:22:03,590 : INFO : optimized alpha [0.04330209, 0.04010959, 0.046612468, 0.033175178, 0.05194796, 0.050631456, 0.03305868, 0.04162866, 0.028296692, 0.04856442]\n",
      "2020-04-22 08:22:03,602 : INFO : topic #8 (0.028): 0.012*\"map\" + 0.009*\"ocular_dominance\" + 0.007*\"direction\" + 0.007*\"self_organizing\" + 0.005*\"feature_map\" + 0.004*\"distance\" + 0.004*\"self\" + 0.004*\"activity\" + 0.004*\"rule\" + 0.003*\"orientation\"\n",
      "2020-04-22 08:22:03,604 : INFO : topic #6 (0.033): 0.013*\"reinforcement_learning\" + 0.010*\"action\" + 0.008*\"policy\" + 0.007*\"reinforcement\" + 0.006*\"control\" + 0.006*\"state_space\" + 0.005*\"optimal\" + 0.005*\"dynamic_programming\" + 0.004*\"time_step\" + 0.004*\"machine_learning\"\n",
      "2020-04-22 08:22:03,606 : INFO : topic #9 (0.049): 0.014*\"cell\" + 0.013*\"neuron\" + 0.007*\"response\" + 0.006*\"stimulus\" + 0.006*\"spike\" + 0.005*\"activity\" + 0.005*\"receptive_field\" + 0.005*\"synaptic\" + 0.004*\"visual\" + 0.004*\"firing\"\n",
      "2020-04-22 08:22:03,607 : INFO : topic #5 (0.051): 0.006*\"gaussian\" + 0.005*\"noise\" + 0.005*\"matrix\" + 0.004*\"density\" + 0.004*\"generalization_error\" + 0.004*\"mixture\" + 0.004*\"likelihood\" + 0.004*\"prior\" + 0.004*\"training_set\" + 0.004*\"component\"\n",
      "2020-04-22 08:22:03,608 : INFO : topic #4 (0.052): 0.013*\"hidden_unit\" + 0.009*\"hidden\" + 0.007*\"recognition\" + 0.007*\"training_set\" + 0.006*\"speech\" + 0.006*\"word\" + 0.005*\"layer\" + 0.005*\"test_set\" + 0.005*\"speech_recognition\" + 0.004*\"trained\"\n",
      "2020-04-22 08:22:03,609 : INFO : topic diff=0.157658, rho=0.288675\n",
      "2020-04-22 08:22:03,626 : INFO : PROGRESS: pass 11, at document #1740/1740\n",
      "2020-04-22 08:22:08,827 : INFO : optimized alpha [0.043540552, 0.04002024, 0.046907566, 0.032972403, 0.052181788, 0.05088321, 0.032682925, 0.041309554, 0.028181862, 0.048025236]\n",
      "2020-04-22 08:22:08,839 : INFO : topic #8 (0.028): 0.012*\"map\" + 0.009*\"ocular_dominance\" + 0.007*\"direction\" + 0.007*\"self_organizing\" + 0.005*\"feature_map\" + 0.004*\"distance\" + 0.004*\"self\" + 0.004*\"rule\" + 0.004*\"activity\" + 0.004*\"orientation\"\n",
      "2020-04-22 08:22:08,841 : INFO : topic #6 (0.033): 0.013*\"reinforcement_learning\" + 0.010*\"action\" + 0.008*\"policy\" + 0.007*\"reinforcement\" + 0.007*\"control\" + 0.006*\"state_space\" + 0.005*\"optimal\" + 0.005*\"dynamic_programming\" + 0.005*\"time_step\" + 0.004*\"machine_learning\"\n",
      "2020-04-22 08:22:08,842 : INFO : topic #9 (0.048): 0.014*\"cell\" + 0.014*\"neuron\" + 0.007*\"response\" + 0.006*\"stimulus\" + 0.006*\"spike\" + 0.006*\"activity\" + 0.005*\"receptive_field\" + 0.005*\"synaptic\" + 0.005*\"visual\" + 0.004*\"firing\"\n",
      "2020-04-22 08:22:08,843 : INFO : topic #5 (0.051): 0.006*\"gaussian\" + 0.005*\"noise\" + 0.005*\"density\" + 0.005*\"matrix\" + 0.004*\"mixture\" + 0.004*\"generalization_error\" + 0.004*\"likelihood\" + 0.004*\"prior\" + 0.004*\"component\" + 0.004*\"em_algorithm\"\n",
      "2020-04-22 08:22:08,844 : INFO : topic #4 (0.052): 0.013*\"hidden_unit\" + 0.009*\"hidden\" + 0.008*\"recognition\" + 0.007*\"training_set\" + 0.006*\"speech\" + 0.006*\"word\" + 0.005*\"layer\" + 0.005*\"test_set\" + 0.005*\"speech_recognition\" + 0.005*\"trained\"\n",
      "2020-04-22 08:22:08,844 : INFO : topic diff=0.149431, rho=0.277350\n",
      "2020-04-22 08:22:08,867 : INFO : PROGRESS: pass 12, at document #1740/1740\n",
      "2020-04-22 08:22:13,959 : INFO : optimized alpha [0.043872844, 0.040079948, 0.04727098, 0.032845892, 0.052540116, 0.05118597, 0.032398656, 0.04115899, 0.02817985, 0.04759361]\n",
      "2020-04-22 08:22:13,971 : INFO : topic #8 (0.028): 0.013*\"map\" + 0.009*\"ocular_dominance\" + 0.008*\"self_organizing\" + 0.007*\"direction\" + 0.005*\"feature_map\" + 0.005*\"distance\" + 0.004*\"self\" + 0.004*\"rule\" + 0.004*\"activity\" + 0.004*\"orientation\"\n",
      "2020-04-22 08:22:13,973 : INFO : topic #6 (0.032): 0.013*\"reinforcement_learning\" + 0.010*\"action\" + 0.008*\"policy\" + 0.007*\"reinforcement\" + 0.007*\"control\" + 0.006*\"state_space\" + 0.005*\"optimal\" + 0.005*\"dynamic_programming\" + 0.005*\"time_step\" + 0.004*\"machine_learning\"\n",
      "2020-04-22 08:22:13,974 : INFO : topic #9 (0.048): 0.015*\"cell\" + 0.014*\"neuron\" + 0.007*\"response\" + 0.006*\"stimulus\" + 0.006*\"spike\" + 0.006*\"activity\" + 0.005*\"receptive_field\" + 0.005*\"synaptic\" + 0.005*\"visual\" + 0.004*\"firing\"\n",
      "2020-04-22 08:22:13,975 : INFO : topic #5 (0.051): 0.006*\"gaussian\" + 0.005*\"noise\" + 0.005*\"density\" + 0.005*\"matrix\" + 0.004*\"mixture\" + 0.004*\"likelihood\" + 0.004*\"generalization_error\" + 0.004*\"prior\" + 0.004*\"em_algorithm\" + 0.004*\"component\"\n",
      "2020-04-22 08:22:13,976 : INFO : topic #4 (0.053): 0.013*\"hidden_unit\" + 0.009*\"hidden\" + 0.008*\"recognition\" + 0.007*\"training_set\" + 0.006*\"speech\" + 0.006*\"word\" + 0.005*\"layer\" + 0.005*\"test_set\" + 0.005*\"speech_recognition\" + 0.005*\"trained\"\n",
      "2020-04-22 08:22:13,977 : INFO : topic diff=0.141190, rho=0.267261\n",
      "2020-04-22 08:22:13,995 : INFO : PROGRESS: pass 13, at document #1740/1740\n",
      "2020-04-22 08:22:18,902 : INFO : optimized alpha [0.044257343, 0.040205948, 0.0476994, 0.032767005, 0.05292074, 0.051531877, 0.032200042, 0.041120835, 0.028238632, 0.047258705]\n",
      "2020-04-22 08:22:18,914 : INFO : topic #8 (0.028): 0.013*\"map\" + 0.009*\"ocular_dominance\" + 0.008*\"self_organizing\" + 0.007*\"direction\" + 0.005*\"feature_map\" + 0.005*\"distance\" + 0.005*\"self\" + 0.004*\"rule\" + 0.004*\"activity\" + 0.004*\"represented_by\"\n",
      "2020-04-22 08:22:18,915 : INFO : topic #6 (0.032): 0.013*\"reinforcement_learning\" + 0.010*\"action\" + 0.008*\"policy\" + 0.007*\"reinforcement\" + 0.007*\"control\" + 0.006*\"state_space\" + 0.005*\"optimal\" + 0.005*\"dynamic_programming\" + 0.005*\"time_step\" + 0.004*\"machine_learning\"\n",
      "2020-04-22 08:22:18,916 : INFO : topic #2 (0.048): 0.006*\"gradient_descent\" + 0.006*\"learning_rate\" + 0.006*\"gradient\" + 0.006*\"fixed_point\" + 0.005*\"hidden_unit\" + 0.005*\"back_propagation\" + 0.005*\"dynamic\" + 0.004*\"local_minimum\" + 0.004*\"hidden\" + 0.004*\"solution\"\n",
      "2020-04-22 08:22:18,917 : INFO : topic #5 (0.052): 0.006*\"gaussian\" + 0.005*\"noise\" + 0.005*\"density\" + 0.005*\"matrix\" + 0.004*\"mixture\" + 0.004*\"likelihood\" + 0.004*\"generalization_error\" + 0.004*\"prior\" + 0.004*\"em_algorithm\" + 0.004*\"component\"\n",
      "2020-04-22 08:22:18,918 : INFO : topic #4 (0.053): 0.013*\"hidden_unit\" + 0.010*\"hidden\" + 0.008*\"recognition\" + 0.007*\"training_set\" + 0.006*\"speech\" + 0.006*\"word\" + 0.005*\"layer\" + 0.005*\"test_set\" + 0.005*\"trained\" + 0.005*\"speech_recognition\"\n",
      "2020-04-22 08:22:18,919 : INFO : topic diff=0.133025, rho=0.258199\n",
      "2020-04-22 08:22:18,936 : INFO : PROGRESS: pass 14, at document #1740/1740\n",
      "2020-04-22 08:22:23,858 : INFO : optimized alpha [0.044672444, 0.04037532, 0.048136912, 0.032739095, 0.05332834, 0.05187085, 0.032063343, 0.04114341, 0.028351208, 0.04701727]\n",
      "2020-04-22 08:22:23,875 : INFO : topic #8 (0.028): 0.013*\"map\" + 0.009*\"ocular_dominance\" + 0.008*\"self_organizing\" + 0.007*\"direction\" + 0.005*\"distance\" + 0.005*\"feature_map\" + 0.005*\"self\" + 0.004*\"rule\" + 0.004*\"activity\" + 0.004*\"represented_by\"\n",
      "2020-04-22 08:22:23,876 : INFO : topic #6 (0.032): 0.013*\"reinforcement_learning\" + 0.010*\"action\" + 0.008*\"policy\" + 0.007*\"reinforcement\" + 0.007*\"control\" + 0.006*\"state_space\" + 0.005*\"optimal\" + 0.005*\"dynamic_programming\" + 0.005*\"time_step\" + 0.004*\"machine_learning\"\n",
      "2020-04-22 08:22:23,878 : INFO : topic #2 (0.048): 0.006*\"gradient_descent\" + 0.006*\"learning_rate\" + 0.006*\"gradient\" + 0.006*\"fixed_point\" + 0.006*\"hidden_unit\" + 0.005*\"back_propagation\" + 0.005*\"dynamic\" + 0.004*\"local_minimum\" + 0.004*\"hidden\" + 0.004*\"solution\"\n",
      "2020-04-22 08:22:23,880 : INFO : topic #5 (0.052): 0.006*\"gaussian\" + 0.005*\"density\" + 0.005*\"noise\" + 0.005*\"matrix\" + 0.004*\"mixture\" + 0.004*\"likelihood\" + 0.004*\"generalization_error\" + 0.004*\"prior\" + 0.004*\"em_algorithm\" + 0.004*\"component\"\n",
      "2020-04-22 08:22:23,881 : INFO : topic #4 (0.053): 0.013*\"hidden_unit\" + 0.010*\"hidden\" + 0.008*\"recognition\" + 0.007*\"training_set\" + 0.006*\"speech\" + 0.006*\"word\" + 0.005*\"layer\" + 0.005*\"test_set\" + 0.005*\"trained\" + 0.005*\"speech_recognition\"\n",
      "2020-04-22 08:22:23,882 : INFO : topic diff=0.125059, rho=0.250000\n",
      "2020-04-22 08:22:23,904 : INFO : PROGRESS: pass 15, at document #1740/1740\n",
      "2020-04-22 08:22:28,810 : INFO : optimized alpha [0.045132395, 0.04057692, 0.048623547, 0.03278201, 0.053754296, 0.05223982, 0.031980637, 0.04117459, 0.028533975, 0.04686946]\n",
      "2020-04-22 08:22:28,822 : INFO : topic #8 (0.029): 0.013*\"map\" + 0.009*\"ocular_dominance\" + 0.008*\"self_organizing\" + 0.007*\"direction\" + 0.005*\"distance\" + 0.005*\"feature_map\" + 0.005*\"self\" + 0.005*\"rule\" + 0.004*\"activity\" + 0.004*\"represented_by\"\n",
      "2020-04-22 08:22:28,824 : INFO : topic #6 (0.032): 0.013*\"reinforcement_learning\" + 0.010*\"action\" + 0.008*\"policy\" + 0.007*\"reinforcement\" + 0.007*\"control\" + 0.006*\"state_space\" + 0.005*\"optimal\" + 0.005*\"dynamic_programming\" + 0.005*\"time_step\" + 0.004*\"machine_learning\"\n",
      "2020-04-22 08:22:28,825 : INFO : topic #2 (0.049): 0.007*\"gradient_descent\" + 0.006*\"learning_rate\" + 0.006*\"gradient\" + 0.006*\"fixed_point\" + 0.006*\"hidden_unit\" + 0.005*\"back_propagation\" + 0.005*\"dynamic\" + 0.004*\"local_minimum\" + 0.004*\"hidden\" + 0.004*\"solution\"\n",
      "2020-04-22 08:22:28,827 : INFO : topic #5 (0.052): 0.006*\"gaussian\" + 0.005*\"density\" + 0.005*\"noise\" + 0.005*\"matrix\" + 0.004*\"mixture\" + 0.004*\"likelihood\" + 0.004*\"generalization_error\" + 0.004*\"prior\" + 0.004*\"em_algorithm\" + 0.004*\"component\"\n",
      "2020-04-22 08:22:28,828 : INFO : topic #4 (0.054): 0.013*\"hidden_unit\" + 0.010*\"hidden\" + 0.008*\"recognition\" + 0.007*\"training_set\" + 0.006*\"speech\" + 0.006*\"word\" + 0.005*\"layer\" + 0.005*\"test_set\" + 0.005*\"trained\" + 0.005*\"speech_recognition\"\n",
      "2020-04-22 08:22:28,829 : INFO : topic diff=0.117430, rho=0.242536\n",
      "2020-04-22 08:22:28,847 : INFO : PROGRESS: pass 16, at document #1740/1740\n",
      "2020-04-22 08:22:33,843 : INFO : optimized alpha [0.045633405, 0.04077115, 0.049164675, 0.032868303, 0.054239683, 0.052677274, 0.0319313, 0.041252088, 0.028747588, 0.04677471]\n",
      "2020-04-22 08:22:33,855 : INFO : topic #8 (0.029): 0.014*\"map\" + 0.008*\"ocular_dominance\" + 0.008*\"self_organizing\" + 0.007*\"direction\" + 0.005*\"distance\" + 0.005*\"self\" + 0.005*\"feature_map\" + 0.005*\"rule\" + 0.004*\"activity\" + 0.004*\"represented_by\"\n",
      "2020-04-22 08:22:33,856 : INFO : topic #6 (0.032): 0.014*\"reinforcement_learning\" + 0.010*\"action\" + 0.008*\"policy\" + 0.007*\"control\" + 0.007*\"reinforcement\" + 0.006*\"state_space\" + 0.005*\"optimal\" + 0.005*\"dynamic_programming\" + 0.005*\"time_step\" + 0.004*\"machine_learning\"\n",
      "2020-04-22 08:22:33,858 : INFO : topic #2 (0.049): 0.007*\"gradient_descent\" + 0.006*\"learning_rate\" + 0.006*\"gradient\" + 0.006*\"fixed_point\" + 0.006*\"hidden_unit\" + 0.005*\"back_propagation\" + 0.005*\"dynamic\" + 0.004*\"local_minimum\" + 0.004*\"hidden\" + 0.004*\"solution\"\n",
      "2020-04-22 08:22:33,859 : INFO : topic #5 (0.053): 0.006*\"gaussian\" + 0.005*\"density\" + 0.005*\"noise\" + 0.005*\"matrix\" + 0.004*\"mixture\" + 0.004*\"likelihood\" + 0.004*\"generalization_error\" + 0.004*\"prior\" + 0.004*\"em_algorithm\" + 0.004*\"component\"\n",
      "2020-04-22 08:22:33,860 : INFO : topic #4 (0.054): 0.013*\"hidden_unit\" + 0.010*\"hidden\" + 0.008*\"recognition\" + 0.008*\"training_set\" + 0.006*\"speech\" + 0.006*\"word\" + 0.006*\"layer\" + 0.005*\"test_set\" + 0.005*\"trained\" + 0.005*\"speech_recognition\"\n",
      "2020-04-22 08:22:33,861 : INFO : topic diff=0.110162, rho=0.235702\n",
      "2020-04-22 08:22:33,878 : INFO : PROGRESS: pass 17, at document #1740/1740\n",
      "2020-04-22 08:22:38,796 : INFO : optimized alpha [0.04613882, 0.041031282, 0.0497405, 0.032983605, 0.05477036, 0.05313102, 0.031920478, 0.041395508, 0.028989471, 0.046724964]\n",
      "2020-04-22 08:22:38,809 : INFO : topic #8 (0.029): 0.014*\"map\" + 0.008*\"ocular_dominance\" + 0.008*\"self_organizing\" + 0.007*\"direction\" + 0.005*\"distance\" + 0.005*\"rule\" + 0.005*\"self\" + 0.005*\"feature_map\" + 0.004*\"activity\" + 0.004*\"represented_by\"\n",
      "2020-04-22 08:22:38,810 : INFO : topic #6 (0.032): 0.014*\"reinforcement_learning\" + 0.010*\"action\" + 0.008*\"policy\" + 0.007*\"control\" + 0.007*\"reinforcement\" + 0.006*\"state_space\" + 0.005*\"optimal\" + 0.005*\"dynamic_programming\" + 0.005*\"time_step\" + 0.004*\"machine_learning\"\n",
      "2020-04-22 08:22:38,813 : INFO : topic #2 (0.050): 0.007*\"gradient_descent\" + 0.006*\"learning_rate\" + 0.006*\"gradient\" + 0.006*\"fixed_point\" + 0.006*\"hidden_unit\" + 0.005*\"back_propagation\" + 0.005*\"dynamic\" + 0.004*\"local_minimum\" + 0.004*\"hidden\" + 0.004*\"solution\"\n",
      "2020-04-22 08:22:38,815 : INFO : topic #5 (0.053): 0.006*\"gaussian\" + 0.005*\"density\" + 0.005*\"noise\" + 0.005*\"mixture\" + 0.005*\"matrix\" + 0.004*\"likelihood\" + 0.004*\"prior\" + 0.004*\"em_algorithm\" + 0.004*\"generalization_error\" + 0.004*\"component\"\n",
      "2020-04-22 08:22:38,816 : INFO : topic #4 (0.055): 0.013*\"hidden_unit\" + 0.010*\"hidden\" + 0.008*\"recognition\" + 0.008*\"training_set\" + 0.006*\"speech\" + 0.006*\"word\" + 0.006*\"layer\" + 0.005*\"test_set\" + 0.005*\"trained\" + 0.005*\"speech_recognition\"\n",
      "2020-04-22 08:22:38,817 : INFO : topic diff=0.103283, rho=0.229416\n",
      "2020-04-22 08:22:38,834 : INFO : PROGRESS: pass 18, at document #1740/1740\n",
      "2020-04-22 08:22:43,898 : INFO : optimized alpha [0.046649143, 0.04136852, 0.050349098, 0.033111304, 0.055294894, 0.05359784, 0.03191817, 0.04157928, 0.029274007, 0.04673491]\n",
      "2020-04-22 08:22:43,911 : INFO : topic #8 (0.029): 0.014*\"map\" + 0.008*\"ocular_dominance\" + 0.008*\"self_organizing\" + 0.007*\"direction\" + 0.005*\"distance\" + 0.005*\"rule\" + 0.005*\"self\" + 0.005*\"feature_map\" + 0.004*\"activity\" + 0.004*\"represented_by\"\n",
      "2020-04-22 08:22:43,913 : INFO : topic #6 (0.032): 0.014*\"reinforcement_learning\" + 0.011*\"action\" + 0.008*\"policy\" + 0.007*\"control\" + 0.007*\"reinforcement\" + 0.006*\"state_space\" + 0.005*\"optimal\" + 0.005*\"dynamic_programming\" + 0.005*\"time_step\" + 0.004*\"machine_learning\"\n",
      "2020-04-22 08:22:43,915 : INFO : topic #2 (0.050): 0.007*\"gradient_descent\" + 0.006*\"learning_rate\" + 0.006*\"gradient\" + 0.006*\"fixed_point\" + 0.006*\"hidden_unit\" + 0.005*\"back_propagation\" + 0.005*\"dynamic\" + 0.004*\"local_minimum\" + 0.004*\"hidden\" + 0.004*\"solution\"\n",
      "2020-04-22 08:22:43,917 : INFO : topic #5 (0.054): 0.006*\"gaussian\" + 0.005*\"density\" + 0.005*\"noise\" + 0.005*\"mixture\" + 0.005*\"matrix\" + 0.004*\"likelihood\" + 0.004*\"prior\" + 0.004*\"em_algorithm\" + 0.004*\"component\" + 0.004*\"generalization_error\"\n",
      "2020-04-22 08:22:43,918 : INFO : topic #4 (0.055): 0.013*\"hidden_unit\" + 0.010*\"hidden\" + 0.008*\"recognition\" + 0.008*\"training_set\" + 0.007*\"speech\" + 0.006*\"word\" + 0.006*\"layer\" + 0.005*\"test_set\" + 0.005*\"trained\" + 0.005*\"speech_recognition\"\n",
      "2020-04-22 08:22:43,919 : INFO : topic diff=0.096830, rho=0.223607\n",
      "2020-04-22 08:22:43,937 : INFO : PROGRESS: pass 19, at document #1740/1740\n",
      "2020-04-22 08:22:48,949 : INFO : optimized alpha [0.047163814, 0.04171824, 0.050964013, 0.03326315, 0.055820376, 0.05406804, 0.031930927, 0.04178132, 0.029602412, 0.04677658]\n",
      "2020-04-22 08:22:48,962 : INFO : topic #8 (0.030): 0.014*\"map\" + 0.008*\"ocular_dominance\" + 0.008*\"self_organizing\" + 0.007*\"direction\" + 0.005*\"rule\" + 0.005*\"distance\" + 0.005*\"self\" + 0.005*\"feature_map\" + 0.004*\"activity\" + 0.004*\"represented_by\"\n",
      "2020-04-22 08:22:48,963 : INFO : topic #6 (0.032): 0.014*\"reinforcement_learning\" + 0.011*\"action\" + 0.008*\"policy\" + 0.007*\"control\" + 0.007*\"reinforcement\" + 0.006*\"state_space\" + 0.005*\"optimal\" + 0.005*\"dynamic_programming\" + 0.005*\"time_step\" + 0.004*\"machine_learning\"\n",
      "2020-04-22 08:22:48,964 : INFO : topic #2 (0.051): 0.007*\"gradient_descent\" + 0.006*\"learning_rate\" + 0.006*\"gradient\" + 0.006*\"fixed_point\" + 0.006*\"hidden_unit\" + 0.005*\"dynamic\" + 0.005*\"back_propagation\" + 0.004*\"local_minimum\" + 0.004*\"hidden\" + 0.004*\"solution\"\n",
      "2020-04-22 08:22:48,966 : INFO : topic #5 (0.054): 0.006*\"gaussian\" + 0.005*\"density\" + 0.005*\"noise\" + 0.005*\"mixture\" + 0.005*\"matrix\" + 0.004*\"likelihood\" + 0.004*\"prior\" + 0.004*\"em_algorithm\" + 0.004*\"component\" + 0.004*\"generalization_error\"\n",
      "2020-04-22 08:22:48,967 : INFO : topic #4 (0.056): 0.014*\"hidden_unit\" + 0.010*\"hidden\" + 0.008*\"recognition\" + 0.008*\"training_set\" + 0.007*\"speech\" + 0.006*\"word\" + 0.006*\"layer\" + 0.005*\"test_set\" + 0.005*\"trained\" + 0.005*\"speech_recognition\"\n",
      "2020-04-22 08:22:48,967 : INFO : topic diff=0.090827, rho=0.218218\n"
     ]
    }
   ],
   "source": [
    "# Train LDA model.\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Set training parameters.\n",
    "num_topics = 10\n",
    "chunksize = 2000\n",
    "passes = 20\n",
    "iterations = 400\n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "# Make a index to word dictionary.\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "model = LdaModel(\n",
    "    corpus       =corpus,\n",
    "    id2word      =id2word,\n",
    "    chunksize    =chunksize,\n",
    "    alpha        ='auto',\n",
    "    eta          ='auto',\n",
    "    iterations   =iterations,\n",
    "    num_topics   =num_topics,\n",
    "    passes       =passes,\n",
    "    eval_every   =eval_every\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can compute the **topic coherence** of each topic. Below we display the\n",
    "average topic coherence & print the topics in order of topic coherence.\n",
    "\n",
    "* Note: we use the \"Umass\" topic coherence measure. Gensim has recently\n",
    "obtained an implementation of the \"AKSW\" topic coherence measure (see http://rare-technologies.com/what-is-topic-coherence/).\n",
    "\n",
    "* Share methods on blog at http://rare-technologies.com/lda-training-tips/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-22 08:27:06,050 : INFO : CorpusAccumulator accumulated stats from 1000 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average topic coherence: -1.3123.\n",
      "[([(0.015313651, 'cell'),\n",
      "   (0.0149424905, 'neuron'),\n",
      "   (0.0073651653, 'response'),\n",
      "   (0.0068880743, 'stimulus'),\n",
      "   (0.006190292, 'spike'),\n",
      "   (0.0059941225, 'activity'),\n",
      "   (0.0050231, 'synaptic'),\n",
      "   (0.004933211, 'receptive_field'),\n",
      "   (0.0046267975, 'firing'),\n",
      "   (0.0045634303, 'visual'),\n",
      "   (0.0043524248, 'firing_rate'),\n",
      "   (0.004018963, 'cortex'),\n",
      "   (0.003965662, 'signal'),\n",
      "   (0.0036408643, 'connection'),\n",
      "   (0.0035720216, 'frequency'),\n",
      "   (0.0034900145, 'spike_train'),\n",
      "   (0.0033038252, 'field'),\n",
      "   (0.0031401764, 'fig'),\n",
      "   (0.0031081017, 'cortical'),\n",
      "   (0.0030926934, 'potential')],\n",
      "  -0.977067078652941),\n",
      " ([(0.013519373, 'hidden_unit'),\n",
      "   (0.009738461, 'hidden'),\n",
      "   (0.007895276, 'recognition'),\n",
      "   (0.00785422, 'training_set'),\n",
      "   (0.0065473174, 'speech'),\n",
      "   (0.0061372593, 'word'),\n",
      "   (0.005670263, 'layer'),\n",
      "   (0.0054374444, 'test_set'),\n",
      "   (0.005188131, 'trained'),\n",
      "   (0.004860516, 'speech_recognition'),\n",
      "   (0.0047743507, 'hidden_layer'),\n",
      "   (0.004734251, 'net'),\n",
      "   (0.00431439, 'error_rate'),\n",
      "   (0.0041613462, 'back_propagation'),\n",
      "   (0.0040823375, 'architecture'),\n",
      "   (0.004017538, 'classification'),\n",
      "   (0.003984906, 'class'),\n",
      "   (0.0038957954, 'classifier'),\n",
      "   (0.0034078925, 'hidden_markov'),\n",
      "   (0.0033050557, 'context')],\n",
      "  -1.0416909721850127),\n",
      " ([(0.0063309087, 'gaussian'),\n",
      "   (0.0051279957, 'density'),\n",
      "   (0.005001504, 'noise'),\n",
      "   (0.004657977, 'mixture'),\n",
      "   (0.0045217476, 'matrix'),\n",
      "   (0.0044277054, 'likelihood'),\n",
      "   (0.0040096566, 'prior'),\n",
      "   (0.0040084193, 'em_algorithm'),\n",
      "   (0.0039610863, 'component'),\n",
      "   (0.0039156494, 'generalization_error'),\n",
      "   (0.0036264048, 'mean_field'),\n",
      "   (0.0036012337, 'bayesian'),\n",
      "   (0.0035607328, 'log'),\n",
      "   (0.0035334553, 'estimate'),\n",
      "   (0.0034882266, 'variance'),\n",
      "   (0.0032917538, 'maximum_likelihood'),\n",
      "   (0.0032816615, 'training_set'),\n",
      "   (0.003197591, 'approximation'),\n",
      "   (0.0031884743, 'em'),\n",
      "   (0.0031251137, 'with_respect')],\n",
      "  -1.1001865481320896),\n",
      " ([(0.006800645, 'gradient_descent'),\n",
      "   (0.0064794854, 'learning_rate'),\n",
      "   (0.006390717, 'gradient'),\n",
      "   (0.0058993553, 'fixed_point'),\n",
      "   (0.0058311354, 'hidden_unit'),\n",
      "   (0.0052888966, 'dynamic'),\n",
      "   (0.0052843452, 'back_propagation'),\n",
      "   (0.0044305003, 'local_minimum'),\n",
      "   (0.0042628376, 'hidden'),\n",
      "   (0.004178341, 'solution'),\n",
      "   (0.003840235, 'layer'),\n",
      "   (0.0038216754, 'matrix'),\n",
      "   (0.003525367, 'minimum'),\n",
      "   (0.0034982366, 'convergence'),\n",
      "   (0.00326987, 'hidden_layer'),\n",
      "   (0.003266484, 'descent'),\n",
      "   (0.0031703312, 'neuron'),\n",
      "   (0.0029377493, 'with_respect'),\n",
      "   (0.0028253358, 'nonlinear'),\n",
      "   (0.0027767753, 'dynamical_system')],\n",
      "  -1.1099140236930656),\n",
      " ([(0.026427267, 'image'),\n",
      "   (0.011732349, 'object'),\n",
      "   (0.006030793, 'recognition'),\n",
      "   (0.005177134, 'receptive_field'),\n",
      "   (0.0050812406, 'face'),\n",
      "   (0.0048718182, 'visual'),\n",
      "   (0.004837333, 'pixel'),\n",
      "   (0.0046464037, 'field'),\n",
      "   (0.0039171516, 'view'),\n",
      "   (0.003369718, 'principal_component'),\n",
      "   (0.0032042817, 'position'),\n",
      "   (0.0031304166, 'scale'),\n",
      "   (0.0030598242, 'layer'),\n",
      "   (0.0030344212, 'distance'),\n",
      "   (0.0029623993, 'location'),\n",
      "   (0.002814901, 'human'),\n",
      "   (0.0028082079, 'object_recognition'),\n",
      "   (0.0027833777, 'scene'),\n",
      "   (0.00277445, 'region'),\n",
      "   (0.0027598361, 'vision')],\n",
      "  -1.134407167076415),\n",
      " ([(0.013602426, 'reinforcement_learning'),\n",
      "   (0.0105326, 'action'),\n",
      "   (0.008229447, 'policy'),\n",
      "   (0.0071788747, 'control'),\n",
      "   (0.0070505985, 'reinforcement'),\n",
      "   (0.00590637, 'state_space'),\n",
      "   (0.0053479527, 'optimal'),\n",
      "   (0.005303604, 'dynamic_programming'),\n",
      "   (0.0047553116, 'time_step'),\n",
      "   (0.004037061, 'machine_learning'),\n",
      "   (0.0037887478, 'reward'),\n",
      "   (0.0036038377, 'dynamic'),\n",
      "   (0.003448981, 'environment'),\n",
      "   (0.0033540595, 'goal'),\n",
      "   (0.0033154127, 'robot'),\n",
      "   (0.0032259272, 'decision'),\n",
      "   (0.0030374401, 'markov'),\n",
      "   (0.002891814, 'controller'),\n",
      "   (0.0027926872, 'sutton'),\n",
      "   (0.0027489834, 'td')],\n",
      "  -1.1367359489156943),\n",
      " ([(0.009959406, 'memory'),\n",
      "   (0.009406027, 'neuron'),\n",
      "   (0.007910751, 'neural_net'),\n",
      "   (0.0073591108, 'net'),\n",
      "   (0.0053545157, 'bit'),\n",
      "   (0.0049165394, 'associative_memory'),\n",
      "   (0.0044316007, 'node'),\n",
      "   (0.0041265055, 'chip'),\n",
      "   (0.004119207, 'layer'),\n",
      "   (0.0040618316, 'connection'),\n",
      "   (0.0039224043, 'rule'),\n",
      "   (0.0038083203, 'activation'),\n",
      "   (0.0036720564, 'noise'),\n",
      "   (0.003640096, 'analog'),\n",
      "   (0.0034120756, 'architecture'),\n",
      "   (0.003334328, 'signal'),\n",
      "   (0.0031500955, 'sequence'),\n",
      "   (0.0029902312, 'code'),\n",
      "   (0.0029181757, 'capacity'),\n",
      "   (0.0028898402, 'recurrent_network')],\n",
      "  -1.2340188572424624),\n",
      " ([(0.007574654, 'class'),\n",
      "   (0.0058366717, 'bound'),\n",
      "   (0.0051959944, 'training_set'),\n",
      "   (0.004411354, 'let'),\n",
      "   (0.004125319, 'classifier'),\n",
      "   (0.004046708, 'support_vector'),\n",
      "   (0.0039739567, 'sample'),\n",
      "   (0.0038047256, 'theorem'),\n",
      "   (0.003550174, 'kernel'),\n",
      "   (0.0034792928, 'vc_dimension'),\n",
      "   (0.0034103023, 'nearest_neighbor'),\n",
      "   (0.0033738194, 'tree'),\n",
      "   (0.003316797, 'classification'),\n",
      "   (0.0032572674, 'threshold'),\n",
      "   (0.003123986, 'xi'),\n",
      "   (0.0030617435, 'at_least'),\n",
      "   (0.0030091156, 'dimension'),\n",
      "   (0.0029622237, 'regression'),\n",
      "   (0.0029157435, 'node'),\n",
      "   (0.0028969015, 'upper_bound')],\n",
      "  -1.4183274383551194),\n",
      " ([(0.010505813, 'circuit'),\n",
      "   (0.0078654, 'signal'),\n",
      "   (0.0075434656, 'control'),\n",
      "   (0.006649271, 'motion'),\n",
      "   (0.0062573995, 'analog_vlsi'),\n",
      "   (0.0057377224, 'motor'),\n",
      "   (0.005698194, 'voltage'),\n",
      "   (0.0056804623, 'analog'),\n",
      "   (0.005171063, 'chip'),\n",
      "   (0.005126712, 'figure_show'),\n",
      "   (0.00509752, 'movement'),\n",
      "   (0.004307303, 'real_time'),\n",
      "   (0.004303448, 'frequency'),\n",
      "   (0.0041625816, 'velocity'),\n",
      "   (0.0038538324, 'field'),\n",
      "   (0.0038312688, 'sound'),\n",
      "   (0.0036296484, 'position'),\n",
      "   (0.0034887397, 'filter'),\n",
      "   (0.00346959, 'vlsi'),\n",
      "   (0.0033987463, 'motor_command')],\n",
      "  -1.5932087936002224),\n",
      " ([(0.013999312, 'map'),\n",
      "   (0.008296326, 'ocular_dominance'),\n",
      "   (0.008200621, 'self_organizing'),\n",
      "   (0.006564683, 'direction'),\n",
      "   (0.005442421, 'rule'),\n",
      "   (0.005437244, 'distance'),\n",
      "   (0.005135121, 'self'),\n",
      "   (0.004813078, 'feature_map'),\n",
      "   (0.003941686, 'activity'),\n",
      "   (0.003847141, 'represented_by'),\n",
      "   (0.0037308717, 'mapping'),\n",
      "   (0.0036321178, 'competitive_learning'),\n",
      "   (0.0035872955, 'orientation'),\n",
      "   (0.0035276387, 'lateral_connection'),\n",
      "   (0.0035063298, 'similarity'),\n",
      "   (0.0033736215, 'tangent'),\n",
      "   (0.0033454776, 'dominance'),\n",
      "   (0.0033207573, 'ocular'),\n",
      "   (0.0032521617, 'organizing'),\n",
      "   (0.0031596057, 'kohonen')],\n",
      "  -2.377320385426313)]\n"
     ]
    }
   ],
   "source": [
    "top_topics = model.top_topics(corpus) #, num_words=20)\n",
    "\n",
    "# Average topic coherence is the sum(topic coherences of all topics)/#topics.\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(top_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to experiment with\n",
    "-------------------------\n",
    "\n",
    "* ``no_above`` and ``no_below`` parameters in ``filter_extremes`` method.\n",
    "* Adding trigrams or even higher order n-grams.\n",
    "* Consider whether using a hold-out set or cross-validation is the way to go for you.\n",
    "* Try other datasets.\n",
    "\n",
    "Where to go from here\n",
    "---------------------\n",
    "\n",
    "* Check out a RaRe blog post on the AKSW topic coherence measure (http://rare-technologies.com/what-is-topic-coherence/).\n",
    "* pyLDAvis (https://pyldavis.readthedocs.io/en/latest/index.html).\n",
    "* Read some more Gensim tutorials (https://github.com/RaRe-Technologies/gensim/blob/develop/tutorials.md#tutorials).\n",
    "* If you haven't already, read [1] and [2] (see references).\n",
    "\n",
    "References\n",
    "----------\n",
    "\n",
    "1. \"Latent Dirichlet Allocation\", Blei et al. 2003.\n",
    "2. \"Online Learning for Latent Dirichlet Allocation\", Hoffman et al. 2010.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
