{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "How to Apply Doc2Vec to Reproduce the ['Paragraph Vector' paper](https://arxiv.org/pdf/1405.4053.pdf)\n",
    "==============================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction\n",
    "------------\n",
    "\n",
    "* We will focus on the paper's Section 3.2: \"Beyond One Sentence - Sentiment Analysis with the IMDB dataset\".\n",
    "\n",
    "* Steps: \n",
    "    - 1) Load the IMDB dataset; \n",
    "    - 2) Train a variety of Doc2Vec models on the dataset;\n",
    "    - 3) Evaluate the performance of each model using a logistic regression\n",
    "    - 4) Examine some of the results directly:\n",
    "\n",
    "* When examining results, try to answer the following questions:\n",
    "    - 1) Are inferred vectors close to the precalculated ones?\n",
    "    - 2) Do close documents seem more related than distant ones?\n",
    "    - 3) Do the word vectors show useful similarities?\n",
    "    - 4) Are the word vectors from this dataset any good at analogies?\n",
    "\n",
    "Load [IMDB archive](http://ai.stanford.edu/~amaas/data/sentiment/)\n",
    "-----------\n",
    "\n",
    "* The corpus contains several thousand movie reviews. Each review is a single line of text containing multiple sentences, for example:\n",
    "\n",
    "```\n",
    "One of the best movie-dramas I have ever seen. We do a lot of acting in the\n",
    "church and this is one that can be used as a resource that highlights all the\n",
    "good things that actors can do in their work. I highly recommend this one,\n",
    "especially for those who have an interest in acting, as a \"must see.\"\n",
    "```\n",
    "\n",
    "* These reviews will be the **documents**. There are 100k reviews:\n",
    "\n",
    "#. 25k reviews for training (12.5k positive, 12.5k negative)\n",
    "#. 25k reviews for testing (12.5k positive, 12.5k negative)\n",
    "#. 50k unlabeled reviews\n",
    "\n",
    "* define a convenient datatype for holding data for a single document:\n",
    "\n",
    "* words: The text of the document, as a ``list`` of words.\n",
    "* tags: Used to keep the index of the document in the entire dataset.\n",
    "* split: one of ``train``\\ , ``test`` or ``extra``. Determines how the document will be used (for training, testing, etc).\n",
    "* sentiment: either 1 (positive), 0 (negative) or None (unlabeled document).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "SentimentDocument = collections.namedtuple('SentimentDocument', 'words tags split sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/bjpcjp/Downloads/aclImdb_v1.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!ls ~/Downloads/*.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-22 22:31:19,648 : WARNING : unable to import 'smart_open.gcs', disabling that module\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import re\n",
    "import tarfile\n",
    "import os.path\n",
    "\n",
    "import smart_open\n",
    "import gensim.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aclImdb_v1.tar.gz\n",
      "ok - it's a file.\n",
      "downloading to local storage first.\n"
     ]
    }
   ],
   "source": [
    "#def download_dataset(url='http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'):\n",
    "\n",
    "url   = 'file://~/Downloads/aclImdb_v1.tar.gz'\n",
    "fname = url.split('/')[-1]\n",
    "print(fname)\n",
    "\n",
    "if os.path.isfile(fname):\n",
    "    print(\"ok - it's a file.\")\n",
    "    \n",
    "with smart_open.open(url, 'rb', ignore_ext=True) as fin:\n",
    "    with smart_open.open(fname, 'wb', ignore_ext=True) as fout:\n",
    "        while True:\n",
    "            buf = fin.read(io.DEFAULT_BUFFER_SIZE)\n",
    "            if not buf:\n",
    "                break\n",
    "            fout.write(buf)\n",
    "            \n",
    "print(\"downloading to local storage first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentiment_document(name, text, index):\n",
    "    _, split, sentiment_str, _ = name.split('/')\n",
    "    sentiment                  = {'pos': 1.0, \n",
    "                                  'neg': 0.0, \n",
    "                                  'unsup': None}[sentiment_str]\n",
    "\n",
    "    if sentiment is None:\n",
    "        split = 'extra'\n",
    "\n",
    "    tokens = gensim.utils.to_unicode(text).split()\n",
    "    \n",
    "    return SentimentDocument(tokens, [index], split, sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_documents():\n",
    "    index = 0\n",
    "\n",
    "    with tarfile.open(fname, mode='r:gz') as tar:\n",
    "        for member in tar.getmembers():\n",
    "            if re.match(\n",
    "                r'aclImdb/(train|test)/(pos|neg|unsup)/\\d+_\\d+.txt$', \n",
    "                member.name):\n",
    "                member_bytes = tar.extractfile(member).read()\n",
    "                member_text  = member_bytes.decode('utf-8', errors='replace')\n",
    "\n",
    "                assert member_text.count('\\n') == 0\n",
    "                yield create_sentiment_document(member.name, member_text, index)\n",
    "                index += 1\n",
    "\n",
    "alldocs = list(extract_documents())\n",
    "#print(alldocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentDocument(words=['I', 'was', 'looking', 'forward', 'to', 'this', 'movie.', 'Trustworthy', 'actors,', 'interesting', 'plot.', 'Great', 'atmosphere', 'then', '?????', 'IF', 'you', 'are', 'going', 'to', 'attempt', 'something', 'that', 'is', 'meant', 'to', 'encapsulate', 'the', 'meaning', 'of', 'life.', 'First.', 'Know', 'it.', 'OK', 'I', 'did', 'not', 'expect', 'the', 'directors', 'or', 'writers', 'to', 'actually', 'know', 'the', 'meaning', 'but', 'I', 'thought', 'they', 'may', 'have', 'offered', 'crumbs', 'to', 'peck', 'at', 'and', 'treats', 'to', 'add', 'fuel', 'to', 'the', 'fire-Which!', 'they', 'almost', 'did.', 'Things', 'I', \"didn't\", 'get.', 'A', 'woman', 'wandering', 'around', 'in', 'dark', 'places', 'and', 'lonely', 'car', 'parks', 'alone-oblivious', 'to', 'the', 'consequences.', 'Great', 'riddles', 'that', 'fell', 'by', 'the', 'wayside.', 'The', 'promise', 'of', 'the', 'knowledge', 'therein', 'contained', 'by', 'the', 'original', 'so-called', 'criminal.', 'I', 'had', 'no', 'problem', 'with', 'the', 'budget', 'and', 'enjoyed', 'the', 'suspense.', 'I', 'understood', 'and', 'can', 'wax', 'lyrical', 'about', 'the', 'fool', 'and', 'found', 'Adrian', 'Pauls', 'role', 'crucial', 'and', 'penetrating', 'and', 'then', '?????', 'Basically', 'the', 'story', 'line', 'and', 'the', 'script', 'where', 'good', 'up', 'to', 'a', 'point', 'and', 'that', 'point', 'was', 'the', 'last', '10', 'minutes', 'or', 'so.', 'What?', 'Run', 'out', 'of', 'ideas!', 'Such', 'a', 'pity', 'that', 'this', 'movie', 'had', 'to', 'let', 'us', 'down', 'so', 'badly.', 'It', 'may', 'not', 'comprehend', 'the', 'meaning', 'and', 'I', 'really', 'did', 'not', 'expect', 'the', 'writers', 'to', 'understand', 'it', 'but', 'I', 'was', 'hoping', 'for', 'an', 'intellectual,', 'if', 'not', 'spiritual', 'ride', 'and', 'got', 'a', 'bump', 'in', 'the', 'road'], tags=[27], split='test', sentiment=0.0)\n"
     ]
    }
   ],
   "source": [
    "# single document:\n",
    "print(alldocs[27])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 docs: 25000 train-sentiment, 25000 test-sentiment\n"
     ]
    }
   ],
   "source": [
    "train_docs = [doc for doc in alldocs if doc.split == 'train']\n",
    "test_docs  = [doc for doc in alldocs if doc.split == 'test']\n",
    "\n",
    "print('%d docs: %d train-sentiment, %d test-sentiment' % (len(alldocs), len(train_docs), len(test_docs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set-up Doc2Vec Training & Evaluation Models\n",
    "-------------------------------------------\n",
    "We approximate the experiment of Le & Mikolov: [Distributed Representations\n",
    "of Sentences and Documents](http://cs.stanford.edu/~quocle/paragraph_vector.pdf) with guidance from\n",
    "Mikolov's example [go.sh](https://groups.google.com/d/msg/word2vec-toolkit/Q49FIrNOQRo/J6KG8mUj45sJ):\n",
    "\n",
    "    ./word2vec -train ../alldata-id.txt -output vectors.txt -cbow 0 -size 100 -window 10 -negative 5 -hs 0 -sample 1e-4 -threads 40 -binary 0 -iter 20 -min-count 1 -sentence-vectors 1\n",
    "\n",
    "We vary the following parameters:\n",
    "\n",
    "* 100-dimensional vectors (the 400-d vectors in the paper eat a lot of memory.)\n",
    "* Frequent word subsampling seems to decrease sentiment-prediction accuracy, so it's left out\n",
    "* *cbow=0* means skip-gram which is equivalent to the paper's 'PV-DBOW' mode, matched in gensim with *dm=0*\n",
    "* Added to that DBOW model are two DM models, one which averages context vectors (\\ ``dm_mean``\\ ) and one which concatenates them (\\ ``dm_concat``\\ , resulting in a much larger, slower, more data-hungry model)\n",
    "* *min_count=2* saves model memory, discarding only words that appear in a single doc (and are thus no more expressive than the unique-to-each doc vectors themselves)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from collections import OrderedDict\n",
    "import gensim.models.doc2vec\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1, \"This will be painfully slow otherwise\"\n",
    "from gensim.models.doc2vec import Doc2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-22 22:32:25,179 : INFO : using concatenative 1100-dimensional layer1\n",
      "2020-04-22 22:32:25,180 : INFO : collecting all words and their counts\n",
      "2020-04-22 22:32:25,181 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2020-04-22 22:32:25,604 : INFO : PROGRESS: at example #10000, processed 2292381 words (5426494/s), 150816 word types, 10000 tags\n",
      "2020-04-22 22:32:26,052 : INFO : PROGRESS: at example #20000, processed 4573645 words (5098860/s), 238497 word types, 20000 tags\n",
      "2020-04-22 22:32:26,483 : INFO : PROGRESS: at example #30000, processed 6865575 words (5329145/s), 312348 word types, 30000 tags\n",
      "2020-04-22 22:32:26,941 : INFO : PROGRESS: at example #40000, processed 9190019 words (5079494/s), 377231 word types, 40000 tags\n",
      "2020-04-22 22:32:27,406 : INFO : PROGRESS: at example #50000, processed 11557847 words (5101861/s), 438729 word types, 50000 tags\n",
      "2020-04-22 22:32:27,875 : INFO : PROGRESS: at example #60000, processed 13899883 words (5000756/s), 493913 word types, 60000 tags\n",
      "2020-04-22 22:32:28,347 : INFO : PROGRESS: at example #70000, processed 16270094 words (5036587/s), 548474 word types, 70000 tags\n",
      "2020-04-22 22:32:28,817 : INFO : PROGRESS: at example #80000, processed 18598876 words (4961426/s), 598272 word types, 80000 tags\n",
      "2020-04-22 22:32:29,371 : INFO : PROGRESS: at example #90000, processed 20916044 words (4186580/s), 646082 word types, 90000 tags\n",
      "2020-04-22 22:32:29,852 : INFO : collected 693922 word types and 100000 unique tags from a corpus of 100000 examples and 23279529 words\n",
      "2020-04-22 22:32:29,853 : INFO : Loading a fresh vocabulary\n",
      "2020-04-22 22:32:31,262 : INFO : effective_min_count=2 retains 265408 unique words (38% of original 693922, drops 428514)\n",
      "2020-04-22 22:32:31,263 : INFO : effective_min_count=2 leaves 22851015 word corpus (98% of original 23279529, drops 428514)\n",
      "2020-04-22 22:32:31,990 : INFO : deleting the raw counts dictionary of 693922 items\n",
      "2020-04-22 22:32:32,006 : INFO : sample=0 downsamples 0 most-common words\n",
      "2020-04-22 22:32:32,007 : INFO : downsampling leaves estimated 22851015 word corpus (100.0% of prior 22851015)\n",
      "2020-04-22 22:32:33,002 : INFO : estimated required memory for 265408 words and 100 dimensions: 385030400 bytes\n",
      "2020-04-22 22:32:33,003 : INFO : resetting layer weights\n",
      "2020-04-22 22:33:38,180 : INFO : collecting all words and their counts\n",
      "2020-04-22 22:33:38,181 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec(dbow,d100,n5,mc2,t8) vocabulary scanned & state initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-22 22:33:38,538 : INFO : PROGRESS: at example #10000, processed 2292381 words (6432541/s), 150816 word types, 10000 tags\n",
      "2020-04-22 22:33:38,913 : INFO : PROGRESS: at example #20000, processed 4573645 words (6098394/s), 238497 word types, 20000 tags\n",
      "2020-04-22 22:33:39,298 : INFO : PROGRESS: at example #30000, processed 6865575 words (5961181/s), 312348 word types, 30000 tags\n",
      "2020-04-22 22:33:39,707 : INFO : PROGRESS: at example #40000, processed 9190019 words (5690294/s), 377231 word types, 40000 tags\n",
      "2020-04-22 22:33:40,109 : INFO : PROGRESS: at example #50000, processed 11557847 words (5907289/s), 438729 word types, 50000 tags\n",
      "2020-04-22 22:33:40,509 : INFO : PROGRESS: at example #60000, processed 13899883 words (5871458/s), 493913 word types, 60000 tags\n",
      "2020-04-22 22:33:40,919 : INFO : PROGRESS: at example #70000, processed 16270094 words (5791071/s), 548474 word types, 70000 tags\n",
      "2020-04-22 22:33:41,322 : INFO : PROGRESS: at example #80000, processed 18598876 words (5786457/s), 598272 word types, 80000 tags\n",
      "2020-04-22 22:33:41,725 : INFO : PROGRESS: at example #90000, processed 20916044 words (5763185/s), 646082 word types, 90000 tags\n",
      "2020-04-22 22:33:42,141 : INFO : collected 693922 word types and 100000 unique tags from a corpus of 100000 examples and 23279529 words\n",
      "2020-04-22 22:33:42,142 : INFO : Loading a fresh vocabulary\n",
      "2020-04-22 22:33:43,650 : INFO : effective_min_count=2 retains 265408 unique words (38% of original 693922, drops 428514)\n",
      "2020-04-22 22:33:43,651 : INFO : effective_min_count=2 leaves 22851015 word corpus (98% of original 23279529, drops 428514)\n",
      "2020-04-22 22:33:44,354 : INFO : deleting the raw counts dictionary of 693922 items\n",
      "2020-04-22 22:33:44,366 : INFO : sample=0 downsamples 0 most-common words\n",
      "2020-04-22 22:33:44,367 : INFO : downsampling leaves estimated 22851015 word corpus (100.0% of prior 22851015)\n",
      "2020-04-22 22:33:45,280 : INFO : estimated required memory for 265408 words and 100 dimensions: 385030400 bytes\n",
      "2020-04-22 22:33:45,281 : INFO : resetting layer weights\n",
      "2020-04-22 22:34:52,498 : INFO : collecting all words and their counts\n",
      "2020-04-22 22:34:52,498 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec(\"alpha=0.05\",dm/m,d100,n5,w10,mc2,t8) vocabulary scanned & state initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-22 22:34:52,912 : INFO : PROGRESS: at example #10000, processed 2292381 words (5549752/s), 150816 word types, 10000 tags\n",
      "2020-04-22 22:34:53,440 : INFO : PROGRESS: at example #20000, processed 4573645 words (4328697/s), 238497 word types, 20000 tags\n",
      "2020-04-22 22:34:53,875 : INFO : PROGRESS: at example #30000, processed 6865575 words (5280158/s), 312348 word types, 30000 tags\n",
      "2020-04-22 22:34:54,286 : INFO : PROGRESS: at example #40000, processed 9190019 words (5670123/s), 377231 word types, 40000 tags\n",
      "2020-04-22 22:34:54,687 : INFO : PROGRESS: at example #50000, processed 11557847 words (5912420/s), 438729 word types, 50000 tags\n",
      "2020-04-22 22:34:55,080 : INFO : PROGRESS: at example #60000, processed 13899883 words (5973820/s), 493913 word types, 60000 tags\n",
      "2020-04-22 22:34:55,486 : INFO : PROGRESS: at example #70000, processed 16270094 words (5849092/s), 548474 word types, 70000 tags\n",
      "2020-04-22 22:34:55,917 : INFO : PROGRESS: at example #80000, processed 18598876 words (5408939/s), 598272 word types, 80000 tags\n",
      "2020-04-22 22:34:56,315 : INFO : PROGRESS: at example #90000, processed 20916044 words (5842533/s), 646082 word types, 90000 tags\n",
      "2020-04-22 22:34:56,717 : INFO : collected 693922 word types and 100000 unique tags from a corpus of 100000 examples and 23279529 words\n",
      "2020-04-22 22:34:56,718 : INFO : Loading a fresh vocabulary\n",
      "2020-04-22 22:34:57,739 : INFO : effective_min_count=2 retains 265408 unique words (38% of original 693922, drops 428514)\n",
      "2020-04-22 22:34:57,740 : INFO : effective_min_count=2 leaves 22851015 word corpus (98% of original 23279529, drops 428514)\n",
      "2020-04-22 22:34:58,437 : INFO : deleting the raw counts dictionary of 693922 items\n",
      "2020-04-22 22:34:58,449 : INFO : sample=0 downsamples 0 most-common words\n",
      "2020-04-22 22:34:58,450 : INFO : downsampling leaves estimated 22851015 word corpus (100.0% of prior 22851015)\n",
      "2020-04-22 22:34:59,315 : INFO : estimated required memory for 265408 words and 100 dimensions: 1446662400 bytes\n",
      "2020-04-22 22:34:59,316 : INFO : resetting layer weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec(dm/c,d100,n5,w5,mc2,t8) vocabulary scanned & state initialized\n"
     ]
    }
   ],
   "source": [
    "common_kwargs = dict(\n",
    "    vector_size=100, \n",
    "    epochs=20, \n",
    "    min_count=2,\n",
    "    sample=0, \n",
    "    workers=multiprocessing.cpu_count(), \n",
    "    negative=5, \n",
    "    hs=0,\n",
    ")\n",
    "\n",
    "simple_models = [\n",
    "    # PV-DBOW plain\n",
    "    Doc2Vec(dm=0, **common_kwargs),\n",
    "    \n",
    "    # PV-DM w/ default averaging; a higher starting alpha may improve CBOW/PV-DM modes\n",
    "    Doc2Vec(dm=1, window=10, alpha=0.05, comment='alpha=0.05', **common_kwargs),\n",
    "    \n",
    "    # PV-DM w/ concatenation - big, slow, experimental mode\n",
    "    # window=5 (both sides) approximates paper's apparent 10-word total window size\n",
    "    Doc2Vec(dm=1, dm_concat=1, window=5, **common_kwargs),\n",
    "]\n",
    "\n",
    "for model in simple_models:\n",
    "    model.build_vocab(alldocs)\n",
    "    print(\"%s vocabulary scanned & state initialized\" % model)\n",
    "\n",
    "models_by_name = OrderedDict((str(model), model) for model in simple_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Le and Mikolov: combining a paragraph vector from Distributed Bag of Words (DBOW) and Distributed Memory (DM) improves performance. Let's try pairing the models together for evaluation. \n",
    "* Concatenate the paragraph vectors from each model with the help of a thin wrapper class included in a gensim test module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'testfixtures'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-0367a76d7ac7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_doc2vec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConcatenatedDoc2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodels_by_name\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dbow+dmm'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConcatenatedDoc2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msimple_models\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimple_models\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodels_by_name\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dbow+dmc'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConcatenatedDoc2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msimple_models\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimple_models\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gensim/test/test_doc2vec.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoves\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnamedtuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtestfixtures\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlog_capture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'testfixtures'"
     ]
    }
   ],
   "source": [
    "from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n",
    "models_by_name['dbow+dmm'] = ConcatenatedDoc2Vec([simple_models[0], simple_models[1]])\n",
    "models_by_name['dbow+dmc'] = ConcatenatedDoc2Vec([simple_models[0], simple_models[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictive Evaluation Methods\n",
    "-----------------------------\n",
    "\n",
    "Given a document, our ``Doc2Vec`` models output a vector representation of the document.\n",
    "How useful is a particular model?\n",
    "In case of sentiment analysis, we want the ouput vector to reflect the sentiment in the input document.\n",
    "So, in vector space, positive documents should be distant from negative documents.\n",
    "\n",
    "We train a logistic regression from the training set:\n",
    "\n",
    "  - regressors (inputs): document vectors from the Doc2Vec model\n",
    "  - target (outpus): sentiment labels\n",
    "\n",
    "So, this logistic regression will be able to predict sentiment given a document vector.\n",
    "\n",
    "Next, we test our logistic regression on the test set, and measure the rate of errors (incorrect predictions).\n",
    "If the document vectors from the Doc2Vec model reflect the actual sentiment well, the error rate will be low.\n",
    "\n",
    "Therefore, the error rate of the logistic regression is indication of *how well* the given Doc2Vec model represents documents as vectors.\n",
    "We can then compare different ``Doc2Vec`` models by looking at their error rates.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from random import sample\n",
    "\n",
    "def logistic_predictor_from_data(train_targets, train_regressors):\n",
    "    \"\"\"Fit a statsmodel logistic predictor on supplied data\"\"\"\n",
    "    logit = sm.Logit(train_targets, train_regressors)\n",
    "    predictor = logit.fit(disp=0)\n",
    "    # print(predictor.summary())\n",
    "    return predictor\n",
    "\n",
    "def error_rate_for_model(test_model, train_set, test_set):\n",
    "    \"\"\"Report error rate on test_doc sentiments, using supplied model and train_docs\"\"\"\n",
    "\n",
    "    train_targets = [doc.sentiment for doc in train_set]\n",
    "    train_regressors = [test_model.docvecs[doc.tags[0]] for doc in train_set]\n",
    "    train_regressors = sm.add_constant(train_regressors)\n",
    "    predictor = logistic_predictor_from_data(train_targets, train_regressors)\n",
    "\n",
    "    test_regressors = [test_model.docvecs[doc.tags[0]] for doc in test_set]\n",
    "    test_regressors = sm.add_constant(test_regressors)\n",
    "\n",
    "    # Predict & evaluate\n",
    "    test_predictions = predictor.predict(test_regressors)\n",
    "    corrects = sum(np.rint(test_predictions) == [doc.sentiment for doc in test_set])\n",
    "    errors = len(test_predictions) - corrects\n",
    "    error_rate = float(errors) / len(test_predictions)\n",
    "    return (error_rate, errors, len(test_predictions), predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bulk Training & Per-Model Evaluation\n",
    "------------------------------------\n",
    "\n",
    "Note that doc-vector training is occurring on *all* documents of the dataset,\n",
    "which includes all TRAIN/TEST/DEV docs.  Because the native document-order\n",
    "has similar-sentiment documents in large clumps – which is suboptimal for\n",
    "training – we work with once-shuffled copy of the training set.\n",
    "\n",
    "We evaluate each model's sentiment predictive power based on error rate, and\n",
    "the evaluation is done for each model.\n",
    "\n",
    "(On a 4-core 2.6Ghz Intel Core i7, these 20 passes training and evaluating 3\n",
    "main models takes about an hour.)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "error_rates = defaultdict(lambda: 1.0)  # To selectively print only best errors achieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "shuffled_alldocs = alldocs[:]\n",
    "shuffle(shuffled_alldocs)\n",
    "\n",
    "for model in simple_models:\n",
    "    print(\"Training %s\" % model)\n",
    "    model.train(shuffled_alldocs, total_examples=len(shuffled_alldocs), epochs=model.epochs)\n",
    "\n",
    "    print(\"\\nEvaluating %s\" % model)\n",
    "    err_rate, err_count, test_count, predictor = error_rate_for_model(model, train_docs, test_docs)\n",
    "    error_rates[str(model)] = err_rate\n",
    "    print(\"\\n%f %s\\n\" % (err_rate, model))\n",
    "\n",
    "for model in [models_by_name['dbow+dmm'], models_by_name['dbow+dmc']]:\n",
    "    print(\"\\nEvaluating %s\" % model)\n",
    "    err_rate, err_count, test_count, predictor = error_rate_for_model(model, train_docs, test_docs)\n",
    "    error_rates[str(model)] = err_rate\n",
    "    print(\"\\n%f %s\\n\" % (err_rate, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Achieved Sentiment-Prediction Accuracy\n",
    "--------------------------------------\n",
    "Compare error rates achieved, best-to-worst\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"Err_rate Model\")\n",
    "for rate, name in sorted((rate, name) for name, rate in error_rates.items()):\n",
    "    print(\"%f %s\" % (rate, name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our testing, contrary to the results of the paper, on this problem,\n",
    "PV-DBOW alone performs as good as anything else. Concatenating vectors from\n",
    "different models only sometimes offers a tiny predictive improvement – and\n",
    "stays generally close to the best-performing solo model included.\n",
    "\n",
    "The best results achieved here are just around 10% error rate, still a long\n",
    "way from the paper's reported 7.42% error rate.\n",
    "\n",
    "(Other trials not shown, with larger vectors and other changes, also don't\n",
    "come close to the paper's reported value. Others around the net have reported\n",
    "a similar inability to reproduce the paper's best numbers. The PV-DM/C mode\n",
    "improves a bit with many more training epochs – but doesn't reach parity with\n",
    "PV-DBOW.)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining Results\n",
    "-----------------\n",
    "\n",
    "Let's look for answers to the following questions:\n",
    "\n",
    "#. Are inferred vectors close to the precalculated ones?\n",
    "#. Do close documents seem more related than distant ones?\n",
    "#. Do the word vectors show useful similarities?\n",
    "#. Are the word vectors from this dataset any good at analogies?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are inferred vectors close to the precalculated ones?\n",
    "-----------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "doc_id = np.random.randint(simple_models[0].docvecs.count)  # Pick random doc; re-run cell for more examples\n",
    "print('for doc %d...' % doc_id)\n",
    "for model in simple_models:\n",
    "    inferred_docvec = model.infer_vector(alldocs[doc_id].words)\n",
    "    print('%s:\\n %s' % (model, model.docvecs.most_similar([inferred_docvec], topn=3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Yes, here the stored vector from 20 epochs of training is usually one of the\n",
    "closest to a freshly-inferred vector for the same words. Defaults for\n",
    "inference may benefit from tuning for each dataset or model parameters.)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do close documents seem more related than distant ones?\n",
    "-------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "doc_id = np.random.randint(simple_models[0].docvecs.count)  # pick random doc, re-run cell for more examples\n",
    "model = random.choice(simple_models)  # and a random model\n",
    "sims = model.docvecs.most_similar(doc_id, topn=model.docvecs.count)  # get *all* similar documents\n",
    "print(u'TARGET (%d): «%s»\\n' % (doc_id, ' '.join(alldocs[doc_id].words)))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    s = sims[index]\n",
    "    i = sims[index][0]\n",
    "    words = ' '.join(alldocs[i].words)\n",
    "    print(u'%s %s: «%s»\\n' % (label, s, words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Somewhat, in terms of reviewer tone, movie genre, etc... the MOST\n",
    "cosine-similar docs usually seem more like the TARGET than the MEDIAN or\n",
    "LEAST... especially if the MOST has a cosine-similarity > 0.5. Re-run the\n",
    "cell to try another random target document.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the word vectors show useful similarities?\n",
    "---------------------------------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "word_models = simple_models[:]\n",
    "\n",
    "def pick_random_word(model, threshold=10):\n",
    "    # pick a random word with a suitable number of occurences\n",
    "    while True:\n",
    "        word = random.choice(model.wv.index2word)\n",
    "        if model.wv.vocab[word].count > threshold:\n",
    "            return word\n",
    "\n",
    "target_word = pick_random_word(word_models[0])\n",
    "# or uncomment below line, to just pick a word from the relevant domain:\n",
    "# target_word = 'comedy/drama'\n",
    "\n",
    "for model in word_models:\n",
    "    print('target_word: %r model: %s similar words:' % (target_word, model))\n",
    "    for i, (word, sim) in enumerate(model.wv.most_similar(target_word, topn=10), 1):\n",
    "        print('    %d. %.2f %r' % (i, sim, word))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the DBOW words look meaningless? That's because the gensim DBOW model\n",
    "doesn't train word vectors – they remain at their random initialized values –\n",
    "unless you ask with the ``dbow_words=1`` initialization parameter. Concurrent\n",
    "word-training slows DBOW mode significantly, and offers little improvement\n",
    "(and sometimes a little worsening) of the error rate on this IMDB\n",
    "sentiment-prediction task, but may be appropriate on other tasks, or if you\n",
    "also need word-vectors.\n",
    "\n",
    "Words from DM models tend to show meaningfully similar words when there are\n",
    "many examples in the training data (as with 'plot' or 'actor'). (All DM modes\n",
    "inherently involve word-vector training concurrent with doc-vector training.)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are the word vectors from this dataset any good at analogies?\n",
    "-------------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# grab the file if not already local\n",
    "questions_filename = 'questions-words.txt'\n",
    "if not os.path.isfile(questions_filename):\n",
    "    # Download IMDB archive\n",
    "    print(\"Downloading analogy questions file...\")\n",
    "    url = u'https://raw.githubusercontent.com/tmikolov/word2vec/master/questions-words.txt'\n",
    "    with smart_open.open(url, 'rb') as fin:\n",
    "        with smart_open.open(questions_filename, 'wb') as fout:\n",
    "            fout.write(fin.read())\n",
    "assert os.path.isfile(questions_filename), \"questions-words.txt unavailable\"\n",
    "print(\"Success, questions-words.txt is available for next steps.\")\n",
    "\n",
    "# Note: this analysis takes many minutes\n",
    "for model in word_models:\n",
    "    score, sections = model.wv.evaluate_word_analogies('questions-words.txt')\n",
    "    correct, incorrect = len(sections[-1]['correct']), len(sections[-1]['incorrect'])\n",
    "    print('%s: %0.2f%% correct (%d of %d)' % (model, float(correct*100)/(correct+incorrect), correct, correct+incorrect))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though this is a tiny, domain-specific dataset, it shows some meager\n",
    "capability on the general word analogies – at least for the DM/mean and\n",
    "DM/concat models which actually train word vectors. (The untrained\n",
    "random-initialized words of the DBOW model of course fail miserably.)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
