{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Doc2Vec Model\n",
    "=============\n",
    "\n",
    "Introduces Gensim's Doc2Vec model and demonstrates its use on the Lee Corpus.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doc2Vec is a `core_concepts_model` that represents each\n",
    "`core_concepts_document` as a `core_concepts_vector`.  This\n",
    "tutorial introduces the model and demonstrates how to train and assess it.\n",
    "\n",
    "Here's a list of what we'll be doing:\n",
    "\n",
    "0. Review the relevant models: bag-of-words, Word2Vec, Doc2Vec\n",
    "1. Load and preprocess the training and test corpora (see `core_concepts_corpus`)\n",
    "2. Train a Doc2Vec `core_concepts_model` model using the training corpus\n",
    "3. Demonstrate how the trained model can be used to infer a `core_concepts_vector`\n",
    "4. Assess the model\n",
    "5. Test the model on the test corpus\n",
    "\n",
    "Review: Bag-of-words\n",
    "--------------------\n",
    "\n",
    ".. Note:: Feel free to skip these review sections if you're already familiar with the models.\n",
    "\n",
    "You may be familiar with the `bag-of-words model\n",
    "<https://en.wikipedia.org/wiki/Bag-of-words_model>`_ from the\n",
    "`core_concepts_vector` section.\n",
    "This model transforms each document to a fixed-length vector of integers.\n",
    "For example, given the sentences:\n",
    "\n",
    "- ``John likes to watch movies. Mary likes movies too.``\n",
    "- ``John also likes to watch football games. Mary hates football.``\n",
    "\n",
    "The model outputs the vectors:\n",
    "\n",
    "- ``[1, 2, 1, 1, 2, 1, 1, 0, 0, 0, 0]``\n",
    "- ``[1, 1, 1, 1, 0, 1, 0, 1, 2, 1, 1]``\n",
    "\n",
    "Each vector has 10 elements, where each element counts the number of times a\n",
    "particular word occurred in the document.\n",
    "The order of elements is arbitrary.\n",
    "In the example above, the order of the elements corresponds to the words:\n",
    "``[\"John\", \"likes\", \"to\", \"watch\", \"movies\", \"Mary\", \"too\", \"also\", \"football\", \"games\", \"hates\"]``.\n",
    "\n",
    "Bag-of-words models are surprisingly effective, but have several weaknesses.\n",
    "\n",
    "First, they lose all information about word order: \"John likes Mary\" and\n",
    "\"Mary likes John\" correspond to identical vectors. There is a solution: bag\n",
    "of `n-grams <https://en.wikipedia.org/wiki/N-gram>`__\n",
    "models consider word phrases of length n to represent documents as\n",
    "fixed-length vectors to capture local word order but suffer from data\n",
    "sparsity and high dimensionality.\n",
    "\n",
    "Second, the model does not attempt to learn the meaning of the underlying\n",
    "words, and as a consequence, the distance between vectors doesn't always\n",
    "reflect the difference in meaning.  The ``Word2Vec`` model addresses this\n",
    "second problem.\n",
    "\n",
    "Review: ``Word2Vec`` Model\n",
    "--------------------------\n",
    "\n",
    "``Word2Vec`` is a more recent model that embeds words in a lower-dimensional\n",
    "vector space using a shallow neural network. The result is a set of\n",
    "word-vectors where vectors close together in vector space have similar\n",
    "meanings based on context, and word-vectors distant to each other have\n",
    "differing meanings. For example, ``strong`` and ``powerful`` would be close\n",
    "together and ``strong`` and ``Paris`` would be relatively far.\n",
    "\n",
    "Gensim's :py:class:`~gensim.models.word2vec.Word2Vec` class implements this model.\n",
    "\n",
    "With the ``Word2Vec`` model, we can calculate the vectors for each **word** in a document.\n",
    "But what if we want to calculate a vector for the **entire document**\\ ?\n",
    "We could average the vectors for each word in the document - while this is quick and crude, it can often be useful.\n",
    "However, there is a better way...\n",
    "\n",
    "Introducing: Paragraph Vector\n",
    "-----------------------------\n",
    "\n",
    ".. Important:: In Gensim, we refer to the Paragraph Vector model as ``Doc2Vec``.\n",
    "\n",
    "Le and Mikolov in 2014 introduced the `Doc2Vec algorithm <https://cs.stanford.edu/~quocle/paragraph_vector.pdf>`__, which usually outperforms such simple-averaging of ``Word2Vec`` vectors.\n",
    "\n",
    "The basic idea is: act as if a document has another floating word-like\n",
    "vector, which contributes to all training predictions, and is updated like\n",
    "other word-vectors, but we will call it a doc-vector. Gensim's\n",
    ":py:class:`~gensim.models.doc2vec.Doc2Vec` class implements this algorithm.\n",
    "\n",
    "There are two implementations:\n",
    "\n",
    "1. Paragraph Vector - Distributed Memory (PV-DM)\n",
    "2. Paragraph Vector - Distributed Bag of Words (PV-DBOW)\n",
    "\n",
    ".. Important::\n",
    "  Don't let the implementation details below scare you.\n",
    "  They're advanced material: if it's too much, then move on to the next section.\n",
    "\n",
    "PV-DM is analogous to Word2Vec CBOW. The doc-vectors are obtained by training\n",
    "a neural network on the synthetic task of predicting a center word based an\n",
    "average of both context word-vectors and the full document's doc-vector.\n",
    "\n",
    "PV-DBOW is analogous to Word2Vec SG. The doc-vectors are obtained by training\n",
    "a neural network on the synthetic task of predicting a target word just from\n",
    "the full document's doc-vector. (It is also common to combine this with\n",
    "skip-gram testing, using both the doc-vector and nearby word-vectors to\n",
    "predict a single target word, but only one at a time.)\n",
    "\n",
    "Prepare the Training and Test Data\n",
    "----------------------------------\n",
    "\n",
    "For this tutorial, we'll be training our model using the `Lee Background\n",
    "Corpus\n",
    "<https://hekyll.services.adelaide.edu.au/dspace/bitstream/2440/28910/1/hdl_28910.pdf>`_\n",
    "included in gensim. This corpus contains 314 documents selected from the\n",
    "Australian Broadcasting Corporationâ€™s news mail service, which provides text\n",
    "e-mails of headline stories and covers a number of broad topics.\n",
    "\n",
    "And we'll test our model by eye using the much shorter `Lee Corpus\n",
    "<https://hekyll.services.adelaide.edu.au/dspace/bitstream/2440/28910/1/hdl_28910.pdf>`_\n",
    "which contains 50 documents.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-21 17:11:48,694 : WARNING : unable to import 'smart_open.gcs', disabling that module\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gensim\n",
    "# Set file names for train and test data\n",
    "test_data_dir = os.path.join(gensim.__path__[0], 'test', 'test_data')\n",
    "lee_train_file = os.path.join(test_data_dir, 'lee_background.cor')\n",
    "lee_test_file = os.path.join(test_data_dir, 'lee.cor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alldata-id-10.txt\n",
      "atmodel_3_0_1_model\n",
      "atmodel_3_0_1_model.expElogbeta.npy\n",
      "atmodel_3_0_1_model.id2word\n",
      "atmodel_3_0_1_model.state\n",
      "bgwiki-latest-pages-articles-shortened.xml.bz2\n",
      "compatible-hash-false.model\n",
      "compatible-hash-true.model\n",
      "cp852_fasttext.bin\n",
      "crime-and-punishment.bin\n",
      "crime-and-punishment.txt\n",
      "crime-and-punishment.vec\n",
      "d2v-lee-v0.13.0\n",
      "doc2vec_old\n",
      "doc2vec_old_sep\n",
      "doc2vec_old_sep.syn0_lockf.npy\n",
      "doc2vec_old_sep.syn1neg.npy\n",
      "DTM\n",
      "dtm_test.dict\n",
      "dtm_test.mm\n",
      "EN.1-10.cbow1_wind5_hs0_neg10_size300_smpl1e-05.txt\n",
      "enwiki-latest-pages-articles1.xml-p000000010p000030302-shortened.bz2\n",
      "enwiki-table-markup.xml.bz2\n",
      "euclidean_vectors.bin\n",
      "fasttext_old\n",
      "fasttext_old_sep\n",
      "fasttext_old_sep.syn0_lockf.npy\n",
      "fasttext_old_sep.syn1neg.npy\n",
      "fb-ngrams.txt\n",
      "ft_kv_3.6.0.model.gz\n",
      "ft_model_2.3.0\n",
      "head500.noblanks.cor\n",
      "head500.noblanks.cor.bz2\n",
      "head500.noblanks.cor_tfidf.model\n",
      "head500.noblanks.cor_wordids.txt\n",
      "high_precision.kv.bin\n",
      "high_precision.kv.txt\n",
      "IT.1-10.cbow1_wind5_hs0_neg10_size300_smpl1e-05.txt\n",
      "large_tag_doc_10_iter50\n",
      "lda_3_0_1_model\n",
      "lda_3_0_1_model.expElogbeta.npy\n",
      "lda_3_0_1_model.id2word\n",
      "lda_3_0_1_model.state\n",
      "ldamodel_python_2_7\n",
      "ldamodel_python_2_7.expElogbeta.npy\n",
      "ldamodel_python_2_7.id2word\n",
      "ldamodel_python_2_7.state\n",
      "ldamodel_python_3_5\n",
      "ldamodel_python_3_5.expElogbeta.npy\n",
      "ldamodel_python_3_5.id2word\n",
      "ldamodel_python_3_5.state\n",
      "ldavowpalwabbit.dict.txt\n",
      "ldavowpalwabbit.txt\n",
      "lee_background.cor\n",
      "lee.cor\n",
      "lee_fasttext\n",
      "lee_fasttext.bin\n",
      "lee_fasttext_new.bin\n",
      "lee_fasttext.vec\n",
      "mihalcea_tarau.kwpos.txt\n",
      "mihalcea_tarau.kw.txt\n",
      "mihalcea_tarau.summ.txt\n",
      "mihalcea_tarau.txt\n",
      "miIslita.cor\n",
      "mini_newsgroup\n",
      "nmf_model\n",
      "non_ascii_fasttext.bin\n",
      "old_d2v_models\n",
      "old_keyedvectors_320.dat\n",
      "old_w2v_models\n",
      "OPUS_en_it_europarl_train_one2ten.txt\n",
      "pang_lee_polarity.cor\n",
      "pang_lee_polarity_fasttext.bin\n",
      "pang_lee_polarity_fasttext.vec\n",
      "para2para_text1.txt\n",
      "para2para_text2.txt\n",
      "PathLineSentences\n",
      "phraser-3.6.0.model\n",
      "phraser-no-common-terms.pkl\n",
      "phraser-no-scoring.pkl\n",
      "phraser-scoring-str.pkl\n",
      "phrases-3.6.0.model\n",
      "phrases-no-common-terms.pkl\n",
      "phrases-no-scoring.pkl\n",
      "phrases-scoring-str.pkl\n",
      "phrases-transformer-new-v3-5-0.pkl\n",
      "phrases-transformer-v3-5-0.pkl\n",
      "poincare_cp852.tsv\n",
      "poincare_hypernyms_large.tsv\n",
      "poincare_hypernyms.tsv\n",
      "poincare_test_3.4.0\n",
      "poincare_utf8.tsv\n",
      "poincare_vectors.bin\n",
      "pre_0_13_2_model\n",
      "pre_0_13_2_model.state\n",
      "pretrained.vec\n",
      "questions-words.txt\n",
      "reproduce.dat\n",
      "reproduce.dat.gz\n",
      "similarities0-1.txt\n",
      "simlex999.txt\n",
      "small_tag_doc_5_iter50\n",
      "testcorpus.blei\n",
      "testcorpus.blei.index\n",
      "testcorpus.blei.vocab\n",
      "testcorpus.low\n",
      "testcorpus.low.index\n",
      "testcorpus.mallet\n",
      "testcorpus.mallet.index\n",
      "testcorpus.mm\n",
      "testcorpus.mm.index\n",
      "test_corpus_ok.mm\n",
      "test_corpus_small.mm\n",
      "testcorpus.svmlight\n",
      "testcorpus.svmlight.index\n",
      "testcorpus.txt\n",
      "testcorpus.uci\n",
      "testcorpus.uci.index\n",
      "testcorpus.uci.vocab\n",
      "testcorpus.xml.bz2\n",
      "test_glove.txt\n",
      "testlowdistinctwords.txt\n",
      "test_mmcorpus_corrupt.mm\n",
      "test_mmcorpus_no_index.mm\n",
      "test_mmcorpus_no_index.mm.bz2\n",
      "test_mmcorpus_no_index.mm.gz\n",
      "test_mmcorpus_overflow.mm\n",
      "test_mmcorpus_with_index.mm\n",
      "test_mmcorpus_with_index.mm.index\n",
      "testrepeatedkeywords.txt\n",
      "testsummarization_unrelated.txt\n",
      "tfidf_model_3_2.tst\n",
      "tfidf_model.tst\n",
      "tfidf_model.tst.bz2\n",
      "toy-data.txt\n",
      "toy-model.bin\n",
      "toy-model-pretrained.bin\n",
      "toy-model.vec\n",
      "varembed_lee_subcorpus.cor\n",
      "varembed_morfessor.bin\n",
      "varembed_vectors.pkl\n",
      "w2v_keyedvectors_load_test.modeldata\n",
      "w2v_keyedvectors_load_test.vocab\n",
      "w2v-lee-v0.12.0\n",
      "word2vec_3.3\n",
      "word2vec_old\n",
      "word2vec_old_sep\n",
      "word2vec_old_sep.syn0_lockf.npy\n",
      "word2vec_old_sep.syn1neg.npy\n",
      "word2vec_pre_kv_c\n",
      "word2vec_pre_kv_py2\n",
      "word2vec_pre_kv_py3\n",
      "word2vec_pre_kv_py3_4\n",
      "word2vec_pre_kv_sep_py2\n",
      "word2vec_pre_kv_sep_py2.neg_labels.npy\n",
      "word2vec_pre_kv_sep_py2.syn0_lockf.npy\n",
      "word2vec_pre_kv_sep_py2.syn0.npy\n",
      "word2vec_pre_kv_sep_py2.syn1neg.npy\n",
      "word2vec_pre_kv_sep_py3\n",
      "word2vec_pre_kv_sep_py3_4\n",
      "word2vec_pre_kv_sep_py3_4.neg_labels.npy\n",
      "word2vec_pre_kv_sep_py3_4.syn0_lockf.npy\n",
      "word2vec_pre_kv_sep_py3_4.syn0.npy\n",
      "word2vec_pre_kv_sep_py3_4.syn1neg.npy\n",
      "word2vec_pre_kv_sep_py3.neg_labels.npy\n",
      "word2vec_pre_kv_sep_py3.syn0_lockf.npy\n",
      "word2vec_pre_kv_sep_py3.syn0.npy\n",
      "word2vec_pre_kv_sep_py3.syn1neg.npy\n",
      "wordsim353.tsv\n"
     ]
    }
   ],
   "source": [
    "!ls $test_data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a Function to Read and Preprocess Text\n",
    "---------------------------------------------\n",
    "\n",
    "Below, we define a function to:\n",
    "\n",
    "- open the train/test file (with latin encoding)\n",
    "- read the file line-by-line\n",
    "- pre-process each line (tokenize text into individual words, remove punctuation, set to lowercase, etc)\n",
    "\n",
    "The file we're reading is a **corpus**.\n",
    "Each line of the file is a **document**.\n",
    "\n",
    ".. Important::\n",
    "  To train the model, we'll need to associate a tag/number with each document\n",
    "  of the training corpus. In our case, the tag is simply the zero-based line\n",
    "  number.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import smart_open\n",
    "\n",
    "def read_corpus(fname, tokens_only=False):\n",
    "    with smart_open.open(fname, encoding=\"iso-8859-1\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            tokens = gensim.utils.simple_preprocess(line)\n",
    "            if tokens_only:\n",
    "                yield tokens\n",
    "            else:\n",
    "                # For training data, add tags\n",
    "                yield gensim.models.doc2vec.TaggedDocument(tokens, [i])\n",
    "\n",
    "train_corpus = list(read_corpus(lee_train_file))\n",
    "test_corpus = list(read_corpus(lee_test_file, tokens_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the training corpus\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TaggedDocument(words=['hundreds', 'of', 'people', 'have', 'been', 'forced', 'to', 'vacate', 'their', 'homes', 'in', 'the', 'southern', 'highlands', 'of', 'new', 'south', 'wales', 'as', 'strong', 'winds', 'today', 'pushed', 'huge', 'bushfire', 'towards', 'the', 'town', 'of', 'hill', 'top', 'new', 'blaze', 'near', 'goulburn', 'south', 'west', 'of', 'sydney', 'has', 'forced', 'the', 'closure', 'of', 'the', 'hume', 'highway', 'at', 'about', 'pm', 'aedt', 'marked', 'deterioration', 'in', 'the', 'weather', 'as', 'storm', 'cell', 'moved', 'east', 'across', 'the', 'blue', 'mountains', 'forced', 'authorities', 'to', 'make', 'decision', 'to', 'evacuate', 'people', 'from', 'homes', 'in', 'outlying', 'streets', 'at', 'hill', 'top', 'in', 'the', 'new', 'south', 'wales', 'southern', 'highlands', 'an', 'estimated', 'residents', 'have', 'left', 'their', 'homes', 'for', 'nearby', 'mittagong', 'the', 'new', 'south', 'wales', 'rural', 'fire', 'service', 'says', 'the', 'weather', 'conditions', 'which', 'caused', 'the', 'fire', 'to', 'burn', 'in', 'finger', 'formation', 'have', 'now', 'eased', 'and', 'about', 'fire', 'units', 'in', 'and', 'around', 'hill', 'top', 'are', 'optimistic', 'of', 'defending', 'all', 'properties', 'as', 'more', 'than', 'blazes', 'burn', 'on', 'new', 'year', 'eve', 'in', 'new', 'south', 'wales', 'fire', 'crews', 'have', 'been', 'called', 'to', 'new', 'fire', 'at', 'gunning', 'south', 'of', 'goulburn', 'while', 'few', 'details', 'are', 'available', 'at', 'this', 'stage', 'fire', 'authorities', 'says', 'it', 'has', 'closed', 'the', 'hume', 'highway', 'in', 'both', 'directions', 'meanwhile', 'new', 'fire', 'in', 'sydney', 'west', 'is', 'no', 'longer', 'threatening', 'properties', 'in', 'the', 'cranebrook', 'area', 'rain', 'has', 'fallen', 'in', 'some', 'parts', 'of', 'the', 'illawarra', 'sydney', 'the', 'hunter', 'valley', 'and', 'the', 'north', 'coast', 'but', 'the', 'bureau', 'of', 'meteorology', 'claire', 'richards', 'says', 'the', 'rain', 'has', 'done', 'little', 'to', 'ease', 'any', 'of', 'the', 'hundred', 'fires', 'still', 'burning', 'across', 'the', 'state', 'the', 'falls', 'have', 'been', 'quite', 'isolated', 'in', 'those', 'areas', 'and', 'generally', 'the', 'falls', 'have', 'been', 'less', 'than', 'about', 'five', 'millimetres', 'she', 'said', 'in', 'some', 'places', 'really', 'not', 'significant', 'at', 'all', 'less', 'than', 'millimetre', 'so', 'there', 'hasn', 'been', 'much', 'relief', 'as', 'far', 'as', 'rain', 'is', 'concerned', 'in', 'fact', 'they', 've', 'probably', 'hampered', 'the', 'efforts', 'of', 'the', 'firefighters', 'more', 'because', 'of', 'the', 'wind', 'gusts', 'that', 'are', 'associated', 'with', 'those', 'thunderstorms'], tags=[0]), TaggedDocument(words=['indian', 'security', 'forces', 'have', 'shot', 'dead', 'eight', 'suspected', 'militants', 'in', 'night', 'long', 'encounter', 'in', 'southern', 'kashmir', 'the', 'shootout', 'took', 'place', 'at', 'dora', 'village', 'some', 'kilometers', 'south', 'of', 'the', 'kashmiri', 'summer', 'capital', 'srinagar', 'the', 'deaths', 'came', 'as', 'pakistani', 'police', 'arrested', 'more', 'than', 'two', 'dozen', 'militants', 'from', 'extremist', 'groups', 'accused', 'of', 'staging', 'an', 'attack', 'on', 'india', 'parliament', 'india', 'has', 'accused', 'pakistan', 'based', 'lashkar', 'taiba', 'and', 'jaish', 'mohammad', 'of', 'carrying', 'out', 'the', 'attack', 'on', 'december', 'at', 'the', 'behest', 'of', 'pakistani', 'military', 'intelligence', 'military', 'tensions', 'have', 'soared', 'since', 'the', 'raid', 'with', 'both', 'sides', 'massing', 'troops', 'along', 'their', 'border', 'and', 'trading', 'tit', 'for', 'tat', 'diplomatic', 'sanctions', 'yesterday', 'pakistan', 'announced', 'it', 'had', 'arrested', 'lashkar', 'taiba', 'chief', 'hafiz', 'mohammed', 'saeed', 'police', 'in', 'karachi', 'say', 'it', 'is', 'likely', 'more', 'raids', 'will', 'be', 'launched', 'against', 'the', 'two', 'groups', 'as', 'well', 'as', 'other', 'militant', 'organisations', 'accused', 'of', 'targetting', 'india', 'military', 'tensions', 'between', 'india', 'and', 'pakistan', 'have', 'escalated', 'to', 'level', 'not', 'seen', 'since', 'their', 'war'], tags=[1])]\n"
     ]
    }
   ],
   "source": [
    "print(train_corpus[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the testing corpus looks like this:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the', 'national', 'executive', 'of', 'the', 'strife', 'torn', 'democrats', 'last', 'night', 'appointed', 'little', 'known', 'west', 'australian', 'senator', 'brian', 'greig', 'as', 'interim', 'leader', 'shock', 'move', 'likely', 'to', 'provoke', 'further', 'conflict', 'between', 'the', 'party', 'senators', 'and', 'its', 'organisation', 'in', 'move', 'to', 'reassert', 'control', 'over', 'the', 'party', 'seven', 'senators', 'the', 'national', 'executive', 'last', 'night', 'rejected', 'aden', 'ridgeway', 'bid', 'to', 'become', 'interim', 'leader', 'in', 'favour', 'of', 'senator', 'greig', 'supporter', 'of', 'deposed', 'leader', 'natasha', 'stott', 'despoja', 'and', 'an', 'outspoken', 'gay', 'rights', 'activist'], ['cash', 'strapped', 'financial', 'services', 'group', 'amp', 'has', 'shelved', 'million', 'plan', 'to', 'buy', 'shares', 'back', 'from', 'investors', 'and', 'will', 'raise', 'million', 'in', 'fresh', 'capital', 'after', 'profits', 'crashed', 'in', 'the', 'six', 'months', 'to', 'june', 'chief', 'executive', 'paul', 'batchelor', 'said', 'the', 'result', 'was', 'solid', 'in', 'what', 'he', 'described', 'as', 'the', 'worst', 'conditions', 'for', 'stock', 'markets', 'in', 'years', 'amp', 'half', 'year', 'profit', 'sank', 'per', 'cent', 'to', 'million', 'or', 'share', 'as', 'australia', 'largest', 'investor', 'and', 'fund', 'manager', 'failed', 'to', 'hit', 'projected', 'per', 'cent', 'earnings', 'growth', 'targets', 'and', 'was', 'battered', 'by', 'falling', 'returns', 'on', 'share', 'markets']]\n"
     ]
    }
   ],
   "source": [
    "print(test_corpus[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the testing corpus is just a list of lists and does not contain\n",
    "any tags.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Model\n",
    "------------------\n",
    "\n",
    "Now, we'll instantiate a Doc2Vec model with a vector size with 50 dimensions and\n",
    "iterating over the training corpus 40 times. We set the minimum word count to\n",
    "2 in order to discard words with very few occurrences. (Without a variety of\n",
    "representative examples, retaining such infrequent words can often make a\n",
    "model worse!) Typical iteration counts in the published `Paragraph Vector paper <https://cs.stanford.edu/~quocle/paragraph_vector.pdf>`__\n",
    "results, using 10s-of-thousands to millions of docs, are 10-20. More\n",
    "iterations take more time and eventually reach a point of diminishing\n",
    "returns.\n",
    "\n",
    "However, this is a very very small dataset (300 documents) with shortish\n",
    "documents (a few hundred words). Adding training passes can sometimes help\n",
    "with such small datasets.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-21 17:30:01,081 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a vocabulary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-21 17:30:06,447 : INFO : collecting all words and their counts\n",
      "2020-04-21 17:30:06,449 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2020-04-21 17:30:06,471 : INFO : collected 6981 word types and 300 unique tags from a corpus of 300 examples and 58152 words\n",
      "2020-04-21 17:30:06,472 : INFO : Loading a fresh vocabulary\n",
      "2020-04-21 17:30:06,486 : INFO : effective_min_count=2 retains 3955 unique words (56% of original 6981, drops 3026)\n",
      "2020-04-21 17:30:06,487 : INFO : effective_min_count=2 leaves 55126 word corpus (94% of original 58152, drops 3026)\n",
      "2020-04-21 17:30:06,499 : INFO : deleting the raw counts dictionary of 6981 items\n",
      "2020-04-21 17:30:06,500 : INFO : sample=0.001 downsamples 46 most-common words\n",
      "2020-04-21 17:30:06,501 : INFO : downsampling leaves estimated 42390 word corpus (76.9% of prior 55126)\n",
      "2020-04-21 17:30:06,511 : INFO : estimated required memory for 3955 words and 50 dimensions: 3619500 bytes\n",
      "2020-04-21 17:30:06,512 : INFO : resetting layer weights\n"
     ]
    }
   ],
   "source": [
    "model.build_vocab(train_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, the vocabulary is a dictionary (accessible via\n",
    "``model.wv.vocab``\\ ) of all of the unique words extracted from the training\n",
    "corpus along with the count (e.g., ``model.wv.vocab['penalty'].count`` for\n",
    "counts for the word ``penalty``\\ ).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, train the model on the corpus.\n",
    "If the BLAS library is being used, this should take no more than 3 seconds.\n",
    "If the BLAS library is not being used, this should take no more than 2\n",
    "minutes, so use BLAS if you value your time.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-21 17:30:23,925 : INFO : training model with 3 workers on 3955 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2020-04-21 17:30:23,987 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-21 17:30:23,989 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-21 17:30:23,990 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-21 17:30:23,991 : INFO : EPOCH - 1 : training on 58152 raw words (42595 effective words) took 0.1s, 684674 effective words/s\n",
      "2020-04-21 17:30:24,049 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-21 17:30:24,051 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-21 17:30:24,053 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-21 17:30:24,053 : INFO : EPOCH - 2 : training on 58152 raw words (42671 effective words) took 0.1s, 719702 effective words/s\n",
      "2020-04-21 17:30:24,102 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-21 17:30:24,113 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-21 17:30:24,116 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-21 17:30:24,117 : INFO : EPOCH - 3 : training on 58152 raw words (42693 effective words) took 0.1s, 684214 effective words/s\n",
      "2020-04-21 17:30:24,170 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-21 17:30:24,173 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-21 17:30:24,175 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-21 17:30:24,176 : INFO : EPOCH - 4 : training on 58152 raw words (42726 effective words) took 0.1s, 752682 effective words/s\n",
      "2020-04-21 17:30:24,225 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-21 17:30:24,226 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-21 17:30:24,228 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-21 17:30:24,229 : INFO : EPOCH - 5 : training on 58152 raw words (42702 effective words) took 0.0s, 857853 effective words/s\n",
      "2020-04-21 17:30:24,284 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-21 17:30:24,288 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-21 17:30:24,290 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-21 17:30:24,291 : INFO : EPOCH - 6 : training on 58152 raw words (42751 effective words) took 0.1s, 717451 effective words/s\n",
      "2020-04-21 17:30:24,347 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-21 17:30:24,348 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-21 17:30:24,349 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-21 17:30:24,350 : INFO : EPOCH - 7 : training on 58152 raw words (42625 effective words) took 0.1s, 761960 effective words/s\n",
      "2020-04-21 17:30:24,407 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-21 17:30:24,410 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-21 17:30:24,413 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-21 17:30:24,414 : INFO : EPOCH - 8 : training on 58152 raw words (42758 effective words) took 0.1s, 689994 effective words/s\n",
      "2020-04-21 17:30:24,470 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-21 17:30:24,474 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-21 17:30:24,475 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-21 17:30:24,476 : INFO : EPOCH - 9 : training on 58152 raw words (42634 effective words) took 0.1s, 716533 effective words/s\n",
      "2020-04-21 17:30:24,526 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-21 17:30:24,535 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-21 17:30:24,537 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-21 17:30:24,537 : INFO : EPOCH - 10 : training on 58152 raw words (42657 effective words) took 0.1s, 713581 effective words/s\n",
      "2020-04-21 17:30:24,587 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-21 17:30:24,594 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-21 17:30:24,597 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-21 17:30:24,597 : INFO : EPOCH - 11 : training on 58152 raw words (42693 effective words) took 0.1s, 745625 effective words/s\n",
      "2020-04-21 17:30:24,650 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-21 17:30:24,653 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-21 17:30:24,655 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-21 17:30:24,656 : INFO : EPOCH - 12 : training on 58152 raw words (42692 effective words) took 0.1s, 747445 effective words/s\n",
      "2020-04-21 17:30:24,717 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-21 17:30:24,720 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-21 17:30:24,725 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-21 17:30:24,726 : INFO : EPOCH - 13 : training on 58152 raw words (42687 effective words) took 0.1s, 628870 effective words/s\n",
      "2020-04-21 17:30:24,776 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-21 17:30:24,780 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-21 17:30:24,782 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-21 17:30:24,782 : INFO : EPOCH - 14 : training on 58152 raw words (42679 effective words) took 0.1s, 789493 effective words/s\n",
      "2020-04-21 17:30:24,831 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-21 17:30:24,832 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-21 17:30:24,833 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-21 17:30:24,834 : INFO : EPOCH - 15 : training on 58152 raw words (42722 effective words) took 0.0s, 879743 effective words/s\n",
      "2020-04-21 17:30:24,892 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-21 17:30:24,896 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-21 17:30:24,898 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-21 17:30:24,898 : INFO : EPOCH - 16 : training on 58152 raw words (42782 effective words) took 0.1s, 686714 effective words/s\n",
      "2020-04-21 17:30:24,957 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-21 17:30:24,959 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-21 17:30:24,962 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-21 17:30:24,962 : INFO : EPOCH - 17 : training on 58152 raw words (42712 effective words) took 0.1s, 690826 effective words/s\n",
      "2020-04-21 17:30:25,017 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-21 17:30:25,019 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-21 17:30:25,023 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-21 17:30:25,023 : INFO : EPOCH - 18 : training on 58152 raw words (42705 effective words) took 0.1s, 721050 effective words/s\n",
      "2020-04-21 17:30:25,079 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-21 17:30:25,080 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-21 17:30:25,083 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-21 17:30:25,084 : INFO : EPOCH - 19 : training on 58152 raw words (42613 effective words) took 0.1s, 726523 effective words/s\n",
      "2020-04-21 17:30:25,138 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-21 17:30:25,143 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-21 17:30:25,145 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-21 17:30:25,145 : INFO : EPOCH - 20 : training on 58152 raw words (42637 effective words) took 0.1s, 722627 effective words/s\n",
      "2020-04-21 17:30:25,200 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-21 17:30:25,201 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-21 17:30:25,205 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-21 17:30:25,206 : INFO : EPOCH - 21 : training on 58152 raw words (42702 effective words) took 0.1s, 728863 effective words/s\n",
      "2020-04-21 17:30:25,262 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-21 17:30:25,266 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-21 17:30:25,267 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-21 17:30:25,267 : INFO : EPOCH - 22 : training on 58152 raw words (42715 effective words) took 0.1s, 717633 effective words/s\n",
      "2020-04-21 17:30:25,317 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-21 17:30:25,319 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-21 17:30:25,320 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-21 17:30:25,320 : INFO : EPOCH - 23 : training on 58152 raw words (42750 effective words) took 0.1s, 834973 effective words/s\n",
      "2020-04-21 17:30:25,370 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-21 17:30:25,377 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-21 17:30:25,378 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-21 17:30:25,379 : INFO : EPOCH - 24 : training on 58152 raw words (42485 effective words) took 0.1s, 742930 effective words/s\n",
      "2020-04-21 17:30:25,432 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-21 17:30:25,435 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-21 17:30:25,437 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-21 17:30:25,438 : INFO : EPOCH - 25 : training on 58152 raw words (42682 effective words) took 0.1s, 755408 effective words/s\n",
      "2020-04-21 17:30:25,492 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-21 17:30:25,498 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-21 17:30:25,501 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-21 17:30:25,501 : INFO : EPOCH - 26 : training on 58152 raw words (42647 effective words) took 0.1s, 705320 effective words/s\n",
      "2020-04-21 17:30:25,550 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-21 17:30:25,551 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-21 17:30:25,553 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-21 17:30:25,553 : INFO : EPOCH - 27 : training on 58152 raw words (42657 effective words) took 0.1s, 852242 effective words/s\n",
      "2020-04-21 17:30:25,604 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-21 17:30:25,608 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-21 17:30:25,610 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-21 17:30:25,611 : INFO : EPOCH - 28 : training on 58152 raw words (42710 effective words) took 0.1s, 765458 effective words/s\n",
      "2020-04-21 17:30:25,667 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-21 17:30:25,669 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-21 17:30:25,670 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-21 17:30:25,671 : INFO : EPOCH - 29 : training on 58152 raw words (42752 effective words) took 0.1s, 734485 effective words/s\n",
      "2020-04-21 17:30:25,725 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-21 17:30:25,729 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-21 17:30:25,730 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-21 17:30:25,731 : INFO : EPOCH - 30 : training on 58152 raw words (42688 effective words) took 0.1s, 740890 effective words/s\n",
      "2020-04-21 17:30:25,785 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-21 17:30:25,788 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-21 17:30:25,793 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-21 17:30:25,794 : INFO : EPOCH - 31 : training on 58152 raw words (42678 effective words) took 0.1s, 696434 effective words/s\n",
      "2020-04-21 17:30:25,851 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-21 17:30:25,853 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-21 17:30:25,856 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-21 17:30:25,856 : INFO : EPOCH - 32 : training on 58152 raw words (42699 effective words) took 0.1s, 721446 effective words/s\n",
      "2020-04-21 17:30:25,913 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-21 17:30:25,917 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-21 17:30:25,920 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-21 17:30:25,920 : INFO : EPOCH - 33 : training on 58152 raw words (42764 effective words) took 0.1s, 692777 effective words/s\n",
      "2020-04-21 17:30:25,976 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-21 17:30:25,980 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-21 17:30:25,983 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-21 17:30:25,983 : INFO : EPOCH - 34 : training on 58152 raw words (42703 effective words) took 0.1s, 707751 effective words/s\n",
      "2020-04-21 17:30:26,040 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-21 17:30:26,044 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-21 17:30:26,047 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-21 17:30:26,047 : INFO : EPOCH - 35 : training on 58152 raw words (42733 effective words) took 0.1s, 693005 effective words/s\n",
      "2020-04-21 17:30:26,097 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-21 17:30:26,105 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-21 17:30:26,105 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-21 17:30:26,106 : INFO : EPOCH - 36 : training on 58152 raw words (42686 effective words) took 0.1s, 747043 effective words/s\n",
      "2020-04-21 17:30:26,155 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-21 17:30:26,161 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-21 17:30:26,162 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-21 17:30:26,163 : INFO : EPOCH - 37 : training on 58152 raw words (42823 effective words) took 0.1s, 779328 effective words/s\n",
      "2020-04-21 17:30:26,213 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-21 17:30:26,222 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-21 17:30:26,227 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-21 17:30:26,228 : INFO : EPOCH - 38 : training on 58152 raw words (42732 effective words) took 0.1s, 668142 effective words/s\n",
      "2020-04-21 17:30:26,279 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-21 17:30:26,282 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-21 17:30:26,283 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-21 17:30:26,283 : INFO : EPOCH - 39 : training on 58152 raw words (42764 effective words) took 0.1s, 816942 effective words/s\n",
      "2020-04-21 17:30:26,340 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-04-21 17:30:26,342 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-04-21 17:30:26,347 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-04-21 17:30:26,347 : INFO : EPOCH - 40 : training on 58152 raw words (42632 effective words) took 0.1s, 681996 effective words/s\n",
      "2020-04-21 17:30:26,348 : INFO : training on a 2326080 raw words (1707736 effective words) took 2.4s, 705422 effective words/s\n"
     ]
    }
   ],
   "source": [
    "model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use the trained model to infer a vector for any piece of text\n",
    "by passing a list of words to the ``model.infer_vector`` function. This\n",
    "vector can then be compared with other vectors via cosine similarity.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.1293208  -0.0767075  -0.21969011 -0.14724249  0.16562241 -0.16629428\n",
      "  0.13758661 -0.08491638 -0.02791383  0.06856409 -0.21471728 -0.0458145\n",
      "  0.15881586  0.08595179  0.2387931   0.02289827  0.07522738  0.17156829\n",
      "  0.15301915 -0.19379547 -0.1035142   0.06611954 -0.00216921 -0.08734662\n",
      "  0.06729241 -0.11214101  0.21938272  0.0982281  -0.08311824 -0.07191215\n",
      "  0.08694583 -0.11711127  0.08953984 -0.2964677  -0.05591629 -0.10117383\n",
      " -0.05215308  0.00179323  0.16895051  0.00361331  0.07078552 -0.35648647\n",
      " -0.01327396  0.01286272 -0.05509156 -0.03617888  0.1477938   0.07298137\n",
      " -0.28239638  0.12804675]\n"
     ]
    }
   ],
   "source": [
    "vector = model.infer_vector(['only', 'you', 'can', 'prevent', 'forest', 'fires'])\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that ``infer_vector()`` does *not* take a string, but rather a list of\n",
    "string tokens, which should have already been tokenized the same way as the\n",
    "``words`` property of original training document objects.\n",
    "\n",
    "Also note that because the underlying training/inference algorithms are an\n",
    "iterative approximation problem that makes use of internal randomization,\n",
    "repeated inferences of the same text will return slightly different vectors.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assessing the Model\n",
    "-------------------\n",
    "\n",
    "To assess our new model, we'll first infer new vectors for each document of\n",
    "the training corpus, compare the inferred vectors with the training corpus,\n",
    "and then returning the rank of the document based on self-similarity.\n",
    "Basically, we're pretending as if the training corpus is some new unseen data\n",
    "and then seeing how they compare with the trained model. The expectation is\n",
    "that we've likely overfit our model (i.e., all of the ranks will be less than\n",
    "2) and so we should be able to find similar documents very easily.\n",
    "Additionally, we'll keep track of the second ranks for a comparison of less\n",
    "similar documents.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-21 17:31:00,863 : INFO : precomputing L2-norms of doc weight vectors\n"
     ]
    }
   ],
   "source": [
    "ranks = []\n",
    "second_ranks = []\n",
    "for doc_id in range(len(train_corpus)):\n",
    "    inferred_vector = model.infer_vector(train_corpus[doc_id].words)\n",
    "    sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "    rank = [docid for docid, sim in sims].index(doc_id)\n",
    "    ranks.append(rank)\n",
    "\n",
    "    second_ranks.append(sims[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's count how each document ranks with respect to the training corpus\n",
    "\n",
    "NB. Results vary between runs due to random seeding and very small corpus\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 292, 1: 8})\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "counter = collections.Counter(ranks)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, greater than 95% of the inferred documents are found to be most\n",
    "similar to itself and about 5% of the time it is mistakenly most similar to\n",
    "another document. Checking the inferred-vector against a\n",
    "training-vector is a sort of 'sanity check' as to whether the model is\n",
    "behaving in a usefully consistent manner, though not a real 'accuracy' value.\n",
    "\n",
    "This is great and not entirely surprising. We can take a look at an example:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document (299): Â«australia will take on france in the doubles rubber of the davis cup tennis final today with the tie levelled at wayne arthurs and todd woodbridge are scheduled to lead australia in the doubles against cedric pioline and fabrice santoro however changes can be made to the line up up to an hour before the match and australian team captain john fitzgerald suggested he might do just that we ll make team appraisal of the whole situation go over the pros and cons and make decision french team captain guy forget says he will not make changes but does not know what to expect from australia todd is the best doubles player in the world right now so expect him to play he said would probably use wayne arthurs but don know what to expect really pat rafter salvaged australia davis cup campaign yesterday with win in the second singles match rafter overcame an arm injury to defeat french number one sebastien grosjean in three sets the australian says he is happy with his form it not very pretty tennis there isn too many consistent bounces you are playing like said bit of classic old grass court rafter said rafter levelled the score after lleyton hewitt shock five set loss to nicholas escude in the first singles rubber but rafter says he felt no added pressure after hewitt defeat knew had good team to back me up even if we were down he said knew could win on the last day know the boys can win doubles so even if we were down still feel we are good enough team to win and vice versa they are good enough team to beat us as wellÂ»\n",
      "\n",
      "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(dm/m,d50,n5,w5,mc2,s0.001,t3):\n",
      "\n",
      "MOST (299, 0.9386616349220276): Â«australia will take on france in the doubles rubber of the davis cup tennis final today with the tie levelled at wayne arthurs and todd woodbridge are scheduled to lead australia in the doubles against cedric pioline and fabrice santoro however changes can be made to the line up up to an hour before the match and australian team captain john fitzgerald suggested he might do just that we ll make team appraisal of the whole situation go over the pros and cons and make decision french team captain guy forget says he will not make changes but does not know what to expect from australia todd is the best doubles player in the world right now so expect him to play he said would probably use wayne arthurs but don know what to expect really pat rafter salvaged australia davis cup campaign yesterday with win in the second singles match rafter overcame an arm injury to defeat french number one sebastien grosjean in three sets the australian says he is happy with his form it not very pretty tennis there isn too many consistent bounces you are playing like said bit of classic old grass court rafter said rafter levelled the score after lleyton hewitt shock five set loss to nicholas escude in the first singles rubber but rafter says he felt no added pressure after hewitt defeat knew had good team to back me up even if we were down he said knew could win on the last day know the boys can win doubles so even if we were down still feel we are good enough team to win and vice versa they are good enough team to beat us as wellÂ»\n",
      "\n",
      "SECOND-MOST (104, 0.8215712308883667): Â«australian cricket captain steve waugh has supported fast bowler brett lee after criticism of his intimidatory bowling to the south african tailenders in the first test in adelaide earlier this month lee was fined for giving new zealand tailender shane bond an unsportsmanlike send off during the third test in perth waugh says tailenders should not be protected from short pitched bowling these days you re earning big money you ve got responsibility to learn how to bat he said mean there no times like years ago when it was not professional and sort of bowlers code these days you re professional our batsmen work very hard at their batting and expect other tailenders to do likewise meanwhile waugh says his side will need to guard against complacency after convincingly winning the first test by runs waugh says despite the dominance of his side in the first test south africa can never be taken lightly it only one test match out of three or six whichever way you want to look at it so there lot of work to go he said but it nice to win the first battle definitely it gives us lot of confidence going into melbourne you know the big crowd there we love playing in front of the boxing day crowd so that will be to our advantage as well south africa begins four day match against new south wales in sydney on thursday in the lead up to the boxing day test veteran fast bowler allan donald will play in the warm up match and is likely to take his place in the team for the second test south african captain shaun pollock expects much better performance from his side in the melbourne test we still believe that we didn play to our full potential so if we can improve on our aspects the output we put out on the field will be lot better and we still believe we have side that is good enough to beat australia on our day he saidÂ»\n",
      "\n",
      "MEDIAN (120, 0.27399903535842896): Â«unions representing qantas maintenance workers have warned of escalating industrial action if the company rejects an offer to have long running dispute arbitrated the parties were locked in private talks yesterday in the industrial relations commission after more than maintenance workers earlier voted to reject qantas proposed wage freeze the national secretary of the australian manufacturing and workers union amwu doug cameron says the unions have done everything possible to resolve the dispute if qantas is not prepared to accept private arbitration there is absolutely no alternative for these workers to take further industrial action escalate the industrial action if necessary to ensure that they get fair go from this company who seemed determined to crush them underfoot he saidÂ»\n",
      "\n",
      "LEAST (85, -0.11200328171253204): Â«hamas militants have fought gun battles with palestinian security forces in the gaza strip trying to arrest one of the islamic group senior political leaders reports say the fight erupted in the gaza strip after dozens of hamas members surrounded the home of abdel aziz al rantissi when palestinian police arrived to detain him the palestinian leader yasser arafat under international pressure to crack down on militants after wave of suicide bombings in israel in the past month has outlawed the military wings of hamas and other groups and arrested dozens of militantsÂ»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Document ({}): Â«{}Â»\\n'.format(doc_id, ' '.join(train_corpus[doc_id].words)))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('SECOND-MOST', 1), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: Â«%sÂ»\\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice above that the most similar document (usually the same text) is has a\n",
    "similarity score approaching 1.0. However, the similarity score for the\n",
    "second-ranked documents should be significantly lower (assuming the documents\n",
    "are in fact different) and the reasoning becomes obvious when we examine the\n",
    "text itself.\n",
    "\n",
    "We can run the next cell repeatedly to see a sampling other target-document\n",
    "comparisons.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Document (268): Â«opposition forces claimed to have captured half of kandahar airport after fierce fighting with taliban troops as residents reported further cranking up of us bombardments on the city we have now taken half of the airport said gul lali key lieutenant to former kandahar governor gul agha lali said that their forces had killed foreign taliban fighters in the operation and overrun building that appeared to have been used as an office by members of suspected terrorist mastermind osama bin laden al qaeda network these were of bin laden men from egypt libya and saudi arabia nineteen more were injured lali said from the battleground opposition commanders had earlier expressed confidence that the airport would fall by the end of the day with the taliban defence tactics hamstrung by aerial attacks from us warplanes we can hope but am not per cent sure we will capture it by the end of the evening spokesman for gul agha said after speaking to the opposition leader at noon local time bombing raids by us warplanes on taliban positions around the airport had helped the push according to fighters on the ground residents who left kandahar early monday confirmed that there had been no let up in the us bombing abdul masood said at that planes were now flying in five strong sorties the frequency has increased they now come within an interval of half an hour he said they are targeting the airport area and taliban positions outside the city they are also hitting the road between the city and the airport saw at least four trucks which had been overturned lying on the road masood also said he had reports that some opposition soldiers had been killed in suicide attack by taliban supporters some people told me that several arabs with grenades strapped around their abdomen managed to enter an advancing column at torkotal near kandahar airport believe there were heavy casualties his claims could not be independently verified but doctor working for the relief agency muslim hands said that he had treated six of agha men after they crossed the border monday they were later dispatched to chaman hospital nearly civilians have been killed and wounded in three nights of us airstrikes near jalalabad the provincial military chief said commander haji mohammad zaman said the bombs targeted an area south of jalalabad near the tora bora mountain cave complex where terror suspect osama bin laden is believed to have hideout zaman who directs military operations in nangarhar province said the first night of bombings left nearly civilians dead and wounded second night of air raids killed eight civilians and wounded he said and the third night left eight dead and many people injuredÂ»\n",
      "\n",
      "Similar Document (200, 0.8164346218109131): Â«the united states says video tape found inside afghanistan proves beyond doubt osama bin laden was behind the attacks on the world trade centre and the pentagon the tape is alleged to show bin laden discussing the success of the mission in the minute tape bin laden is said to be at dinner when told plane had crashed into the world trade centre he is alleged to have told others present what had happened and they cheered us vice president dick cheney says the video shows bin laden was clearly behind the attacks there ve some disputes in some quarters about it but this is one more piece of evidence confirming his responsibility he said republican chuck hagel of the foreign relations committee says the administration must make the tapes public the world needs to see this he said some officials hope it will be shown to counter concerns in the muslim world that bin laden has been unjustly accused osama bin laden was said to be staging defiant stand in the afghan mountains as taliban rule finally came to an ignominious end with the surrender of the last province under their control spokesman for the northern alliance said bin laden was now leading the defence of his mountain hideouts in person with about loyal fighters from his al qaeda organisation osama himself has taken the command of the fighting mohammad amin told the reuters news agency from the eastern city of jalalabad he along with around of his people including some taliban officials have now dug themselves into the forests of spin ghar after we overran all their bases in tora bora he is here for sure mr amin said american planes have been carrying out regular and severe bombings to kill him mr amin added that at least one of bin laden arab fighters had been killed in very intense fighting the saudi born islamist accused by washington of ordering the september attacks on the united states appeared ever more isolated after his taliban protectors handed over the zabul province to tribal elders the rule of the taliban in afghanistan has totally ended the pakistan based afghan news agency afghan islamic press aip said in reporting the surrender of zabul at least civilians were killed and injured in weekend bombing raids by us warplanes in afghanistan south eastern paktika province the afghan islamic press aip said late sunday the pakistan based news agency quoting informed sources said the us jets blasted several vehicles at sharana the provincial capital of paktika on saturday killing people and injuring several others the dead were five children four women and five men another people were killed when us planes bombarded vehicles in pre dawn raids on sunday in the mosh khil area near sharana aip said it said mosque was destroyed in the raids aip said taliban rule had been ended in paktika and the administration was being run by tribal shura councilÂ»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pick a random document from the corpus and infer a vector from the model\n",
    "import random\n",
    "doc_id = random.randint(0, len(train_corpus) - 1)\n",
    "\n",
    "# Compare and print the second-most-similar document\n",
    "print('Train Document ({}): Â«{}Â»\\n'.format(doc_id, ' '.join(train_corpus[doc_id].words)))\n",
    "sim_id = second_ranks[doc_id]\n",
    "print('Similar Document {}: Â«{}Â»\\n'.format(sim_id, ' '.join(train_corpus[sim_id[0]].words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the Model\n",
    "-----------------\n",
    "\n",
    "Using the same approach above, we'll infer the vector for a randomly chosen\n",
    "test document, and compare the document to our model by eye.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Document (21): Â«the federal government says changes announced today to the work for the dole scheme will benefit participants and taxpayers federal employment services minister mal brough says that from july those taking part in work for the dole will be able to perform extra hours to complete their mutual obligation more quickly to access training creditsÂ»\n",
      "\n",
      "SIMILAR/DISSIMILAR DOCS PER MODEL Doc2Vec(dm/m,d50,n5,w5,mc2,s0.001,t3):\n",
      "\n",
      "MOST (238, 0.7216726541519165): Â«centrelink is urging people affected by job cuts at regional pay tv operator austar and travel company traveland to seek information about their income support options traveland has announced it is shedding more than jobs around australia and austar is letting employees go centrelink finance information officer peter murray says those facing uncertain futures should head to centrelink in the next few days centrelink is the shopfront now for commonwealth services for income support and the employment network so that it is important if people haven been to us before they might get pleasant surprise at the range of services that we do offer to try and help them through situations where things might have changed for them mr murray saidÂ»\n",
      "\n",
      "MEDIAN (271, 0.3613166809082031): Â«director of defunct swiss company that organised canyoning trip in that ended with people dying of them australians has denied responsibility for the tragedy along with two co directors who are also charged with manslaughter adventure world director stephan friedli appeared in court on the first day of their trial he described the deaths of people in the saxeten river gorge as an accident that was unforeseeable and not preventable friedli said he was aware of the possibility the river could flood but when asked whether his company carried out risk analysis he replied we know the region we live here to the question you know what you are accused of have you made any mistakes mr friedli replied no don think soÂ»\n",
      "\n",
      "LEAST (140, -0.07452496886253357): Â«osama bin laden admitted planning the september terrorist attacks on the united states in videotape released by the pentagon today in the videotape lasting roughly one hour bin laden explains planning aspects of the operation and his own calculations in advance concerning the scale of the damage to the world trade center in new york and the number of casualties he said he expected the fire and gas from the attacks on the world trade center to topple the floors above the points where hijacked planes struck not the entire structure we calculated in advance the number of casualties from the enemy who would be killed based on the position of the tower he said according to transcript translated into english from the arabic due to my experience in this field was thinking that the fire from the gas in the plane would melt the iron structure of the building and collapse the area where the plane hit and all the floors above it only he said that is all that we had hoped for the video tape showed bin laden speaking to supporters in room possibly in kandahar in mid november the pentagon said in releasing the amateur videotape which it said was made with the knowledge of bin laden and those present the tape showed the end of the meeting first followed by an unrelated segment of videotaped material and ending with segment recorded at the beginning of the meeting we had notification since the previous thursday that the event would take place that day he said speaking to unidentified sheikh we had finished our work that day and had the radio on it was pm our time was sitting with dr ahmad abu al khair he said immediately we heard the news that plane had hit the world trade center after while they announced that another plane had hit the world trade center the brothers who heard the news were overjoyed by it he saidÂ»\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pick a random document from the test corpus and infer a vector from the model\n",
    "doc_id = random.randint(0, len(test_corpus) - 1)\n",
    "inferred_vector = model.infer_vector(test_corpus[doc_id])\n",
    "sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "\n",
    "# Compare and print the most/median/least similar documents from the train corpus\n",
    "print('Test Document ({}): Â«{}Â»\\n'.format(doc_id, ' '.join(test_corpus[doc_id])))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: Â«%sÂ»\\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "----------\n",
    "\n",
    "Let's review what we've seen in this tutorial:\n",
    "\n",
    "0. Review the relevant models: bag-of-words, Word2Vec, Doc2Vec\n",
    "1. Load and preprocess the training and test corpora (see `core_concepts_corpus`)\n",
    "2. Train a Doc2Vec `core_concepts_model` model using the training corpus\n",
    "3. Demonstrate how the trained model can be used to infer a `core_concepts_vector`\n",
    "4. Assess the model\n",
    "5. Test the model on the test corpus\n",
    "\n",
    "That's it! Doc2Vec is a great way to explore relationships between documents.\n",
    "\n",
    "Additional Resources\n",
    "--------------------\n",
    "\n",
    "If you'd like to know more about the subject matter of this tutorial, check out the links below.\n",
    "\n",
    "* `Word2Vec Paper <https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf>`_\n",
    "* `Doc2Vec Paper <https://cs.stanford.edu/~quocle/paragraph_vector.pdf>`_\n",
    "* `Dr. Michael D. Lee's Website <http://faculty.sites.uci.edu/mdlee>`_\n",
    "* `Lee Corpus <http://faculty.sites.uci.edu/mdlee/similarity-data/>`__\n",
    "* `IMDB Doc2Vec Tutorial <doc2vec-IMDB.ipynb>`_\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
