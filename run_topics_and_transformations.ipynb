{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Topics and Transformations\n",
    "===========================\n",
    "\n",
    "Introduces transformations and demonstrates their use on a toy corpus.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "#import logging\n",
    "#logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the Corpus\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to import 'smart_open.gcs', disabling that module\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from gensim import corpora\n",
    "\n",
    "documents = [\n",
    "    \"Human machine interface for lab abc computer applications\",\n",
    "    \"A survey of user opinion of computer system response time\",\n",
    "    \"The EPS user interface management system\",\n",
    "    \"System and human system engineering testing of EPS\",\n",
    "    \"Relation of user perceived response time to error measurement\",\n",
    "    \"The generation of random binary unordered trees\",\n",
    "    \"The intersection graph of paths in trees\",\n",
    "    \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "    \"Graph minors A survey\",\n",
    "]\n",
    "\n",
    "# remove common words and tokenize\n",
    "stoplist = set('for a of the and to in'.split())\n",
    "texts = [\n",
    "    [word for word in document.lower().split() if word not in stoplist]\n",
    "    for document in documents\n",
    "]\n",
    "\n",
    "# remove words that appear only once\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "texts = [\n",
    "    [token for token in text if frequency[token] > 1]\n",
    "    for text in texts\n",
    "]\n",
    "\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "tfidf = models.TfidfModel(corpus)  # step 1 -- initialize a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We used the corpus from tutorial 1 to train the transformer. \n",
    "* Different transformers require different initialization parameters - TfIdf simply goes through the corpus once. Training other models such as LSA or LDA is much more involved.\n",
    "* Note: transforms convert between two vector spaces. The same vector space (= the same set of feature ids) must be used for training & subsequent transforms. Failure to use the same input feature space, such as applying a different string preprocessing, using different\n",
    "  feature ids, or using bag-of-words vectors where TfIdf vectors are expected, will\n",
    "  result in feature mismatch during transformation calls, resulting in garbage output and/or runtime exceptions.\n",
    "\n",
    "### Transforming vectors\n",
    "\n",
    "* From now on, _tfidf_ is treated as a read-only object that can be used to convert any vector from the old representation (BoW integer counts) to the new representation (TfIdf real-valued weights)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.7071067811865476), (1, 0.7071067811865476)]\n"
     ]
    }
   ],
   "source": [
    "doc_bow = [(0, 1), (1, 1)]\n",
    "print(tfidf[doc_bow])  # step 2 -- use the model to transform vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Apply transformation to entire corpus:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.5773502691896257), (1, 0.5773502691896257), (2, 0.5773502691896257)]\n",
      "[(0, 0.44424552527467476), (3, 0.44424552527467476), (4, 0.44424552527467476), (5, 0.3244870206138555), (6, 0.44424552527467476), (7, 0.3244870206138555)]\n",
      "[(2, 0.5710059809418182), (5, 0.4170757362022777), (7, 0.4170757362022777), (8, 0.5710059809418182)]\n",
      "[(1, 0.49182558987264147), (5, 0.7184811607083769), (8, 0.49182558987264147)]\n",
      "[(3, 0.6282580468670046), (6, 0.6282580468670046), (7, 0.45889394536615247)]\n",
      "[(9, 1.0)]\n",
      "[(9, 0.7071067811865475), (10, 0.7071067811865475)]\n",
      "[(9, 0.5080429008916749), (10, 0.5080429008916749), (11, 0.695546419520037)]\n",
      "[(4, 0.6282580468670046), (10, 0.45889394536615247), (11, 0.6282580468670046)]\n"
     ]
    }
   ],
   "source": [
    "corpus_tfidf = tfidf[corpus]\n",
    "for doc in corpus_tfidf:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Once a transformer model is initialized, it can be used on any vectors (provided they come from the same vector space, of course), even if they were not used for training. This is done using a process called __folding-in for LSA__, by topic inference for LDA etc.\n",
    "\n",
    "* Note: calling _model[corpus]_ only creates a wrapper around the old _corpus_ document stream -- actual conversions are done on-the-fly, during document iteration.\n",
    "\n",
    "* We cannot convert the entire corpus when calling _corpus_transformed = model[corpus]_, because it requires storing the result in main memory - this violates gensim's objective of memory indepedence.\n",
    "\n",
    "* If you will be iterating over the transformed _corpus_transformed_ multiple times, and the transformation is costly, **serialize the resulting corpus to disk first.** Transformations can also be serialized, one on top of another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "lsi_model = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=2)  # initialize an LSI transformation\n",
    "corpus_lsi = lsi_model[corpus_tfidf]  # create a double wrapper over the original corpus: bow->tfidf->fold-in-lsi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here we transformed our Tf-Idf corpus using [Latent Semantic Indexing](http://en.wikipedia.org/wiki/Latent_semantic_indexing) into a latent 2-D space (2-D because we set _num_topics=2-). \n",
    "* What do these two latent dimensions stand for?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.703*\"trees\" + 0.538*\"graph\" + 0.402*\"minors\" + 0.187*\"survey\" + 0.061*\"system\" + 0.060*\"time\" + 0.060*\"response\" + 0.058*\"user\" + 0.049*\"computer\" + 0.035*\"interface\"'),\n",
       " (1,\n",
       "  '-0.460*\"system\" + -0.373*\"user\" + -0.332*\"eps\" + -0.328*\"interface\" + -0.320*\"time\" + -0.320*\"response\" + -0.293*\"computer\" + -0.280*\"human\" + -0.171*\"survey\" + 0.161*\"trees\"')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi_model.print_topics(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* according to LSI:\n",
    "    - trees\", \"graph\" and \"minors\" are all related words (and contribute the most to the 1st topic)\n",
    "    - the 2nd topic practically concerns itself with all the other words. \n",
    "* As expected, the first five documents are more strongly related to the second topic while the remaining four documents to the first topic:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.06600783396090415), (1, -0.5200703306361852)] Human machine interface for lab abc computer applications\n",
      "[(0, 0.196675928591426), (1, -0.7609563167700037)] A survey of user opinion of computer system response time\n",
      "[(0, 0.08992639972446562), (1, -0.7241860626752503)] The EPS user interface management system\n",
      "[(0, 0.07585847652178271), (1, -0.6320551586003426)] System and human system engineering testing of EPS\n",
      "[(0, 0.10150299184980228), (1, -0.5737308483002944)] Relation of user perceived response time to error measurement\n",
      "[(0, 0.7032108939378311), (1, 0.1611518021402591)] The generation of random binary unordered trees\n",
      "[(0, 0.8774787673119828), (1, 0.16758906864659567)] The intersection graph of paths in trees\n",
      "[(0, 0.9098624686818573), (1, 0.1408655362871918)] Graph minors IV Widths of trees and well quasi ordering\n",
      "[(0, 0.6165825350569274), (1, -0.05392907566389242)] Graph minors A survey\n"
     ]
    }
   ],
   "source": [
    "# both bow->tfidf and tfidf->lsi transformations are done on the fly\n",
    "for doc, as_text in zip(corpus_lsi, documents):\n",
    "    print(doc, as_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "\n",
    "with tempfile.NamedTemporaryFile(\n",
    "    prefix='model-', \n",
    "    suffix='.lsi', \n",
    "    delete=False) as tmp:\n",
    "    lsi_model.save(tmp.name)  # same for tfidf, lda, ...\n",
    "\n",
    "loaded_lsi_model = models.LsiModel.load(tmp.name)\n",
    "\n",
    "os.unlink(tmp.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/model-fmfq2wm3.lsi.projection\n"
     ]
    }
   ],
   "source": [
    "!ls /tmp/*lsi*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Available transformations\n",
    "--------------------------\n",
    "\n",
    "Gensim implements several popular Vector Space Model algorithms:\n",
    "\n",
    "* [Term Frequency * Inverse Document Frequency, Tf-Idf](http://en.wikipedia.org/wiki/Tf%E2%80%93idf) expects a bag-of-words (integer values) training corpus. It accepts a vector & returns another vector of the same dimensionality -- the features which were rare in the training corpus will have their value increased. It therefore converts integer-valued vectors into real-valued ones, while leaving #dimensions intact. It can also optionally normalize the results to (Euclidean) unit length.\n",
    "    - _model = models.TfidfModel(corpus, normalize=True)_\n",
    "\n",
    "\n",
    "* [Latent Semantic Indexing, LSI (sometimes LSA)](http://en.wikipedia.org/wiki/Latent_semantic_indexing) transforms documents from either bag-of-words or (preferrably) TfIdf into a latent space of a lower dimensionality. For the toy corpus we used only 2 latent dimensions, but on real corpora, target dimensionality of 200--500 is recommended.\n",
    "    - _model = models.LsiModel(tfidf_corpus, id2word=dictionary, num_topics=300)_\n",
    "\n",
    "\n",
    "* LSI training is unique in that we can continue \"training\" at any point, simply by providing more training documents. This is done by incremental updates to the underlying moded (called **online training**). So the input document stream can be infinite -- just keep feeding LSI new documents as they arrive, while using the computed transformation model as read-only in the meanwhile!\n",
    "\n",
    "    - _model.add_documents(another_tfidf_corpus)_  # now LSI has been trained on tfidf_corpus + another_tfidf_corpus\n",
    "    - _lsi_vec = model[tfidf_vec]_  # convert some new document into the LSI space, without affecting the model\n",
    "    - _model.add_documents(more_documents)_  # tfidf_corpus + another_tfidf_corpus + more_documents\n",
    "    - _lsi_vec = model[tfidf_vec]_\n",
    "\n",
    "* See _gensim.models.lsimodel_ to learn how to make LSI gradually \"forget\" old observations in infinite streams.\n",
    "\n",
    "* _gensim_ uses an online incremental streamed distributed training algorithm, published in [5]. _gensim_ also executes a stochastic multi-pass algorithm, from Halko et al. [4], to accelerate in-core computations. \n",
    "\n",
    "\n",
    "* [Random Projections (RP)](http://www.cis.hut.fi/ella/publications/randproj_kdd.pdf) reduces dimensionality. This is a very memory- and CPU-efficient approach to approximating TfIdf distances between documents by throwing in a little randomness.\n",
    "  Recommended target dimensionality is again in the hundreds/thousands, depending on your dataset.\n",
    "      - _model = models.RpModel(tfidf_corpus, num_topics=500)_\n",
    "\n",
    "\n",
    "* [Latent Dirichlet Allocation, LDA](http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) also transforms vectors from BoW counts into a topic space of lower dimensionality. It is a probabilistic extension of LSA (also called multinomial PCA) - LDA's topics can be interpreted as probability distributions over words. _gensim`s_ implementation uses [2], modified to run in distributed mode on clusters.\n",
    "    - _model = models.LdaModel(corpus, id2word=dictionary, num_topics=100)_\n",
    "\n",
    "\n",
    "* [Hierarchical Dirichlet Process, HDP](http://jmlr.csail.mit.edu/proceedings/papers/v15/wang11a/wang11a.pdf) is a non-parametric bayesian method (note the missing number of requested topics). _gensim_`s implementation is based on [3]. The HDP model is a new addition to _gensim_ and still rough around the edges -- use with care.\n",
    "    - _model = models.HdpModel(corpus, id2word=dictionary)_\n",
    "\n",
    "\n",
    "* Adding new VSM (Vector Space Model) transformers (such as different weighting schemes):\n",
    "    - see the API or [Python code](https://github.com/piskvorky/gensim/blob/develop/gensim/models/tfidfmodel.py)\n",
    "\n",
    "\n",
    "References\n",
    "----------\n",
    "\n",
    "[1] Bradford. 2008. An empirical study of required dimensionality for large-scale latent semantic indexing applications.\n",
    "\n",
    "[2] Hoffman, Blei, Bach. 2010. Online learning for Latent Dirichlet Allocation.\n",
    "\n",
    "[3] Wang, Paisley, Blei. 2011. Online variational inference for the hierarchical Dirichlet process.\n",
    "\n",
    "[4] Halko, Martinsson, Tropp. 2009. Finding structure with randomness.\n",
    "\n",
    "[5] Řehůřek. 2011. Subspace tracking for Latent Semantic Analysis.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
