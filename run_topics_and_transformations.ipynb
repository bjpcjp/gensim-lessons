{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Topics and Transformations\n",
    "===========================\n",
    "\n",
    "Introduces transformations and demonstrates their use on a toy corpus.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the Corpus\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-22 12:24:07,552 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-04-22 12:24:07,553 : INFO : built Dictionary(12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...) from 9 documents (total 29 corpus positions)\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from gensim import corpora\n",
    "\n",
    "documents = [\n",
    "    \"Human machine interface for lab abc computer applications\",\n",
    "    \"A survey of user opinion of computer system response time\",\n",
    "    \"The EPS user interface management system\",\n",
    "    \"System and human system engineering testing of EPS\",\n",
    "    \"Relation of user perceived response time to error measurement\",\n",
    "    \"The generation of random binary unordered trees\",\n",
    "    \"The intersection graph of paths in trees\",\n",
    "    \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "    \"Graph minors A survey\",\n",
    "]\n",
    "\n",
    "# remove common words and tokenize\n",
    "stoplist = set('for a of the and to in'.split())\n",
    "texts = [\n",
    "    [word for word in document.lower().split() if word not in stoplist]\n",
    "    for document in documents\n",
    "]\n",
    "\n",
    "# remove words that appear only once\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "texts = [\n",
    "    [token for token in text if frequency[token] > 1]\n",
    "    for text in texts\n",
    "]\n",
    "\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-22 12:24:28,654 : INFO : collecting document frequencies\n",
      "2020-04-22 12:24:28,655 : INFO : PROGRESS: processing document #0\n",
      "2020-04-22 12:24:28,657 : INFO : calculating IDF weights for 9 documents and 12 features (28 matrix non-zeros)\n"
     ]
    }
   ],
   "source": [
    "from gensim import models\n",
    "tfidf = models.TfidfModel(corpus)  # step 1 -- initialize a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We used the corpus from tutorial 1 to train the transformer. \n",
    "* Different transformers require different initialization parameters - TfIdf simply goes through the corpus once. Training other models such as LSA or LDA is much more involved.\n",
    "* Note: transforms convert between two vector spaces. The same vector space (= the same set of feature ids) must be used for training & subsequent transforms. Failure to use the same input feature space, such as applying a different string preprocessing, using different\n",
    "  feature ids, or using bag-of-words vectors where TfIdf vectors are expected, will\n",
    "  result in feature mismatch during transformation calls, resulting in garbage output and/or runtime exceptions.\n",
    "\n",
    "### Transforming vectors\n",
    "\n",
    "From now on, ``tfidf`` is treated as a read-only object that can be used to convert\n",
    "any vector from the old representation (BoW integer counts) to the new representation\n",
    "(TfIdf real-valued weights)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.7071067811865476), (1, 0.7071067811865476)]\n"
     ]
    }
   ],
   "source": [
    "doc_bow = [(0, 1), (1, 1)]\n",
    "print(tfidf[doc_bow])  # step 2 -- use the model to transform vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Apply transformation to entire corpus:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.5773502691896257), (1, 0.5773502691896257), (2, 0.5773502691896257)]\n",
      "[(0, 0.44424552527467476), (3, 0.44424552527467476), (4, 0.44424552527467476), (5, 0.3244870206138555), (6, 0.44424552527467476), (7, 0.3244870206138555)]\n",
      "[(2, 0.5710059809418182), (5, 0.4170757362022777), (7, 0.4170757362022777), (8, 0.5710059809418182)]\n",
      "[(1, 0.49182558987264147), (5, 0.7184811607083769), (8, 0.49182558987264147)]\n",
      "[(3, 0.6282580468670046), (6, 0.6282580468670046), (7, 0.45889394536615247)]\n",
      "[(9, 1.0)]\n",
      "[(9, 0.7071067811865475), (10, 0.7071067811865475)]\n",
      "[(9, 0.5080429008916749), (10, 0.5080429008916749), (11, 0.695546419520037)]\n",
      "[(4, 0.6282580468670046), (10, 0.45889394536615247), (11, 0.6282580468670046)]\n"
     ]
    }
   ],
   "source": [
    "corpus_tfidf = tfidf[corpus]\n",
    "for doc in corpus_tfidf:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Once a transformer model is initialized, it can be used on any vectors (provided they come from the same vector space, of course), even if they were not used for training. This is done usin a process called folding-in for LSA, by topic inference for LDA etc.\n",
    "\n",
    "* Note: calling ``model[corpus]`` only creates a wrapper around the old ``corpus``\n",
    "  document stream -- actual conversions are done on-the-fly, during document iteration.\n",
    "  We cannot convert the entire corpus at the time of calling ``corpus_transformed = model[corpus]``, because it requires storing the result in main memory - this violates gensim's objective of memory indepedence.\n",
    "* If you will be iterating over the transformed ``corpus_transformed`` multiple times, and the transformation is costly, **serialize the resulting corpus to disk first.** Transformations can also be serialized, one on top of another, in a sort of chain:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-22 12:32:46,095 : INFO : using serial LSI version on this node\n",
      "2020-04-22 12:32:46,097 : INFO : updating model with new documents\n",
      "2020-04-22 12:32:46,099 : INFO : preparing a new chunk of documents\n",
      "2020-04-22 12:32:46,100 : INFO : using 100 extra samples and 2 power iterations\n",
      "2020-04-22 12:32:46,101 : INFO : 1st phase: constructing (12, 102) action matrix\n",
      "2020-04-22 12:32:46,103 : INFO : orthonormalizing (12, 102) action matrix\n",
      "2020-04-22 12:32:46,105 : INFO : 2nd phase: running dense svd on (12, 9) matrix\n",
      "2020-04-22 12:32:46,106 : INFO : computing the final decomposition\n",
      "2020-04-22 12:32:46,107 : INFO : keeping 2 factors (discarding 47.565% of energy spectrum)\n",
      "2020-04-22 12:32:46,108 : INFO : processed documents up to #9\n",
      "2020-04-22 12:32:46,108 : INFO : topic #0(1.594): 0.703*\"trees\" + 0.538*\"graph\" + 0.402*\"minors\" + 0.187*\"survey\" + 0.061*\"system\" + 0.060*\"time\" + 0.060*\"response\" + 0.058*\"user\" + 0.049*\"computer\" + 0.035*\"interface\"\n",
      "2020-04-22 12:32:46,109 : INFO : topic #1(1.476): -0.460*\"system\" + -0.373*\"user\" + -0.332*\"eps\" + -0.328*\"interface\" + -0.320*\"response\" + -0.320*\"time\" + -0.293*\"computer\" + -0.280*\"human\" + -0.171*\"survey\" + 0.161*\"trees\"\n"
     ]
    }
   ],
   "source": [
    "lsi_model = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=2)  # initialize an LSI transformation\n",
    "corpus_lsi = lsi_model[corpus_tfidf]  # create a double wrapper over the original corpus: bow->tfidf->fold-in-lsi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here we transformed our Tf-Idf corpus using [Latent Semantic Indexing](http://en.wikipedia.org/wiki/Latent_semantic_indexing) into a latent 2-D space (2-D because we set ``num_topics=2``). \n",
    "* What do these two latent dimensions stand for? Let's see with :func:`models.LsiModel.print_topics`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-22 12:36:20,935 : INFO : topic #0(1.594): 0.703*\"trees\" + 0.538*\"graph\" + 0.402*\"minors\" + 0.187*\"survey\" + 0.061*\"system\" + 0.060*\"time\" + 0.060*\"response\" + 0.058*\"user\" + 0.049*\"computer\" + 0.035*\"interface\"\n",
      "2020-04-22 12:36:20,936 : INFO : topic #1(1.476): -0.460*\"system\" + -0.373*\"user\" + -0.332*\"eps\" + -0.328*\"interface\" + -0.320*\"response\" + -0.320*\"time\" + -0.293*\"computer\" + -0.280*\"human\" + -0.171*\"survey\" + 0.161*\"trees\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.703*\"trees\" + 0.538*\"graph\" + 0.402*\"minors\" + 0.187*\"survey\" + 0.061*\"system\" + 0.060*\"time\" + 0.060*\"response\" + 0.058*\"user\" + 0.049*\"computer\" + 0.035*\"interface\"'),\n",
       " (1,\n",
       "  '-0.460*\"system\" + -0.373*\"user\" + -0.332*\"eps\" + -0.328*\"interface\" + -0.320*\"response\" + -0.320*\"time\" + -0.293*\"computer\" + -0.280*\"human\" + -0.171*\"survey\" + 0.161*\"trees\"')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi_model.print_topics(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* according to LSI:\n",
    "    - trees\", \"graph\" and \"minors\" are all related words (and contribute the most to the 1st topic)\n",
    "    - the 2nd topic practically concerns itself with all the other words. \n",
    "* As expected, the first five documents are more strongly related to the second topic while the remaining four documents to the first topic:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.06600783396090451), (1, -0.5200703306361849)] Human machine interface for lab abc computer applications\n",
      "[(0, 0.1966759285914269), (1, -0.7609563167700043)] A survey of user opinion of computer system response time\n",
      "[(0, 0.08992639972446562), (1, -0.7241860626752503)] The EPS user interface management system\n",
      "[(0, 0.07585847652178268), (1, -0.6320551586003422)] System and human system engineering testing of EPS\n",
      "[(0, 0.10150299184980302), (1, -0.5737308483002957)] Relation of user perceived response time to error measurement\n",
      "[(0, 0.7032108939378308), (1, 0.1611518021402595)] The generation of random binary unordered trees\n",
      "[(0, 0.8774787673119828), (1, 0.16758906864659615)] The intersection graph of paths in trees\n",
      "[(0, 0.9098624686818575), (1, 0.14086553628719237)] Graph minors IV Widths of trees and well quasi ordering\n",
      "[(0, 0.6165825350569282), (1, -0.05392907566389199)] Graph minors A survey\n"
     ]
    }
   ],
   "source": [
    "# both bow->tfidf and tfidf->lsi transformations are done on the fly\n",
    "for doc, as_text in zip(corpus_lsi, documents):\n",
    "    print(doc, as_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use :func:`save` and :func:`load` functions to persist items.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-22 12:39:14,701 : INFO : saving Projection object under /tmp/model-a77xtk9n.lsi.projection, separately None\n",
      "2020-04-22 12:39:14,703 : INFO : saved /tmp/model-a77xtk9n.lsi.projection\n",
      "2020-04-22 12:39:14,703 : INFO : saving LsiModel object under /tmp/model-a77xtk9n.lsi, separately None\n",
      "2020-04-22 12:39:14,704 : INFO : not storing attribute projection\n",
      "2020-04-22 12:39:14,705 : INFO : not storing attribute dispatcher\n",
      "2020-04-22 12:39:14,706 : INFO : saved /tmp/model-a77xtk9n.lsi\n",
      "2020-04-22 12:39:14,707 : INFO : loading LsiModel object from /tmp/model-a77xtk9n.lsi\n",
      "2020-04-22 12:39:14,708 : INFO : loading id2word recursively from /tmp/model-a77xtk9n.lsi.id2word.* with mmap=None\n",
      "2020-04-22 12:39:14,709 : INFO : setting ignored attribute projection to None\n",
      "2020-04-22 12:39:14,710 : INFO : setting ignored attribute dispatcher to None\n",
      "2020-04-22 12:39:14,710 : INFO : loaded /tmp/model-a77xtk9n.lsi\n",
      "2020-04-22 12:39:14,711 : INFO : loading LsiModel object from /tmp/model-a77xtk9n.lsi.projection\n",
      "2020-04-22 12:39:14,712 : INFO : loaded /tmp/model-a77xtk9n.lsi.projection\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tempfile\n",
    "\n",
    "with tempfile.NamedTemporaryFile(\n",
    "    prefix='model-', \n",
    "    suffix='.lsi', \n",
    "    delete=False) as tmp:\n",
    "    lsi_model.save(tmp.name)  # same for tfidf, lda, ...\n",
    "\n",
    "loaded_lsi_model = models.LsiModel.load(tmp.name)\n",
    "\n",
    "os.unlink(tmp.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/model-5xidaxof.lsi.projection  /tmp/model-si39wa43.lsi.projection\n",
      "/tmp/model-a77xtk9n.lsi.projection\n"
     ]
    }
   ],
   "source": [
    "!ls /tmp/*lsi*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How similar are those documents?\n",
    "\n",
    "Available transformations\n",
    "--------------------------\n",
    "\n",
    "Gensim implements several popular Vector Space Model algorithms:\n",
    "\n",
    "* [Term Frequency * Inverse Document Frequency, Tf-Idf](http://en.wikipedia.org/wiki/Tf%E2%80%93idf) expects a bag-of-words (integer values) training corpus. It accepts a vector & returns another vector of the same dimensionality -- the features which were rare in the training corpus will have their value increased. It therefore converts integer-valued vectors into real-valued ones, while leaving #dimensions intact. It can also optionally normalize the results to (Euclidean) unit length.\n",
    "\n",
    "``model = models.TfidfModel(corpus, normalize=True)``\n",
    "\n",
    "* [Latent Semantic Indexing, LSI (sometimes LSA)](http://en.wikipedia.org/wiki/Latent_semantic_indexing) transforms documents from either bag-of-words or (preferrably) TfIdf into a latent space of a lower dimensionality. For the toy corpus we used only 2 latent dimensions, but on real corpora, target dimensionality of 200--500 is recommended. \n",
    "\n",
    "``model = models.LsiModel(tfidf_corpus, id2word=dictionary, num_topics=300)``\n",
    "\n",
    "* LSI training is unique in that we can continue \"training\" at any point, simply by providing more training documents. This is done by incremental updates to the underlying moded (called **online training**). So the input document stream can be infinite -- just keep feeding LSI new documents as they arrive, while using the computed transformation model as read-only in the meanwhile!\n",
    "\n",
    "``model.add_documents(another_tfidf_corpus)``  # now LSI has been trained on tfidf_corpus + another_tfidf_corpus\n",
    "``lsi_vec = model[tfidf_vec]``  # convert some new document into the LSI space, without affecting the model\n",
    "``model.add_documents(more_documents)``  # tfidf_corpus + another_tfidf_corpus + more_documents\n",
    "``lsi_vec = model[tfidf_vec]``\n",
    "\n",
    "* See ``gensim.models.lsimodel`` docs to learn how to make LSI gradually \"forget\" old observations in infinite streams. If you want to get dirty, there are also parameters you can tweak that affect speed vs. memory footprint vs. numerical precision of the LSI algorithm.\n",
    "\n",
    "* `gensim` uses an online incremental streamed distributed training algorithm, published in [5]_. `gensim` also executes a stochastic multi-pass algorithm, from Halko et al. [4]_ to accelerate in-core computations. \n",
    "* See the wiki for clustering help.\n",
    "\n",
    "* `Random Projections, RP <http://www.cis.hut.fi/ella/publications/randproj_kdd.pdf>`_ aim to\n",
    "  reduce vector space dimensionality. This is a very efficient (both memory- and\n",
    "  CPU-friendly) approach to approximating TfIdf distances between documents, by throwing in a little randomness.\n",
    "  Recommended target dimensionality is again in the hundreds/thousands, depending on your dataset.\n",
    "\n",
    "''model = models.RpModel(tfidf_corpus, num_topics=500)''\n",
    "\n",
    "* [Latent Dirichlet Allocation, LDA](http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) also trransforms vectors from BoW counts into a topic space of lower dimensionality. \n",
    "* It is a probabilistic extension of LSA (also called multinomial PCA) - LDA's topics can be interpreted as probability distributions over words. \n",
    "* `gensim`s implementation of online LDA parameter estimation uses [2]_, modified to run in `distributed mode <distributed>` on clusters.\n",
    "\n",
    "``model = models.LdaModel(corpus, id2word=dictionary, num_topics=100)``\n",
    "\n",
    "* [Hierarchical Dirichlet Process, HDP](http://jmlr.csail.mit.edu/proceedings/papers/v15/wang11a/wang11a.pdf) is a non-parametric bayesian method (note the missing number of requested topics). `gensim`s implementation is based on [3]_. The HDP model is a new addition to `gensim`, and still rough around its academic edges -- use with care.\n",
    "\n",
    "''model = models.HdpModel(corpus, id2word=dictionary)''\n",
    "\n",
    "* Adding new VSM (Vector Space Model) transformers (such as different weighting schemes):\n",
    "    - see the `apiref` or [Python code](https://github.com/piskvorky/gensim/blob/develop/gensim/models/tfidfmodel.py)\n",
    "\n",
    "References\n",
    "----------\n",
    "\n",
    "[1] Bradford. 2008. An empirical study of required dimensionality for large-scale latent semantic indexing applications.\n",
    "\n",
    "[2] Hoffman, Blei, Bach. 2010. Online learning for Latent Dirichlet Allocation.\n",
    "\n",
    "[3] Wang, Paisley, Blei. 2011. Online variational inference for the hierarchical Dirichlet process.\n",
    "\n",
    "[4] Halko, Martinsson, Tropp. 2009. Finding structure with randomness.\n",
    "\n",
    "[5] Řehůřek. 2011. Subspace tracking for Latent Semantic Analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "#img = mpimg.imread('run_topics_and_transformations.png')\n",
    "#imgplot = plt.imshow(img)\n",
    "#plt.axis('off')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
