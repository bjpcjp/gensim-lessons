{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Movers' Distance\n",
    "=====================\n",
    "* It allows us to submit a query and return the most relevant documents. See _wmdistance_.\n",
    "\n",
    "* WMD assesses the distance between two documents even when they have no words in common. It uses [word2vec](http://rare-technologies.com/word2vec-tutorial/) vector embeddings of\n",
    "words. It been shown to outperform many of SOTA methods in KNN classification [3].\n",
    "\n",
    "* Below: WMD for two very similar sentences - [see Vlad Niculae's blog](http://vene.ro/blog/word-movers-distance-in-python.html>). \n",
    "* The sentences have no words in common, but by matching the relevant words, WMD is able to measure the (dis)similarity between them.\n",
    "* The method uses a BoW representation of the documents. The reasoning is that we find the minimum \"traveling distance\" between documents - aka the most efficient way to \"move\" the\n",
    "distribution of document 1 to the distribution of document 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* From [From Word Embeddings To Document Distances\" by Matt Kusner](http://jmlr.org/proceedings/papers/v37/kusnerb15.pdf>). It uses a \"transportation problem\" solver.\n",
    "\n",
    "* Gensim's WMD functions: the ``wmdistance`` method for distance computation, and \n",
    "``WmdSimilarity`` class for corpus based similarity queries.\n",
    "\n",
    "* If you use Gensim's WMD functionality, please consider citing [1], [2] and [3].\n",
    "\n",
    "### Computation\n",
    "-----------\n",
    "\n",
    "* To use WMD, you need an existing word embedding. Let's use an existing Word2Vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Initialize logging.\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "sentence_obama     = 'Obama speaks to the media in Illinois'\n",
    "sentence_president = 'The president greets the press in Chicago'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* These sentences have similar content - WMD should be low.\n",
    "* Remove stopwords (\"the\", \"to\") before proceeeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/bjpcjp/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import and download stopwords from NLTK.\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import download\n",
    "\n",
    "download('stopwords')  # Download stopwords list.\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def preprocess(sentence):\n",
    "    return [w for w in sentence.lower().split() if w not in stop_words]\n",
    "\n",
    "sentence_obama = preprocess(sentence_obama)\n",
    "sentence_president = preprocess(sentence_president)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Load these pre-trained embeddings into a Gensim Word2Vec model class.\n",
    "* Note: these embeddings require a lot of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GoogleNews-vectors-negative300.bin  GoogleNews-vectors-negative300.bin.gz\n"
     ]
    }
   ],
   "source": [
    "!ls ~/projects/*datasets*/google-news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 14:32:13,784 : WARNING : unable to import 'smart_open.gcs', disabling that module\n",
      "2020-05-05 14:32:15,285 : INFO : loading projection weights from /home/bjpcjp/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz\n",
      "2020-05-05 14:35:34,362 : INFO : loaded (3000000, 300) matrix from /home/bjpcjp/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz\n"
     ]
    }
   ],
   "source": [
    "# if downloading this model for the first time, grab a coffee. It's 1.6GB.\n",
    "import gensim.downloader as api\n",
    "model = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 14:35:34,368 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-05-05 14:35:34,370 : INFO : built Dictionary(8 unique tokens: ['illinois', 'media', 'obama', 'speaks', 'chicago']...) from 2 documents (total 8 corpus positions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance = 3.3741\n"
     ]
    }
   ],
   "source": [
    "distance = model.wmdistance(sentence_obama, sentence_president)\n",
    "print('distance = %.4f' % distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* try again with two unrelated sentences. The distance should be larger.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 14:35:34,519 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-05-05 14:35:34,521 : INFO : built Dictionary(7 unique tokens: ['illinois', 'media', 'obama', 'speaks', 'favorite']...) from 2 documents (total 7 corpus positions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance = 4.3802\n"
     ]
    }
   ],
   "source": [
    "sentence_orange = preprocess('Oranges are my favorite fruit')\n",
    "distance = model.wmdistance(sentence_obama, sentence_orange)\n",
    "print('distance = %.4f' % distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing word2vec vectors\n",
    "\n",
    "* You should first normalize word2vec vectors to equal lengths. Call **model.init_sims(replace=True)** to do this.\n",
    "* We often use **[cosine distance](https://en.wikipedia.org/wiki/Cosine_similarity)** when comparing word2vec vectors - it measures the **angle between vectors**. \n",
    "* WMD instead uses **Euclidean distance**. The ED between two vectors might be large due to different lengths, but the cosine distance can be small.\n",
    "* Note: normalizing the vectors can take some time, especially if you have a large vocabulary and/or large vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-05 14:39:04,982 : INFO : precomputing L2-norms of word weight vectors\n",
      "2020-05-05 14:39:07,220 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-05-05 14:39:07,221 : INFO : built Dictionary(8 unique tokens: ['illinois', 'media', 'obama', 'speaks', 'chicago']...) from 2 documents (total 8 corpus positions)\n",
      "2020-05-05 14:39:07,222 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2020-05-05 14:39:07,223 : INFO : built Dictionary(7 unique tokens: ['illinois', 'media', 'obama', 'speaks', 'favorite']...) from 2 documents (total 7 corpus positions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance(obama:president): 1.0174646259300113\n",
      "distance(obama:orange) = 1.3663\n"
     ]
    }
   ],
   "source": [
    "model.init_sims(replace=True)  # Normalizes the vectors in the word2vec class.\n",
    "\n",
    "distance = model.wmdistance(sentence_obama, sentence_president)\n",
    "print('distance(obama:president): %r' % distance)\n",
    "\n",
    "distance = model.wmdistance(sentence_obama, sentence_orange)\n",
    "print('distance(obama:orange) = %.4f' % distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References\n",
    "----------\n",
    "\n",
    "1. Ofir Pele and Michael Werman, *A linear time histogram metric for improved SIFT matching*\\ , 2008.\n",
    "2. Ofir Pele and Michael Werman, *Fast and robust earth mover's distances*\\ , 2009.\n",
    "3. Matt Kusner et al. *From Embeddings To Document Distances*\\ , 2015.\n",
    "4. Thomas Mikolov et al. *Efficient Estimation of Word Representations in Vector Space*\\ , 2013.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
